{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Tutorial: Books to Scrape\n",
    "\n",
    "## üìö Project Overview\n",
    "\n",
    "This comprehensive tutorial demonstrates web scraping techniques using **BeautifulSoup4** to extract book data from [books.toscrape.com](http://books.toscrape.com), a website designed specifically for practicing web scraping.\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand HTML structure and DOM navigation\n",
    "2. Master BeautifulSoup4 parsing techniques\n",
    "3. Handle pagination across multiple pages\n",
    "4. Extract structured data (titles, prices, ratings, availability)\n",
    "5. Clean and transform scraped data\n",
    "6. Store data in multiple formats (CSV, JSON)\n",
    "7. Implement proper error handling and logging\n",
    "\n",
    "### Website Structure:\n",
    "- **Target Site**: http://books.toscrape.com/\n",
    "- **Content**: 1000 books across 50 pages (20 books per page)\n",
    "- **Data Points**: Title, Price, Rating, Availability, Category, Image URL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Library Installation\n",
    "\n",
    "### Objective:\n",
    "Install and import all necessary libraries for web scraping, data manipulation, and storage.\n",
    "\n",
    "### Libraries Used:\n",
    "- **requests**: HTTP library to fetch web pages\n",
    "- **beautifulsoup4**: HTML parsing and navigation\n",
    "- **lxml**: Fast XML and HTML parser (BeautifulSoup backend)\n",
    "- **pandas**: Data manipulation and analysis\n",
    "- **datetime**: Timestamp generation\n",
    "- **time**: Rate limiting between requests\n",
    "- **json**: JSON data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run this cell first if libraries are not installed)\n",
    "# Uncomment the line below if you need to install the packages\n",
    "# !pip install requests beautifulsoup4 lxml pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìÖ Current timestamp: 2025-10-24 17:17:23.203749\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Current timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Understanding the Website Structure\n",
    "\n",
    "### Objective:\n",
    "Before scraping, we need to understand the HTML structure of the target website.\n",
    "\n",
    "### Process:\n",
    "1. Visit the website in a browser\n",
    "2. Right-click and select \"Inspect\" to view HTML\n",
    "3. Identify CSS classes and HTML tags containing our target data\n",
    "\n",
    "### Key HTML Elements:\n",
    "```html\n",
    "<article class=\"product_pod\">\n",
    "    <h3><a title=\"Book Title\">...</a></h3>\n",
    "    <p class=\"price_color\">¬£51.77</p>\n",
    "    <p class=\"star-rating Three\">...</p>\n",
    "    <p class=\"instock availability\">In stock</p>\n",
    "</article>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Base URL: http://books.toscrape.com/\n",
      "üìÇ Catalogue URL: http://books.toscrape.com/catalogue/\n"
     ]
    }
   ],
   "source": [
    "# Define base URL and initial configuration\n",
    "BASE_URL = \"http://books.toscrape.com/\"\n",
    "CATALOGUE_URL = f\"{BASE_URL}catalogue/\"\n",
    "\n",
    "# Headers to mimic a real browser request\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "}\n",
    "\n",
    "print(f\"üåê Base URL: {BASE_URL}\")\n",
    "print(f\"üìÇ Catalogue URL: {CATALOGUE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Fetching a Single Web Page\n",
    "\n",
    "### Objective:\n",
    "Create a function to fetch HTML content from a URL with proper error handling.\n",
    "\n",
    "### Function Details:\n",
    "- **Input**: URL string\n",
    "- **Output**: HTML content as string or None if error\n",
    "- **Error Handling**: HTTP errors, timeouts, connection errors\n",
    "- **Timeout**: 10 seconds to prevent hanging\n",
    "\n",
    "### HTTP Status Codes:\n",
    "- 200: Success\n",
    "- 404: Page not found\n",
    "- 500: Server error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:17:23,209 - INFO - Fetching URL: http://books.toscrape.com/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fetch_page function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:17:23,456 - INFO - ‚úÖ Successfully fetched http://books.toscrape.com/ (Status: 200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully fetched homepage\n",
      "üìä HTML length: 51294 characters\n",
      "üìÑ First 200 characters:\n",
      "<!DOCTYPE html>\n",
      "<!--[if lt IE 7]>      <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\n",
      "<!--[if IE 7]>         <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\n",
      "<!--[if I...\n"
     ]
    }
   ],
   "source": [
    "def fetch_page(url: str, timeout: int = 10) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch HTML content from a given URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to fetch\n",
    "        timeout (int): Request timeout in seconds (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        str: HTML content if successful, None otherwise\n",
    "    \n",
    "    Raises:\n",
    "        Logs errors but does not raise exceptions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Fetching URL: {url}\")\n",
    "        response = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "        \n",
    "        # Raise exception for bad status codes (4xx, 5xx)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        logging.info(f\"‚úÖ Successfully fetched {url} (Status: {response.status_code})\")\n",
    "        return response.text\n",
    "        \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logging.error(f\"‚ùå HTTP Error: {e}\")\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        logging.error(f\"‚ùå Connection Error: {e}\")\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        logging.error(f\"‚ùå Timeout Error: {e}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"‚ùå Request Error: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test the function with the homepage\n",
    "print(\"Testing fetch_page function...\")\n",
    "html_content = fetch_page(BASE_URL)\n",
    "\n",
    "if html_content:\n",
    "    print(f\"‚úÖ Successfully fetched homepage\")\n",
    "    print(f\"üìä HTML length: {len(html_content)} characters\")\n",
    "    print(f\"üìÑ First 200 characters:\\n{html_content[:200]}...\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to fetch homepage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Parsing HTML with BeautifulSoup\n",
    "\n",
    "### Objective:\n",
    "Parse raw HTML into a BeautifulSoup object for easy navigation and data extraction.\n",
    "\n",
    "### BeautifulSoup Parsers:\n",
    "- **lxml**: Fast and lenient (recommended)\n",
    "- **html.parser**: Built-in Python parser (slower)\n",
    "- **html5lib**: Most lenient, very slow\n",
    "\n",
    "### Key Methods:\n",
    "- `find()`: Find first matching element\n",
    "- `find_all()`: Find all matching elements\n",
    "- `select()`: CSS selector based search\n",
    "- `get_text()`: Extract text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:17:23,494 - INFO - ‚úÖ HTML parsed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Page Title: \n",
      "    All products | Books to Scrape - Sandbox\n",
      "\n",
      "üìö Number of books found: 20\n",
      "üìÑ Pagination: Page 1 of 50\n"
     ]
    }
   ],
   "source": [
    "def parse_html(html_content: str) -> Optional[BeautifulSoup]:\n",
    "    \"\"\"\n",
    "    Parse HTML content into BeautifulSoup object.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup: Parsed HTML object or None if parsing fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        logging.info(\"‚úÖ HTML parsed successfully\")\n",
    "        return soup\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Error parsing HTML: {e}\")\n",
    "        return None\n",
    "\n",
    "# Parse the homepage HTML\n",
    "soup = parse_html(html_content)\n",
    "\n",
    "if soup:\n",
    "    # Extract page title\n",
    "    page_title = soup.find('title').get_text()\n",
    "    print(f\"üìñ Page Title: {page_title}\")\n",
    "    \n",
    "    # Count number of books on the page\n",
    "    books = soup.find_all('article', class_='product_pod')\n",
    "    print(f\"üìö Number of books found: {len(books)}\")\n",
    "    \n",
    "    # Find pagination info\n",
    "    pager = soup.find('li', class_='current')\n",
    "    if pager:\n",
    "        print(f\"üìÑ Pagination: {pager.get_text().strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Extracting Book Data from a Single Page\n",
    "\n",
    "### Objective:\n",
    "Extract detailed information about each book from a single page.\n",
    "\n",
    "### Data Points to Extract:\n",
    "1. **Title**: Book name\n",
    "2. **Price**: Price in GBP (¬£)\n",
    "3. **Rating**: Star rating (One to Five)\n",
    "4. **Availability**: In stock or out of stock\n",
    "5. **Image URL**: Book cover image URL\n",
    "6. **Product URL**: Link to book detail page\n",
    "\n",
    "### CSS Selectors Used:\n",
    "- `article.product_pod`: Each book container\n",
    "- `h3 a`: Book title and URL\n",
    "- `p.price_color`: Price\n",
    "- `p.star-rating`: Rating class\n",
    "- `p.instock.availability`: Availability status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:17:23,509 - INFO - Found 20 books on the page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Extracting books from the first page...\n",
      "\n",
      "‚úÖ Successfully extracted 20 books\n",
      "\n",
      "üìö Sample Book Data (First Book):\n",
      "\n",
      "{\n",
      "  \"title\": \"A Light in the Attic\",\n",
      "  \"price\": 51.77,\n",
      "  \"rating\": 3,\n",
      "  \"availability\": \"In stock\",\n",
      "  \"image_url\": \"http://books.toscrape.com/media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\",\n",
      "  \"product_url\": \"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\",\n",
      "  \"scraped_at\": \"2025-10-24T17:17:23.510022\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def clean_price_text(price_text: str) -> float:\n",
    "    \"\"\"\n",
    "    Clean and convert price text to float, handling encoding issues.\n",
    "    \n",
    "    Args:\n",
    "        price_text (str): Raw price text from HTML (e.g., '¬£51.77', '√Ç¬£51.77')\n",
    "    \n",
    "    Returns:\n",
    "        float: Cleaned price value\n",
    "    \"\"\"\n",
    "    # Remove all non-ASCII characters, currency symbols, and whitespace\n",
    "    # Keep only digits and decimal point\n",
    "    cleaned = re.sub(r'[^\\d.]', '', price_text)\n",
    "    \n",
    "    try:\n",
    "        return float(cleaned)\n",
    "    except ValueError:\n",
    "        # If conversion still fails, log and return 0\n",
    "        logging.warning(f\"Could not convert price: '{price_text}' -> '{cleaned}'\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def extract_rating(rating_class: str) -> int:\n",
    "    \"\"\"\n",
    "    Convert star rating class to numeric value.\n",
    "    \n",
    "    Args:\n",
    "        rating_class (str): CSS class containing rating (e.g., 'star-rating Three')\n",
    "    \n",
    "    Returns:\n",
    "        int: Numeric rating (1-5)\n",
    "    \"\"\"\n",
    "    rating_map = {\n",
    "        'One': 1,\n",
    "        'Two': 2,\n",
    "        'Three': 3,\n",
    "        'Four': 4,\n",
    "        'Five': 5\n",
    "    }\n",
    "    \n",
    "    # Extract rating word from class string\n",
    "    for word, value in rating_map.items():\n",
    "        if word in rating_class:\n",
    "            return value\n",
    "    \n",
    "    return 0  # Default if no rating found\n",
    "\n",
    "\n",
    "def parse_book_data(book_element: BeautifulSoup, base_url: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract all relevant data from a single book element.\n",
    "    \n",
    "    Args:\n",
    "        book_element (BeautifulSoup): Parsed book article element\n",
    "        base_url (str): Base URL for constructing absolute URLs\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing book information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract title and URL\n",
    "        title_element = book_element.find('h3').find('a')\n",
    "        title = title_element.get('title')\n",
    "        product_url = base_url + title_element.get('href')\n",
    "        \n",
    "        # Extract price using robust cleaning function\n",
    "        price_text = book_element.find('p', class_='price_color').get_text()\n",
    "        price = clean_price_text(price_text)\n",
    "        \n",
    "        # Extract rating\n",
    "        rating_element = book_element.find('p', class_='star-rating')\n",
    "        rating_class = rating_element.get('class')[1]  # Get second class (rating word)\n",
    "        rating = extract_rating(rating_class)\n",
    "        \n",
    "        # Extract availability\n",
    "        availability_element = book_element.find('p', class_='instock')\n",
    "        availability = availability_element.get_text().strip()\n",
    "        \n",
    "        # Extract image URL\n",
    "        image_element = book_element.find('img')\n",
    "        image_url = base_url + image_element.get('src')\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'rating': rating,\n",
    "            'availability': availability,\n",
    "            'image_url': image_url,\n",
    "            'product_url': product_url,\n",
    "            'scraped_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Error parsing book data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_books_from_page(soup: BeautifulSoup, base_url: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract all books from a single page.\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML page\n",
    "        base_url (str): Base URL for constructing absolute URLs\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing book data\n",
    "    \"\"\"\n",
    "    books_data = []\n",
    "    \n",
    "    # Find all book elements\n",
    "    book_elements = soup.find_all('article', class_='product_pod')\n",
    "    \n",
    "    logging.info(f\"Found {len(book_elements)} books on the page\")\n",
    "    \n",
    "    for book_element in book_elements:\n",
    "        book_data = parse_book_data(book_element, base_url)\n",
    "        if book_data:\n",
    "            books_data.append(book_data)\n",
    "    \n",
    "    return books_data\n",
    "\n",
    "\n",
    "# Test: Extract books from the first page\n",
    "print(\"\\nüîç Extracting books from the first page...\\n\")\n",
    "books_on_page = scrape_books_from_page(soup, BASE_URL)\n",
    "\n",
    "print(f\"‚úÖ Successfully extracted {len(books_on_page)} books\\n\")\n",
    "\n",
    "if len(books_on_page) > 0:\n",
    "    print(\"üìö Sample Book Data (First Book):\\n\")\n",
    "    print(json.dumps(books_on_page[0], indent=2))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No books were extracted. Please check the HTML structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Handling Pagination\n",
    "\n",
    "### Objective:\n",
    "Navigate through multiple pages to scrape all books from the website.\n",
    "\n",
    "### Pagination Strategy:\n",
    "1. Start with page 1\n",
    "2. Look for \"Next\" button or page number\n",
    "3. Extract next page URL\n",
    "4. Repeat until no more pages\n",
    "\n",
    "### URL Pattern:\n",
    "- Page 1: `http://books.toscrape.com/`\n",
    "- Page 2: `http://books.toscrape.com/catalogue/page-2.html`\n",
    "- Page N: `http://books.toscrape.com/catalogue/page-N.html`\n",
    "\n",
    "### Rate Limiting:\n",
    "- Add 1-2 second delay between requests\n",
    "- Prevents overwhelming the server\n",
    "- Mimics human browsing behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:17:23,518 - INFO - Fetching URL: http://books.toscrape.com/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pagination with first 3 pages...\n",
      "\n",
      "üöÄ Starting to scrape all books...\n",
      "\n",
      "üìÑ Scraping page 1: http://books.toscrape.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:17:23,918 - INFO - ‚úÖ Successfully fetched http://books.toscrape.com/ (Status: 200)\n",
      "2025-10-24 17:17:23,945 - INFO - ‚úÖ HTML parsed successfully\n",
      "2025-10-24 17:17:23,946 - INFO - Found 20 books on the page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Extracted 20 books (Total: 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:17:24,956 - INFO - Fetching URL: http://books.toscrape.com/catalogue/catalogue/page-2.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Scraping page 2: http://books.toscrape.com/catalogue/catalogue/page-2.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:17:25,278 - ERROR - ‚ùå HTTP Error: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/catalogue/page-2.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed to fetch page 2\n",
      "\n",
      "üéâ Scraping complete!\n",
      "üìä Total pages scraped: 2\n",
      "üìö Total books extracted: 20\n"
     ]
    }
   ],
   "source": [
    "def get_next_page_url(soup: BeautifulSoup, current_url: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the URL of the next page from pagination links.\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed current page\n",
    "        current_url (str): Current page URL\n",
    "    \n",
    "    Returns:\n",
    "        str: Next page URL or None if no next page\n",
    "    \"\"\"\n",
    "    # Find the \"next\" button\n",
    "    next_button = soup.find('li', class_='next')\n",
    "    \n",
    "    if next_button:\n",
    "        next_link = next_button.find('a')\n",
    "        if next_link:\n",
    "            next_page = next_link.get('href')\n",
    "            \n",
    "            # Construct absolute URL\n",
    "            if current_url.endswith('index.html') or current_url == BASE_URL:\n",
    "                next_url = CATALOGUE_URL + next_page\n",
    "            else:\n",
    "                # Replace current page with next page\n",
    "                base = current_url.rsplit('/', 1)[0]\n",
    "                next_url = base + '/' + next_page\n",
    "            \n",
    "            return next_url\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def scrape_all_books(max_pages: Optional[int] = None, delay: float = 1.5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape all books from all pages on the website.\n",
    "    \n",
    "    Args:\n",
    "        max_pages (int, optional): Maximum number of pages to scrape (None = all pages)\n",
    "        delay (float): Delay in seconds between page requests (default: 1.5)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of all book dictionaries\n",
    "    \"\"\"\n",
    "    all_books = []\n",
    "    current_url = BASE_URL\n",
    "    page_count = 0\n",
    "    \n",
    "    print(\"üöÄ Starting to scrape all books...\\n\")\n",
    "    \n",
    "    while current_url:\n",
    "        page_count += 1\n",
    "        \n",
    "        # Check max pages limit\n",
    "        if max_pages and page_count > max_pages:\n",
    "            print(f\"\\n‚èπÔ∏è  Reached maximum page limit: {max_pages}\")\n",
    "            break\n",
    "        \n",
    "        print(f\"üìÑ Scraping page {page_count}: {current_url}\")\n",
    "        \n",
    "        # Fetch and parse page\n",
    "        html = fetch_page(current_url)\n",
    "        if not html:\n",
    "            print(f\"‚ùå Failed to fetch page {page_count}\")\n",
    "            break\n",
    "        \n",
    "        soup = parse_html(html)\n",
    "        if not soup:\n",
    "            print(f\"‚ùå Failed to parse page {page_count}\")\n",
    "            break\n",
    "        \n",
    "        # Extract books from current page\n",
    "        books = scrape_books_from_page(soup, BASE_URL)\n",
    "        all_books.extend(books)\n",
    "        print(f\"   ‚úÖ Extracted {len(books)} books (Total: {len(all_books)})\")\n",
    "        \n",
    "        # Get next page URL\n",
    "        next_url = get_next_page_url(soup, current_url)\n",
    "        \n",
    "        if next_url:\n",
    "            current_url = next_url\n",
    "            # Rate limiting: wait before next request\n",
    "            time.sleep(delay)\n",
    "        else:\n",
    "            print(\"\\n‚úÖ No more pages to scrape\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nüéâ Scraping complete!\")\n",
    "    print(f\"üìä Total pages scraped: {page_count}\")\n",
    "    print(f\"üìö Total books extracted: {len(all_books)}\")\n",
    "    \n",
    "    return all_books\n",
    "\n",
    "\n",
    "# Test: Scrape first 3 pages only (for demonstration)\n",
    "print(\"Testing pagination with first 3 pages...\\n\")\n",
    "sample_books = scrape_all_books(max_pages=3, delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Data Cleaning and Validation\n",
    "\n",
    "### Objective:\n",
    "Clean and validate the scraped data to ensure quality and consistency.\n",
    "\n",
    "### Cleaning Operations:\n",
    "1. Remove duplicate books (based on title)\n",
    "2. Handle missing values\n",
    "3. Validate data types\n",
    "4. Standardize text fields\n",
    "5. Check for data anomalies\n",
    "\n",
    "### Validation Checks:\n",
    "- Price should be positive number\n",
    "- Rating should be between 1-5\n",
    "- Title should not be empty\n",
    "- URLs should be valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning and validating data...\n",
      "\n",
      "üìä Initial dataset shape: (20, 7)\n",
      "üìã Columns: ['title', 'price', 'rating', 'availability', 'image_url', 'product_url', 'scraped_at']\n",
      "\n",
      "üîç Duplicate books found: 0\n",
      "\n",
      "‚ùì Missing values:\n",
      "title           0\n",
      "price           0\n",
      "rating          0\n",
      "availability    0\n",
      "image_url       0\n",
      "product_url     0\n",
      "scraped_at      0\n",
      "dtype: int64\n",
      "\n",
      "üí∞ Invalid prices (‚â§0): 0\n",
      "‚≠ê Invalid ratings (not 1-5): 0\n",
      "\n",
      "‚úÖ Data cleaning complete!\n",
      "üìä Final dataset shape: (20, 9)\n",
      "\n",
      "üìö Sample Data (First 5 Books):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>availability</th>\n",
       "      <th>image_url</th>\n",
       "      <th>product_url</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>title_length</th>\n",
       "      <th>in_stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>51.77</td>\n",
       "      <td>3</td>\n",
       "      <td>In stock</td>\n",
       "      <td>http://books.toscrape.com/media/cache/2c/da/2c...</td>\n",
       "      <td>http://books.toscrape.com/catalogue/a-light-in...</td>\n",
       "      <td>2025-10-24T17:17:23.947269</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>53.74</td>\n",
       "      <td>1</td>\n",
       "      <td>In stock</td>\n",
       "      <td>http://books.toscrape.com/media/cache/26/0c/26...</td>\n",
       "      <td>http://books.toscrape.com/catalogue/tipping-th...</td>\n",
       "      <td>2025-10-24T17:17:23.947420</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>50.10</td>\n",
       "      <td>1</td>\n",
       "      <td>In stock</td>\n",
       "      <td>http://books.toscrape.com/media/cache/3e/ef/3e...</td>\n",
       "      <td>http://books.toscrape.com/catalogue/soumission...</td>\n",
       "      <td>2025-10-24T17:17:23.947565</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>47.82</td>\n",
       "      <td>4</td>\n",
       "      <td>In stock</td>\n",
       "      <td>http://books.toscrape.com/media/cache/32/51/32...</td>\n",
       "      <td>http://books.toscrape.com/catalogue/sharp-obje...</td>\n",
       "      <td>2025-10-24T17:17:23.947710</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>54.23</td>\n",
       "      <td>5</td>\n",
       "      <td>In stock</td>\n",
       "      <td>http://books.toscrape.com/media/cache/be/a5/be...</td>\n",
       "      <td>http://books.toscrape.com/catalogue/sapiens-a-...</td>\n",
       "      <td>2025-10-24T17:17:23.947856</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  price  rating availability  \\\n",
       "0                   A Light in the Attic  51.77       3     In stock   \n",
       "1                     Tipping the Velvet  53.74       1     In stock   \n",
       "2                             Soumission  50.10       1     In stock   \n",
       "3                          Sharp Objects  47.82       4     In stock   \n",
       "4  Sapiens: A Brief History of Humankind  54.23       5     In stock   \n",
       "\n",
       "                                           image_url  \\\n",
       "0  http://books.toscrape.com/media/cache/2c/da/2c...   \n",
       "1  http://books.toscrape.com/media/cache/26/0c/26...   \n",
       "2  http://books.toscrape.com/media/cache/3e/ef/3e...   \n",
       "3  http://books.toscrape.com/media/cache/32/51/32...   \n",
       "4  http://books.toscrape.com/media/cache/be/a5/be...   \n",
       "\n",
       "                                         product_url  \\\n",
       "0  http://books.toscrape.com/catalogue/a-light-in...   \n",
       "1  http://books.toscrape.com/catalogue/tipping-th...   \n",
       "2  http://books.toscrape.com/catalogue/soumission...   \n",
       "3  http://books.toscrape.com/catalogue/sharp-obje...   \n",
       "4  http://books.toscrape.com/catalogue/sapiens-a-...   \n",
       "\n",
       "                   scraped_at  title_length  in_stock  \n",
       "0  2025-10-24T17:17:23.947269            20      True  \n",
       "1  2025-10-24T17:17:23.947420            18      True  \n",
       "2  2025-10-24T17:17:23.947565            10      True  \n",
       "3  2025-10-24T17:17:23.947710            13      True  \n",
       "4  2025-10-24T17:17:23.947856            37      True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_and_validate_data(books_data: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and validate scraped book data.\n",
    "    \n",
    "    Args:\n",
    "        books_data (list): List of book dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned and validated data\n",
    "    \"\"\"\n",
    "    print(\"üßπ Cleaning and validating data...\\n\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(books_data)\n",
    "    \n",
    "    print(f\"üìä Initial dataset shape: {df.shape}\")\n",
    "    print(f\"üìã Columns: {list(df.columns)}\\n\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated(subset=['title']).sum()\n",
    "    print(f\"üîç Duplicate books found: {duplicates}\")\n",
    "    if duplicates > 0:\n",
    "        df = df.drop_duplicates(subset=['title'], keep='first')\n",
    "        print(f\"   ‚úÖ Removed {duplicates} duplicates\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    print(f\"\\n‚ùì Missing values:\\n{missing}\")\n",
    "    \n",
    "    # Validate price\n",
    "    invalid_prices = df[df['price'] <= 0].shape[0]\n",
    "    print(f\"\\nüí∞ Invalid prices (‚â§0): {invalid_prices}\")\n",
    "    \n",
    "    # Validate rating\n",
    "    invalid_ratings = df[(df['rating'] < 1) | (df['rating'] > 5)].shape[0]\n",
    "    print(f\"‚≠ê Invalid ratings (not 1-5): {invalid_ratings}\")\n",
    "    \n",
    "    # Add derived columns\n",
    "    df['title_length'] = df['title'].str.len()\n",
    "    df['in_stock'] = df['availability'].str.contains('In stock', case=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data cleaning complete!\")\n",
    "    print(f\"üìä Final dataset shape: {df.shape}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Clean and validate the sample data\n",
    "df_books = clean_and_validate_data(sample_books)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"üìö Sample Data (First 5 Books):\\n\")\n",
    "display(df_books.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Data Analysis and Statistics\n",
    "\n",
    "### Objective:\n",
    "Perform exploratory data analysis on the scraped data.\n",
    "\n",
    "### Analysis Areas:\n",
    "1. Price statistics (mean, median, min, max)\n",
    "2. Rating distribution\n",
    "3. Availability analysis\n",
    "4. Top rated books\n",
    "5. Most expensive and cheapest books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DATA ANALYSIS REPORT\n",
      "\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  DATASET OVERVIEW\n",
      "------------------------------------------------------------\n",
      "Total Books: 20\n",
      "Books in Stock: 20\n",
      "Books Out of Stock: 0\n",
      "\n",
      "2Ô∏è‚É£  PRICE STATISTICS (¬£)\n",
      "------------------------------------------------------------\n",
      "Average Price: ¬£38.05\n",
      "Median Price: ¬£41.38\n",
      "Minimum Price: ¬£13.99\n",
      "Maximum Price: ¬£57.25\n",
      "Standard Deviation: ¬£15.14\n",
      "\n",
      "3Ô∏è‚É£  RATING DISTRIBUTION\n",
      "------------------------------------------------------------\n",
      "‚≠ê (1): 6 books (30.0%)\n",
      "‚≠ê‚≠ê (2): 3 books (15.0%)\n",
      "‚≠ê‚≠ê‚≠ê (3): 3 books (15.0%)\n",
      "‚≠ê‚≠ê‚≠ê‚≠ê (4): 4 books (20.0%)\n",
      "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5): 4 books (20.0%)\n",
      "\n",
      "Average Rating: 2.85 ‚≠ê\n",
      "\n",
      "4Ô∏è‚É£  TOP 5 MOST EXPENSIVE BOOKS\n",
      "------------------------------------------------------------\n",
      "¬£57.25 - Our Band Could Be Your Life: Scenes from the Ameri... (‚≠ê3)\n",
      "¬£54.23 - Sapiens: A Brief History of Humankind... (‚≠ê5)\n",
      "¬£53.74 - Tipping the Velvet... (‚≠ê1)\n",
      "¬£52.29 - Scott Pilgrim's Precious Little Life (Scott Pilgri... (‚≠ê5)\n",
      "¬£52.15 - The Black Maria... (‚≠ê1)\n",
      "\n",
      "5Ô∏è‚É£  TOP 5 CHEAPEST BOOKS\n",
      "------------------------------------------------------------\n",
      "¬£13.99 - Starving Hearts (Triangular Trade Trilogy, #1)... (‚≠ê2)\n",
      "¬£17.46 - Set Me Free... (‚≠ê5)\n",
      "¬£17.93 - The Coming Woman: A Novel Based on the Life of the... (‚≠ê3)\n",
      "¬£20.66 - Shakespeare's Sonnets... (‚≠ê4)\n",
      "¬£22.60 - The Boys in the Boat: Nine Americans and Their Epi... (‚≠ê4)\n",
      "\n",
      "6Ô∏è‚É£  FIVE-STAR BOOKS\n",
      "------------------------------------------------------------\n",
      "Total 5-star books: 4\n",
      "\n",
      "Sample 5-star books:\n",
      "  ‚Ä¢ Sapiens: A Brief History of Humankind (¬£54.23)\n",
      "  ‚Ä¢ Set Me Free (¬£17.46)\n",
      "  ‚Ä¢ Scott Pilgrim's Precious Little Life (Scott Pilgrim #1) (¬£52.29)\n",
      "  ‚Ä¢ Rip it Up and Start Again (¬£35.02)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def analyze_book_data(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on book dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Book data\n",
    "    \"\"\"\n",
    "    print(\"üìä DATA ANALYSIS REPORT\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n1Ô∏è‚É£  DATASET OVERVIEW\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total Books: {len(df)}\")\n",
    "    print(f\"Books in Stock: {df['in_stock'].sum()}\")\n",
    "    print(f\"Books Out of Stock: {(~df['in_stock']).sum()}\")\n",
    "    \n",
    "    # Price statistics\n",
    "    print(\"\\n2Ô∏è‚É£  PRICE STATISTICS (¬£)\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Average Price: ¬£{df['price'].mean():.2f}\")\n",
    "    print(f\"Median Price: ¬£{df['price'].median():.2f}\")\n",
    "    print(f\"Minimum Price: ¬£{df['price'].min():.2f}\")\n",
    "    print(f\"Maximum Price: ¬£{df['price'].max():.2f}\")\n",
    "    print(f\"Standard Deviation: ¬£{df['price'].std():.2f}\")\n",
    "    \n",
    "    # Rating distribution\n",
    "    print(\"\\n3Ô∏è‚É£  RATING DISTRIBUTION\")\n",
    "    print(\"-\" * 60)\n",
    "    rating_counts = df['rating'].value_counts().sort_index()\n",
    "    for rating, count in rating_counts.items():\n",
    "        stars = '‚≠ê' * rating\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"{stars} ({rating}): {count} books ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nAverage Rating: {df['rating'].mean():.2f} ‚≠ê\")\n",
    "    \n",
    "    # Top 5 most expensive books\n",
    "    print(\"\\n4Ô∏è‚É£  TOP 5 MOST EXPENSIVE BOOKS\")\n",
    "    print(\"-\" * 60)\n",
    "    top_expensive = df.nlargest(5, 'price')[['title', 'price', 'rating']]\n",
    "    for idx, row in top_expensive.iterrows():\n",
    "        print(f\"¬£{row['price']:.2f} - {row['title'][:50]}... (‚≠ê{row['rating']})\")\n",
    "    \n",
    "    # Top 5 cheapest books\n",
    "    print(\"\\n5Ô∏è‚É£  TOP 5 CHEAPEST BOOKS\")\n",
    "    print(\"-\" * 60)\n",
    "    top_cheap = df.nsmallest(5, 'price')[['title', 'price', 'rating']]\n",
    "    for idx, row in top_cheap.iterrows():\n",
    "        print(f\"¬£{row['price']:.2f} - {row['title'][:50]}... (‚≠ê{row['rating']})\")\n",
    "    \n",
    "    # Top rated books (5 stars)\n",
    "    print(\"\\n6Ô∏è‚É£  FIVE-STAR BOOKS\")\n",
    "    print(\"-\" * 60)\n",
    "    five_star = df[df['rating'] == 5]\n",
    "    print(f\"Total 5-star books: {len(five_star)}\")\n",
    "    if len(five_star) > 0:\n",
    "        print(\"\\nSample 5-star books:\")\n",
    "        for idx, row in five_star.head(5).iterrows():\n",
    "            print(f\"  ‚Ä¢ {row['title'][:60]} (¬£{row['price']:.2f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "analyze_book_data(df_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Saving Data to Files\n",
    "\n",
    "### Objective:\n",
    "Save the scraped and cleaned data to multiple file formats for further use.\n",
    "\n",
    "### File Formats:\n",
    "1. **CSV**: Comma-separated values (universal format)\n",
    "2. **JSON**: JavaScript Object Notation (web-friendly)\n",
    "3. **Excel**: Microsoft Excel format (business-friendly)\n",
    "4. **Parquet**: Columnar storage (big data optimized)\n",
    "\n",
    "### Use Cases:\n",
    "- CSV: Import into databases, spreadsheet analysis\n",
    "- JSON: Web APIs, JavaScript applications\n",
    "- Excel: Business reports, manual review\n",
    "- Parquet: PySpark processing, data lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving data to files...\n",
      "\n",
      "‚úÖ Saved CSV: data/books_20251024_171725.csv\n",
      "‚úÖ Saved JSON: data/books_20251024_171725.json\n",
      "‚ö†Ô∏è  Excel save skipped (install openpyxl to enable)\n",
      "‚ö†Ô∏è  Parquet save skipped (install pyarrow to enable)\n",
      "\n",
      "üéâ All data saved to 'data/' directory\n",
      "\n",
      "üìÅ File Sizes:\n",
      "   books_20251024_171725.json: 8.51 KB\n",
      "   books_20251024_171725.csv: 5.09 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def save_data(df: pd.DataFrame, output_dir: str = 'data') -> None:\n",
    "    \"\"\"\n",
    "    Save DataFrame to multiple file formats.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Data to save\n",
    "        output_dir (str): Output directory path\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    print(\"üíæ Saving data to files...\\n\")\n",
    "    \n",
    "    # Save as CSV\n",
    "    csv_path = f\"{output_dir}/books_{timestamp}.csv\"\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"‚úÖ Saved CSV: {csv_path}\")\n",
    "    \n",
    "    # Save as JSON\n",
    "    json_path = f\"{output_dir}/books_{timestamp}.json\"\n",
    "    df.to_json(json_path, orient='records', indent=2, force_ascii=False)\n",
    "    print(f\"‚úÖ Saved JSON: {json_path}\")\n",
    "    \n",
    "    # Save as Excel (requires openpyxl)\n",
    "    try:\n",
    "        excel_path = f\"{output_dir}/books_{timestamp}.xlsx\"\n",
    "        df.to_excel(excel_path, index=False, engine='openpyxl')\n",
    "        print(f\"‚úÖ Saved Excel: {excel_path}\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Excel save skipped (install openpyxl to enable)\")\n",
    "    \n",
    "    # Save as Parquet (requires pyarrow)\n",
    "    try:\n",
    "        parquet_path = f\"{output_dir}/books_{timestamp}.parquet\"\n",
    "        df.to_parquet(parquet_path, index=False, engine='pyarrow')\n",
    "        print(f\"‚úÖ Saved Parquet: {parquet_path}\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Parquet save skipped (install pyarrow to enable)\")\n",
    "    \n",
    "    print(f\"\\nüéâ All data saved to '{output_dir}/' directory\")\n",
    "    \n",
    "    # Display file sizes\n",
    "    print(\"\\nüìÅ File Sizes:\")\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if timestamp in filename:\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            size_kb = os.path.getsize(filepath) / 1024\n",
    "            print(f\"   {filename}: {size_kb:.2f} KB\")\n",
    "\n",
    "\n",
    "# Save the data\n",
    "save_data(df_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Complete Pipeline - Scrape All Books\n",
    "\n",
    "### Objective:\n",
    "Execute the complete scraping pipeline to collect all books from the website.\n",
    "\n",
    "### Pipeline Steps:\n",
    "1. Scrape all pages (50 pages, ~1000 books)\n",
    "2. Clean and validate data\n",
    "3. Perform analysis\n",
    "4. Save to multiple formats\n",
    "\n",
    "### Estimated Time:\n",
    "- With 1.5 second delay per page: ~75 seconds (50 pages √ó 1.5s)\n",
    "- Total processing time: ~2-3 minutes\n",
    "\n",
    "‚ö†Ô∏è **Note**: Uncomment and run the cell below to scrape ALL books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° To run the complete pipeline, uncomment one of the lines above.\n"
     ]
    }
   ],
   "source": [
    "def run_complete_pipeline(max_pages: Optional[int] = None, delay: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Execute the complete web scraping pipeline.\n",
    "    \n",
    "    Args:\n",
    "        max_pages (int, optional): Maximum pages to scrape (None = all)\n",
    "        delay (float): Delay between requests in seconds\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned and validated book data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üöÄ STARTING COMPLETE WEB SCRAPING PIPELINE\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Scrape all books\n",
    "    print(\"\\nüìñ STEP 1: SCRAPING BOOKS\")\n",
    "    print(\"-\" * 70)\n",
    "    all_books = scrape_all_books(max_pages=max_pages, delay=delay)\n",
    "    \n",
    "    # Step 2: Clean and validate\n",
    "    print(\"\\nüßπ STEP 2: CLEANING AND VALIDATING DATA\")\n",
    "    print(\"-\" * 70)\n",
    "    df_clean = clean_and_validate_data(all_books)\n",
    "    \n",
    "    # Step 3: Analyze data\n",
    "    print(\"\\nüìä STEP 3: ANALYZING DATA\")\n",
    "    print(\"-\" * 70)\n",
    "    analyze_book_data(df_clean)\n",
    "    \n",
    "    # Step 4: Save data\n",
    "    print(\"\\nüíæ STEP 4: SAVING DATA\")\n",
    "    print(\"-\" * 70)\n",
    "    save_data(df_clean)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n‚è±Ô∏è  Total execution time: {minutes}m {seconds}s\")\n",
    "    print(f\"üìö Total books scraped: {len(df_clean)}\")\n",
    "    print(f\"üíæ Data saved to 'data/' directory\\n\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# UNCOMMENT THE LINE BELOW TO SCRAPE ALL BOOKS (50 pages)\n",
    "# This will take approximately 2-3 minutes to complete\n",
    "\n",
    "# df_all_books = run_complete_pipeline(max_pages=None, delay=1.5)\n",
    "\n",
    "# Or scrape just the first 5 pages for testing:\n",
    "# df_test = run_complete_pipeline(max_pages=5, delay=1.0)\n",
    "\n",
    "print(\"\\nüí° To run the complete pipeline, uncomment one of the lines above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: Advanced Features (Optional)\n",
    "\n",
    "### Additional Enhancements:\n",
    "1. Scrape detailed book information (description, UPC, reviews)\n",
    "2. Extract category information\n",
    "3. Download book cover images\n",
    "4. Create data visualizations\n",
    "5. Export to database (SQLite, PostgreSQL)\n",
    "\n",
    "Below are examples of advanced scraping techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:17:25,357 - INFO - Fetching URL: http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scraping detailed information...\n",
      "\n",
      "URL: http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:17:25,694 - INFO - ‚úÖ Successfully fetched http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html (Status: 200)\n",
      "2025-10-24 17:17:25,703 - INFO - ‚úÖ HTML parsed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Book Details:\n",
      "\n",
      "{\n",
      "  \"description\": \"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\",\n",
      "  \"upc\": \"a897fe39b1053632\",\n",
      "  \"product_type\": \"Books\",\n",
      "  \"tax\": \"\\u00c2\\u00a30.00\",\n",
      "  \"num_reviews\": \"0\",\n",
      "  \"category\": \"Poetry\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def scrape_book_details(book_url: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Scrape detailed information from individual book page.\n",
    "    \n",
    "    Args:\n",
    "        book_url (str): URL of the book detail page\n",
    "    \n",
    "    Returns:\n",
    "        dict: Detailed book information\n",
    "    \"\"\"\n",
    "    html = fetch_page(book_url)\n",
    "    if not html:\n",
    "        return None\n",
    "    \n",
    "    soup = parse_html(html)\n",
    "    if not soup:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Extract description\n",
    "        description_elem = soup.find('article', class_='product_page').find('p', recursive=False)\n",
    "        description = description_elem.get_text() if description_elem else 'No description'\n",
    "        \n",
    "        # Extract product information table\n",
    "        table = soup.find('table', class_='table-striped')\n",
    "        product_info = {}\n",
    "        \n",
    "        if table:\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                header = row.find('th').get_text()\n",
    "                value = row.find('td').get_text()\n",
    "                product_info[header] = value\n",
    "        \n",
    "        # Extract category\n",
    "        breadcrumb = soup.find('ul', class_='breadcrumb')\n",
    "        category = breadcrumb.find_all('a')[2].get_text() if breadcrumb else 'Unknown'\n",
    "        \n",
    "        return {\n",
    "            'description': description,\n",
    "            'upc': product_info.get('UPC', ''),\n",
    "            'product_type': product_info.get('Product Type', ''),\n",
    "            'tax': product_info.get('Tax', ''),\n",
    "            'num_reviews': product_info.get('Number of reviews', 0),\n",
    "            'category': category\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping book details: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example: Scrape details for the first book\n",
    "if len(df_books) > 0:\n",
    "    first_book_url = df_books.iloc[0]['product_url']\n",
    "    print(f\"üîç Scraping detailed information...\\n\")\n",
    "    print(f\"URL: {first_book_url}\\n\")\n",
    "    \n",
    "    details = scrape_book_details(first_book_url)\n",
    "    \n",
    "    if details:\n",
    "        print(\"üìö Book Details:\\n\")\n",
    "        print(json.dumps(details, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Best Practices\n",
    "\n",
    "### What We Learned:\n",
    "1. ‚úÖ How to fetch HTML content using requests library\n",
    "2. ‚úÖ How to parse HTML with BeautifulSoup4\n",
    "3. ‚úÖ How to navigate DOM and extract data\n",
    "4. ‚úÖ How to handle pagination\n",
    "5. ‚úÖ How to clean and validate scraped data\n",
    "6. ‚úÖ How to save data in multiple formats\n",
    "7. ‚úÖ How to implement error handling and logging\n",
    "\n",
    "### Web Scraping Best Practices:\n",
    "\n",
    "#### Legal and Ethical:\n",
    "- ‚úÖ Always check robots.txt file\n",
    "- ‚úÖ Read and respect Terms of Service\n",
    "- ‚úÖ Only scrape publicly available data\n",
    "- ‚úÖ Use websites designed for scraping practice (like toscrape.com)\n",
    "\n",
    "#### Technical:\n",
    "- ‚úÖ Implement rate limiting (delays between requests)\n",
    "- ‚úÖ Use appropriate User-Agent headers\n",
    "- ‚úÖ Handle errors gracefully\n",
    "- ‚úÖ Validate and clean data\n",
    "- ‚úÖ Log all operations\n",
    "- ‚úÖ Cache responses when appropriate\n",
    "\n",
    "#### Code Quality:\n",
    "- ‚úÖ Write modular, reusable functions\n",
    "- ‚úÖ Use type hints\n",
    "- ‚úÖ Add comprehensive docstrings\n",
    "- ‚úÖ Follow PEP 8 style guidelines\n",
    "- ‚úÖ Implement proper error handling\n",
    "\n",
    "### Next Steps:\n",
    "1. **Apply to Your Project**: Use these techniques for quotes.toscrape.com\n",
    "2. **Add PySpark**: Process scraped data with PySpark DataFrames\n",
    "3. **Database Integration**: Store data in PostgreSQL/MongoDB\n",
    "4. **Automation**: Schedule scraping with cron jobs or Airflow\n",
    "5. **Advanced Scraping**: Learn Selenium for JavaScript-heavy sites\n",
    "\n",
    "### Resources:\n",
    "- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Requests Documentation](https://docs.python-requests.org/)\n",
    "- [Web Scraping Best Practices](https://www.scraperapi.com/blog/web-scraping-best-practices/)\n",
    "- [Practice Sites](http://toscrape.com/)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed this comprehensive web scraping tutorial. You now have the skills to:\n",
    "- Scrape data from websites\n",
    "- Parse and extract structured information\n",
    "- Handle pagination and navigation\n",
    "- Clean and validate data\n",
    "- Save data in various formats\n",
    "\n",
    "**Happy Scraping! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
