{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Test Notebook\n",
    "Test connection to Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/09 01:10:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Word Count Results ===\n",
      "spark: 6\n",
      "is: 2\n",
      "and: 1\n",
      "powerful: 1\n",
      "pyspark: 1\n",
      "makes: 1\n",
      "to: 1\n",
      "use: 1\n",
      "apache: 1\n",
      "amazing: 1\n",
      "fast: 1\n",
      "easy: 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple Word Count Example\n",
    "Counts word frequency in a text\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "text_data = [\n",
    "    \"Apache Spark is amazing\",\n",
    "    \"Spark is fast and powerful\",\n",
    "    \"PySpark makes Spark easy to use\",\n",
    "    \"Spark Spark Spark\"\n",
    "]\n",
    "\n",
    "# Create RDD and perform word count\n",
    "rdd = spark.sparkContext.parallelize(text_data)\n",
    "word_counts = rdd.flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word.lower(), 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"\\n=== Word Count Results ===\")\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Original DataFrame ===\n",
      "+-------+-----------+------+---+\n",
      "|   name| department|salary|age|\n",
      "+-------+-----------+------+---+\n",
      "|   John|      Sales|  5000| 28|\n",
      "|  Alice|Engineering|  8000| 32|\n",
      "|    Bob|      Sales|  6000| 35|\n",
      "|Charlie|Engineering|  9000| 29|\n",
      "|  Diana|         HR|  5500| 30|\n",
      "|    Eve|Engineering|  7500| 27|\n",
      "+-------+-----------+------+---+\n",
      "\n",
      "\n",
      "=== High Earners (Salary > 6000) ===\n",
      "+-------+-----------+------+---+\n",
      "|   name| department|salary|age|\n",
      "+-------+-----------+------+---+\n",
      "|  Alice|Engineering|  8000| 32|\n",
      "|Charlie|Engineering|  9000| 29|\n",
      "|    Eve|Engineering|  7500| 27|\n",
      "+-------+-----------+------+---+\n",
      "\n",
      "\n",
      "=== Names and Departments (Uppercase) ===\n",
      "+-------+-----------+\n",
      "|   name| department|\n",
      "+-------+-----------+\n",
      "|   JOHN|      Sales|\n",
      "|  ALICE|Engineering|\n",
      "|    BOB|      Sales|\n",
      "|CHARLIE|Engineering|\n",
      "|  DIANA|         HR|\n",
      "|    EVE|Engineering|\n",
      "+-------+-----------+\n",
      "\n",
      "\n",
      "=== Average Salary by Department ===\n",
      "+-----------+-----------------+\n",
      "| department|      avg(salary)|\n",
      "+-----------+-----------------+\n",
      "|      Sales|           5500.0|\n",
      "|Engineering|8166.666666666667|\n",
      "|         HR|           5500.0|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DataFrame Operations Example\n",
    "Filter, select, and transform data\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, upper\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrameOperations\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data - Employee records\n",
    "data = [\n",
    "    (\"John\", \"Sales\", 5000, 28),\n",
    "    (\"Alice\", \"Engineering\", 8000, 32),\n",
    "    (\"Bob\", \"Sales\", 6000, 35),\n",
    "    (\"Charlie\", \"Engineering\", 9000, 29),\n",
    "    (\"Diana\", \"HR\", 5500, 30),\n",
    "    (\"Eve\", \"Engineering\", 7500, 27)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"department\", \"salary\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"\\n=== Original DataFrame ===\")\n",
    "df.show()\n",
    "\n",
    "# Filter employees with salary > 6000\n",
    "print(\"\\n=== High Earners (Salary > 6000) ===\")\n",
    "df.filter(col(\"salary\") > 6000).show()\n",
    "\n",
    "# Select specific columns with transformation\n",
    "print(\"\\n=== Names and Departments (Uppercase) ===\")\n",
    "df.select(upper(col(\"name\")).alias(\"name\"), col(\"department\")).show()\n",
    "\n",
    "# Group by department and calculate average salary\n",
    "print(\"\\n=== Average Salary by Department ===\")\n",
    "df.groupBy(\"department\").avg(\"salary\").show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sales Data ===\n",
      "+--------+-----------+-----+--------+\n",
      "| product|   category|price|quantity|\n",
      "+--------+-----------+-----+--------+\n",
      "|  Laptop|Electronics| 1200|       5|\n",
      "|   Mouse|Electronics|   25|      50|\n",
      "|    Desk|  Furniture|  300|      10|\n",
      "|   Chair|  Furniture|  150|      20|\n",
      "|Keyboard|Electronics|   75|      30|\n",
      "| Monitor|Electronics|  400|      15|\n",
      "|   Table|  Furniture|  250|       8|\n",
      "+--------+-----------+-----+--------+\n",
      "\n",
      "\n",
      "=== Sales with Revenue ===\n",
      "+--------+-----------+-----+--------+-------+\n",
      "| product|   category|price|quantity|revenue|\n",
      "+--------+-----------+-----+--------+-------+\n",
      "|  Laptop|Electronics| 1200|       5|   6000|\n",
      "|   Mouse|Electronics|   25|      50|   1250|\n",
      "|    Desk|  Furniture|  300|      10|   3000|\n",
      "|   Chair|  Furniture|  150|      20|   3000|\n",
      "|Keyboard|Electronics|   75|      30|   2250|\n",
      "| Monitor|Electronics|  400|      15|   6000|\n",
      "|   Table|  Furniture|  250|       8|   2000|\n",
      "+--------+-----------+-----+--------+-------+\n",
      "\n",
      "\n",
      "=== Category Statistics ===\n",
      "+-----------+------------+-------------+------------------+------------+------------+\n",
      "|   category|num_products|total_revenue|         avg_price|max_quantity|min_quantity|\n",
      "+-----------+------------+-------------+------------------+------------+------------+\n",
      "|Electronics|           4|        15500|             425.0|          50|           5|\n",
      "|  Furniture|           3|         8000|233.33333333333334|          20|           8|\n",
      "+-----------+------------+-------------+------------------+------------+------------+\n",
      "\n",
      "\n",
      "=== Overall Statistics ===\n",
      "+-------------+------------------+--------------+\n",
      "|total_revenue|         avg_price|total_products|\n",
      "+-------------+------------------+--------------+\n",
      "|        23500|342.85714285714283|             7|\n",
      "+-------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Aggregation Example\n",
    "Statistical operations and aggregations\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, max, min, sum, count\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataAggregation\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data - Product sales\n",
    "sales_data = [\n",
    "    (\"Laptop\", \"Electronics\", 1200, 5),\n",
    "    (\"Mouse\", \"Electronics\", 25, 50),\n",
    "    (\"Desk\", \"Furniture\", 300, 10),\n",
    "    (\"Chair\", \"Furniture\", 150, 20),\n",
    "    (\"Keyboard\", \"Electronics\", 75, 30),\n",
    "    (\"Monitor\", \"Electronics\", 400, 15),\n",
    "    (\"Table\", \"Furniture\", 250, 8)\n",
    "]\n",
    "\n",
    "columns = [\"product\", \"category\", \"price\", \"quantity\"]\n",
    "df = spark.createDataFrame(sales_data, columns)\n",
    "\n",
    "print(\"\\n=== Sales Data ===\")\n",
    "df.show()\n",
    "\n",
    "# Calculate total revenue\n",
    "df_with_revenue = df.withColumn(\"revenue\", col(\"price\") * col(\"quantity\"))\n",
    "\n",
    "print(\"\\n=== Sales with Revenue ===\")\n",
    "df_with_revenue.show()\n",
    "\n",
    "# Aggregate statistics by category\n",
    "print(\"\\n=== Category Statistics ===\")\n",
    "category_stats = df_with_revenue.groupBy(\"category\").agg(\n",
    "    count(\"product\").alias(\"num_products\"),\n",
    "    sum(\"revenue\").alias(\"total_revenue\"),\n",
    "    avg(\"price\").alias(\"avg_price\"),\n",
    "    max(\"quantity\").alias(\"max_quantity\"),\n",
    "    min(\"quantity\").alias(\"min_quantity\")\n",
    ")\n",
    "category_stats.show()\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\n=== Overall Statistics ===\")\n",
    "overall_stats = df_with_revenue.agg(\n",
    "    sum(\"revenue\").alias(\"total_revenue\"),\n",
    "    avg(\"price\").alias(\"avg_price\"),\n",
    "    count(\"product\").alias(\"total_products\")\n",
    ")\n",
    "overall_stats.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Customers ===\n",
      "+-----------+-----------+-----------+\n",
      "|customer_id|       name|       city|\n",
      "+-----------+-----------+-----------+\n",
      "|          1|   John Doe|   New York|\n",
      "|          2| Jane Smith|Los Angeles|\n",
      "|          3|Bob Johnson|    Chicago|\n",
      "|          4|Alice Brown|    Houston|\n",
      "+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "=== Orders ===\n",
      "+--------+-----------+------+\n",
      "|order_id|customer_id|amount|\n",
      "+--------+-----------+------+\n",
      "|     101|          1| 250.0|\n",
      "|     102|          2| 150.0|\n",
      "|     103|          1| 300.0|\n",
      "|     104|          3| 200.0|\n",
      "|     105|          2| 450.0|\n",
      "|     106|          5| 100.0|\n",
      "+--------+-----------+------+\n",
      "\n",
      "\n",
      "=== Inner Join: Customers with Orders ===\n",
      "+-----------+-----------+-----------+--------+------+\n",
      "|customer_id|       name|       city|order_id|amount|\n",
      "+-----------+-----------+-----------+--------+------+\n",
      "|          1|   John Doe|   New York|     101| 250.0|\n",
      "|          1|   John Doe|   New York|     103| 300.0|\n",
      "|          2| Jane Smith|Los Angeles|     102| 150.0|\n",
      "|          2| Jane Smith|Los Angeles|     105| 450.0|\n",
      "|          3|Bob Johnson|    Chicago|     104| 200.0|\n",
      "+-----------+-----------+-----------+--------+------+\n",
      "\n",
      "\n",
      "=== Left Join: All Customers ===\n",
      "+-----------+-----------+-----------+--------+------+\n",
      "|customer_id|       name|       city|order_id|amount|\n",
      "+-----------+-----------+-----------+--------+------+\n",
      "|          1|   John Doe|   New York|     103| 300.0|\n",
      "|          1|   John Doe|   New York|     101| 250.0|\n",
      "|          2| Jane Smith|Los Angeles|     105| 450.0|\n",
      "|          2| Jane Smith|Los Angeles|     102| 150.0|\n",
      "|          3|Bob Johnson|    Chicago|     104| 200.0|\n",
      "|          4|Alice Brown|    Houston|    NULL|  NULL|\n",
      "+-----------+-----------+-----------+--------+------+\n",
      "\n",
      "\n",
      "=== Total Order Amount per Customer ===\n",
      "+-----------+-----------+-----------+------------+\n",
      "|customer_id|       name|       city|total_amount|\n",
      "+-----------+-----------+-----------+------------+\n",
      "|          1|   John Doe|   New York|       550.0|\n",
      "|          2| Jane Smith|Los Angeles|       600.0|\n",
      "|          3|Bob Johnson|    Chicago|       200.0|\n",
      "|          4|Alice Brown|    Houston|        NULL|\n",
      "+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Join Operations Example\n",
    "Combining multiple DataFrames\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JoinOperations\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data - Customers\n",
    "customers = [\n",
    "    (1, \"John Doe\", \"New York\"),\n",
    "    (2, \"Jane Smith\", \"Los Angeles\"),\n",
    "    (3, \"Bob Johnson\", \"Chicago\"),\n",
    "    (4, \"Alice Brown\", \"Houston\")\n",
    "]\n",
    "customer_df = spark.createDataFrame(customers, [\"customer_id\", \"name\", \"city\"])\n",
    "\n",
    "# Sample data - Orders\n",
    "orders = [\n",
    "    (101, 1, 250.00),\n",
    "    (102, 2, 150.00),\n",
    "    (103, 1, 300.00),\n",
    "    (104, 3, 200.00),\n",
    "    (105, 2, 450.00),\n",
    "    (106, 5, 100.00)  # customer_id 5 doesn't exist in customers\n",
    "]\n",
    "order_df = spark.createDataFrame(orders, [\"order_id\", \"customer_id\", \"amount\"])\n",
    "\n",
    "print(\"\\n=== Customers ===\")\n",
    "customer_df.show()\n",
    "\n",
    "print(\"\\n=== Orders ===\")\n",
    "order_df.show()\n",
    "\n",
    "# Inner Join - only matching records\n",
    "print(\"\\n=== Inner Join: Customers with Orders ===\")\n",
    "inner_join = customer_df.join(order_df, \"customer_id\", \"inner\")\n",
    "inner_join.show()\n",
    "\n",
    "# Left Join - all customers, with or without orders\n",
    "print(\"\\n=== Left Join: All Customers ===\")\n",
    "left_join = customer_df.join(order_df, \"customer_id\", \"left\")\n",
    "left_join.show()\n",
    "\n",
    "# Aggregate: Total amount per customer\n",
    "print(\"\\n=== Total Order Amount per Customer ===\")\n",
    "customer_totals = customer_df.join(order_df, \"customer_id\", \"left\") \\\n",
    "    .groupBy(\"customer_id\", \"name\", \"city\") \\\n",
    "    .agg({\"amount\": \"sum\"}) \\\n",
    "    .withColumnRenamed(\"sum(amount)\", \"total_amount\") \\\n",
    "    .orderBy(\"customer_id\")\n",
    "customer_totals.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Original Data ===\n",
      "+----------+---------+-----+--------+\n",
      "|      date|  product|price|quantity|\n",
      "+----------+---------+-----+--------+\n",
      "|2024-01-15|Product A|  100|      50|\n",
      "|2024-01-20|Product B|  150|      30|\n",
      "|2024-02-10|Product A|  120|      45|\n",
      "|2024-02-15|Product C|  200|      25|\n",
      "|2024-03-05|Product B|  180|      35|\n",
      "|2024-03-20|Product A|  110|      55|\n",
      "+----------+---------+-----+--------+\n",
      "\n",
      "\n",
      "=== Writing to /data/sales_data.csv ===\n",
      "Data written successfully!\n",
      "\n",
      "=== Reading from /data/sales_data.csv ===\n",
      "+----------+---------+-----+--------+\n",
      "|      date|  product|price|quantity|\n",
      "+----------+---------+-----+--------+\n",
      "|2024-01-20|Product B|  150|      30|\n",
      "|2024-02-10|Product A|  120|      45|\n",
      "|2024-03-05|Product B|  180|      35|\n",
      "|2024-03-20|Product A|  110|      55|\n",
      "|2024-01-15|Product A|  100|      50|\n",
      "|2024-02-15|Product C|  200|      25|\n",
      "+----------+---------+-----+--------+\n",
      "\n",
      "\n",
      "=== Processing and Writing Summary ===\n",
      "+---------+--------------+---------+\n",
      "|  product|total_quantity|avg_price|\n",
      "+---------+--------------+---------+\n",
      "|Product A|           150|    110.0|\n",
      "|Product B|            65|    165.0|\n",
      "|Product C|            25|    200.0|\n",
      "+---------+--------------+---------+\n",
      "\n",
      "Summary written to /data/sales_summary.csv\n",
      "\n",
      "=== Files Created ===\n",
      "1. /data/sales_data.csv/\n",
      "2. /data/sales_summary.csv/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reading and Writing CSV Files\n",
    "File I/O operations with PySpark\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadWriteCSV\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sample data\n",
    "sample_data = [\n",
    "    (\"2024-01-15\", \"Product A\", 100, 50),\n",
    "    (\"2024-01-20\", \"Product B\", 150, 30),\n",
    "    (\"2024-02-10\", \"Product A\", 120, 45),\n",
    "    (\"2024-02-15\", \"Product C\", 200, 25),\n",
    "    (\"2024-03-05\", \"Product B\", 180, 35),\n",
    "    (\"2024-03-20\", \"Product A\", 110, 55)\n",
    "]\n",
    "\n",
    "columns = [\"date\", \"product\", \"price\", \"quantity\"]\n",
    "df = spark.createDataFrame(sample_data, columns)\n",
    "\n",
    "print(\"\\n=== Original Data ===\")\n",
    "df.show()\n",
    "\n",
    "# Write to CSV\n",
    "output_path = \"/data/sales_data.csv\"\n",
    "print(f\"\\n=== Writing to {output_path} ===\")\n",
    "df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "print(\"Data written successfully!\")\n",
    "\n",
    "# Read from CSV\n",
    "print(f\"\\n=== Reading from {output_path} ===\")\n",
    "df_read = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(output_path)\n",
    "df_read.show()\n",
    "\n",
    "# Transform and write processed data\n",
    "print(\"\\n=== Processing and Writing Summary ===\")\n",
    "df_read_typed = df_read.withColumn(\"price\", col(\"price\").cast(\"int\")) \\\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(\"int\"))\n",
    "\n",
    "summary = df_read_typed.groupBy(\"product\").agg(\n",
    "    {\"price\": \"avg\", \"quantity\": \"sum\"}\n",
    ").withColumnRenamed(\"avg(price)\", \"avg_price\") \\\n",
    " .withColumnRenamed(\"sum(quantity)\", \"total_quantity\")\n",
    "\n",
    "summary.show()\n",
    "\n",
    "# Write summary\n",
    "summary_path = \"/data/sales_summary.csv\"\n",
    "summary.write.mode(\"overwrite\").option(\"header\", \"true\").csv(summary_path)\n",
    "print(f\"Summary written to {summary_path}\")\n",
    "\n",
    "# Show what files were created\n",
    "print(\"\\n=== Files Created ===\")\n",
    "print(\"1. /data/sales_data.csv/\")\n",
    "print(\"2. /data/sales_summary.csv/\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create Spark Session\u001b[39;00m\n\u001b[1;32m      8\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWordCount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Sample data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m text_data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApache Spark is amazing\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark is fast and powerful\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySpark makes Spark easy to use\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark Spark Spark\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple Word Count Example\n",
    "Counts word frequency in a text\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "text_data = [\n",
    "    \"Apache Spark is amazing\",\n",
    "    \"Spark is fast and powerful\",\n",
    "    \"PySpark makes Spark easy to use\",\n",
    "    \"Spark Spark Spark\"\n",
    "]\n",
    "\n",
    "# Create RDD and perform word count\n",
    "rdd = spark.sparkContext.parallelize(text_data)\n",
    "word_counts = rdd.flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word.lower(), 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"\\n=== Word Count Results ===\")\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWordCount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark://spark-master:7077\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.cores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    " spark = SparkSession.builder \\\n",
    "      .appName(\"WordCount\") \\\n",
    "      .master(\"spark://spark-master:7077\") \\\n",
    "      .config(\"spark.executor.memory\", \"1g\") \\\n",
    "      .config(\"spark.executor.cores\", \"1\") \\\n",
    "      .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
