{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Quick Start with Spark Cluster\n",
    "\n",
    "This notebook demonstrates how to use PySpark with a Spark cluster running in Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, explode, split, lower\n",
    "\n",
    "# Create Spark Session connected to the cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount Example\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.cores.max\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data\n",
    "text_data = [\n",
    "    \"Apache Spark is amazing\",\n",
    "    \"Spark is fast and powerful\",\n",
    "    \"PySpark makes Spark easy to use\",\n",
    "    \"Spark Spark Spark\"\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame([(line,) for line in text_data], [\"line\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count transformation\n",
    "word_counts = df.select(explode(split(lower(col(\"line\")), \" \")).alias(\"word\")) \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "word_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Working with Structured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "data = [\n",
    "    (\"Alice\", 34, \"Engineer\"),\n",
    "    (\"Bob\", 45, \"Manager\"),\n",
    "    (\"Charlie\", 28, \"Engineer\"),\n",
    "    (\"Diana\", 35, \"Analyst\"),\n",
    "    (\"Eve\", 29, \"Engineer\")\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"age\", \"role\"]\n",
    "employees_df = spark.createDataFrame(data, columns)\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and group operations\n",
    "print(\"Engineers:\")\n",
    "employees_df.filter(col(\"role\") == \"Engineer\").show()\n",
    "\n",
    "print(\"\\nCount by Role:\")\n",
    "employees_df.groupBy(\"role\").count().show()\n",
    "\n",
    "print(\"\\nAverage Age by Role:\")\n",
    "employees_df.groupBy(\"role\").avg(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Reading from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample CSV file\n",
    "csv_data = [\n",
    "    (\"product_1\", 100, 25.50),\n",
    "    (\"product_2\", 200, 15.75),\n",
    "    (\"product_3\", 150, 30.00),\n",
    "    (\"product_4\", 75, 45.25)\n",
    "]\n",
    "\n",
    "products_df = spark.createDataFrame(csv_data, [\"product_id\", \"quantity\", \"price\"])\n",
    "\n",
    "# Save as CSV\n",
    "products_df.write.mode(\"overwrite\").csv(\"/data/products.csv\", header=True)\n",
    "print(\"CSV file created at /data/products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "loaded_df = spark.read.csv(\"/data/products.csv\", header=True, inferSchema=True)\n",
    "loaded_df.show()\n",
    "loaded_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total value\n",
    "from pyspark.sql.functions import round as spark_round\n",
    "\n",
    "loaded_df.withColumn(\"total_value\", spark_round(col(\"quantity\") * col(\"price\"), 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Your Spark Job\n",
    "\n",
    "While running these cells, you can monitor your Spark jobs at:\n",
    "- **Spark Master UI**: http://localhost:8080\n",
    "- **Application UI**: http://localhost:4040 (when a job is running)\n",
    "- **Worker 1 UI**: http://localhost:8081\n",
    "- **Worker 2 UI**: http://localhost:8082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session when done\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
