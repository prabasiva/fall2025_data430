{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Advanced Examples\n",
    "## Master-Worker Cluster Setup\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Joining & Grouping** - 3 elaborative examples\n",
    "2. **Multidimensional DataFrames** - 3 elaborative examples\n",
    "3. **Nesting Columns** - 3 elaborative examples\n",
    "\n",
    "Running on a Spark cluster with 1 master and 2 worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n",
      "Master URL: spark://spark-master:7077\n",
      "Application ID: app-20251015150014-0005\n",
      "Default parallelism: 2\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession connected to the cluster\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Advanced Examples\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Master URL: {spark.sparkContext.master}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Default parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Joining & Grouping Examples\n",
    "\n",
    "This section demonstrates three comprehensive examples of joining and grouping operations in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1.1: E-Commerce Order Analysis with Multiple Joins and Aggregations\n",
    "\n",
    "Scenario: Analyze customer orders, products, and categories with complex joins and grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Customers ===\n",
      "+-----------+-------------+---------------+---------------+\n",
      "|customer_id|         name|          email|membership_tier|\n",
      "+-----------+-------------+---------------+---------------+\n",
      "|          1|Alice Johnson|alice@email.com|        Premium|\n",
      "|          2|    Bob Smith|  bob@email.com|       Standard|\n",
      "|          3|  Carol White|carol@email.com|        Premium|\n",
      "|          4|  David Brown|david@email.com|       Standard|\n",
      "|          5|    Eve Davis|  eve@email.com|        Premium|\n",
      "+-----------+-------------+---------------+---------------+\n",
      "\n",
      "\n",
      "=== Orders ===\n",
      "+--------+-----------+----------+------------+\n",
      "|order_id|customer_id|order_date|total_amount|\n",
      "+--------+-----------+----------+------------+\n",
      "|     101|          1|2024-01-15|       250.0|\n",
      "|     102|          1|2024-01-20|       180.0|\n",
      "|     103|          2|2024-01-18|       420.0|\n",
      "|     104|          3|2024-01-22|       310.0|\n",
      "|     105|          2|2024-01-25|       150.0|\n",
      "|     106|          4|2024-01-28|       275.0|\n",
      "|     107|          1|2024-02-01|       190.0|\n",
      "|     108|          5|2024-02-03|       500.0|\n",
      "+--------+-----------+----------+------------+\n",
      "\n",
      "\n",
      "=== Order Items ===\n",
      "+-------+--------+----------+--------+----------+\n",
      "|item_id|order_id|product_id|quantity|unit_price|\n",
      "+-------+--------+----------+--------+----------+\n",
      "|      1|     101|      1001|       2|      50.0|\n",
      "|      2|     101|      1002|       1|     150.0|\n",
      "|      3|     102|      1003|       3|      60.0|\n",
      "|      4|     103|      1001|       4|      50.0|\n",
      "|      5|     103|      1004|       2|     110.0|\n",
      "|      6|     104|      1002|       2|     155.0|\n",
      "|      7|     105|      1005|       1|     150.0|\n",
      "|      8|     106|      1003|       5|      55.0|\n",
      "|      9|     107|      1004|       1|     190.0|\n",
      "|     10|     108|      1001|      10|      50.0|\n",
      "+-------+--------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for e-commerce analysis\n",
    "customers_data = [\n",
    "    (1, \"Alice Johnson\", \"alice@email.com\", \"Premium\"),\n",
    "    (2, \"Bob Smith\", \"bob@email.com\", \"Standard\"),\n",
    "    (3, \"Carol White\", \"carol@email.com\", \"Premium\"),\n",
    "    (4, \"David Brown\", \"david@email.com\", \"Standard\"),\n",
    "    (5, \"Eve Davis\", \"eve@email.com\", \"Premium\")\n",
    "]\n",
    "\n",
    "orders_data = [\n",
    "    (101, 1, \"2024-01-15\", 250.00),\n",
    "    (102, 1, \"2024-01-20\", 180.00),\n",
    "    (103, 2, \"2024-01-18\", 420.00),\n",
    "    (104, 3, \"2024-01-22\", 310.00),\n",
    "    (105, 2, \"2024-01-25\", 150.00),\n",
    "    (106, 4, \"2024-01-28\", 275.00),\n",
    "    (107, 1, \"2024-02-01\", 190.00),\n",
    "    (108, 5, \"2024-02-03\", 500.00)\n",
    "]\n",
    "\n",
    "order_items_data = [\n",
    "    (1, 101, 1001, 2, 50.00),\n",
    "    (2, 101, 1002, 1, 150.00),\n",
    "    (3, 102, 1003, 3, 60.00),\n",
    "    (4, 103, 1001, 4, 50.00),\n",
    "    (5, 103, 1004, 2, 110.00),\n",
    "    (6, 104, 1002, 2, 155.00),\n",
    "    (7, 105, 1005, 1, 150.00),\n",
    "    (8, 106, 1003, 5, 55.00),\n",
    "    (9, 107, 1004, 1, 190.00),\n",
    "    (10, 108, 1001, 10, 50.00)\n",
    "]\n",
    "\n",
    "products_data = [\n",
    "    (1001, \"Wireless Mouse\", 101),\n",
    "    (1002, \"Mechanical Keyboard\", 101),\n",
    "    (1003, \"USB-C Cable\", 102),\n",
    "    (1004, \"Monitor Stand\", 103),\n",
    "    (1005, \"Laptop Bag\", 104)\n",
    "]\n",
    "\n",
    "categories_data = [\n",
    "    (101, \"Computer Accessories\"),\n",
    "    (102, \"Cables & Adapters\"),\n",
    "    (103, \"Furniture\"),\n",
    "    (104, \"Bags & Cases\")\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"name\", \"email\", \"membership_tier\"])\n",
    "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"])\n",
    "order_items_df = spark.createDataFrame(order_items_data, [\"item_id\", \"order_id\", \"product_id\", \"quantity\", \"unit_price\"])\n",
    "products_df = spark.createDataFrame(products_data, [\"product_id\", \"product_name\", \"category_id\"])\n",
    "categories_df = spark.createDataFrame(categories_data, [\"category_id\", \"category_name\"])\n",
    "\n",
    "print(\"\\n=== Customers ===\")\n",
    "customers_df.show()\n",
    "\n",
    "print(\"\\n=== Orders ===\")\n",
    "orders_df.show()\n",
    "\n",
    "print(\"\\n=== Order Items ===\")\n",
    "order_items_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Complete Order Details ===\n",
      "+--------+-------------+---------------+-------------------+--------------------+--------+----------+\n",
      "|order_id|name         |membership_tier|product_name       |category_name       |quantity|unit_price|\n",
      "+--------+-------------+---------------+-------------------+--------------------+--------+----------+\n",
      "|103     |Bob Smith    |Standard       |Monitor Stand      |Furniture           |2       |110.0     |\n",
      "|107     |Alice Johnson|Premium        |Monitor Stand      |Furniture           |1       |190.0     |\n",
      "|105     |Bob Smith    |Standard       |Laptop Bag         |Bags & Cases        |1       |150.0     |\n",
      "|104     |Carol White  |Premium        |Mechanical Keyboard|Computer Accessories|2       |155.0     |\n",
      "|101     |Alice Johnson|Premium        |Mechanical Keyboard|Computer Accessories|1       |150.0     |\n",
      "|108     |Eve Davis    |Premium        |Wireless Mouse     |Computer Accessories|10      |50.0      |\n",
      "|103     |Bob Smith    |Standard       |Wireless Mouse     |Computer Accessories|4       |50.0      |\n",
      "|101     |Alice Johnson|Premium        |Wireless Mouse     |Computer Accessories|2       |50.0      |\n",
      "|106     |David Brown  |Standard       |USB-C Cable        |Cables & Adapters   |5       |55.0      |\n",
      "|102     |Alice Johnson|Premium        |USB-C Cable        |Cables & Adapters   |3       |60.0      |\n",
      "+--------+-------------+---------------+-------------------+--------------------+--------+----------+\n",
      "\n",
      "\n",
      "=== Customer Analytics ===\n",
      "+-----------+-------------+---------------+------------+-----------+---------------+--------------------+\n",
      "|customer_id|         name|membership_tier|total_orders|total_spent|avg_order_value|categories_purchased|\n",
      "+-----------+-------------+---------------+------------+-----------+---------------+--------------------+\n",
      "|          1|Alice Johnson|        Premium|           4|      620.0|          155.0|                   3|\n",
      "|          2|    Bob Smith|       Standard|           3|      570.0|          190.0|                   3|\n",
      "|          5|    Eve Davis|        Premium|           1|      500.0|          500.0|                   1|\n",
      "|          3|  Carol White|        Premium|           1|      310.0|          310.0|                   1|\n",
      "|          4|  David Brown|       Standard|           1|      275.0|          275.0|                   1|\n",
      "+-----------+-------------+---------------+------------+-----------+---------------+--------------------+\n",
      "\n",
      "\n",
      "=== Category Analytics by Membership Tier ===\n",
      "+--------------------+---------------+-------------------+-------------+----------------+\n",
      "|category_name       |membership_tier|total_quantity_sold|total_revenue|unique_customers|\n",
      "+--------------------+---------------+-------------------+-------------+----------------+\n",
      "|Bags & Cases        |Standard       |1                  |150.0        |1               |\n",
      "|Cables & Adapters   |Premium        |3                  |180.0        |1               |\n",
      "|Cables & Adapters   |Standard       |5                  |275.0        |1               |\n",
      "|Computer Accessories|Premium        |15                 |1060.0       |3               |\n",
      "|Computer Accessories|Standard       |4                  |200.0        |1               |\n",
      "|Furniture           |Premium        |1                  |190.0        |1               |\n",
      "|Furniture           |Standard       |2                  |220.0        |1               |\n",
      "+--------------------+---------------+-------------------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform complex joins and aggregations\n",
    "# Join all tables to get complete order information\n",
    "complete_orders = order_items_df \\\n",
    "    .join(orders_df, \"order_id\") \\\n",
    "    .join(customers_df, \"customer_id\") \\\n",
    "    .join(products_df, \"product_id\") \\\n",
    "    .join(categories_df, \"category_id\")\n",
    "\n",
    "print(\"\\n=== Complete Order Details ===\")\n",
    "complete_orders.select(\n",
    "    \"order_id\", \"name\", \"membership_tier\", \n",
    "    \"product_name\", \"category_name\", \"quantity\", \"unit_price\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Group by customer and membership tier with aggregations\n",
    "customer_analytics = complete_orders.groupBy(\"customer_id\", \"name\", \"membership_tier\") \\\n",
    "    .agg(\n",
    "        count(\"order_id\").alias(\"total_orders\"),\n",
    "        sum(col(\"quantity\") * col(\"unit_price\")).alias(\"total_spent\"),\n",
    "        avg(col(\"quantity\") * col(\"unit_price\")).alias(\"avg_order_value\"),\n",
    "        countDistinct(\"category_name\").alias(\"categories_purchased\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_spent\"))\n",
    "\n",
    "print(\"\\n=== Customer Analytics ===\")\n",
    "customer_analytics.show()\n",
    "\n",
    "# Group by category with membership tier analysis\n",
    "category_analytics = complete_orders.groupBy(\"category_name\", \"membership_tier\") \\\n",
    "    .agg(\n",
    "        sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "        sum(col(\"quantity\") * col(\"unit_price\")).alias(\"total_revenue\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    ) \\\n",
    "    .orderBy(\"category_name\", \"membership_tier\")\n",
    "\n",
    "print(\"\\n=== Category Analytics by Membership Tier ===\")\n",
    "category_analytics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1.2: Employee Department Analysis with Self-Joins and Window Functions\n",
    "\n",
    "Scenario: Analyze employee hierarchy, salaries, and department performance using self-joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Employees ===\n",
      "+------+---------------+----------+-------+------+----------+\n",
      "|emp_id|       emp_name|manager_id|dept_id|salary| hire_date|\n",
      "+------+---------------+----------+-------+------+----------+\n",
      "|     1|       John CEO|      NULL|      1|250000|2020-01-01|\n",
      "|     2|   Sarah VP-Eng|         1|      1|180000|2020-02-01|\n",
      "|     3|  Mike VP-Sales|         1|      2|175000|2020-02-15|\n",
      "|     4|   Emma Manager|         2|      1|120000|2020-03-01|\n",
      "|     5| David Engineer|         4|      1| 95000|2021-01-15|\n",
      "|     6|  Lisa Engineer|         4|      1| 92000|2021-02-01|\n",
      "|     7|  Tom Sales Rep|         3|      2| 75000|2021-03-01|\n",
      "|     8| Anna Sales Rep|         3|      2| 78000|2021-04-01|\n",
      "|     9| Chris Engineer|         4|      1| 98000|2021-05-01|\n",
      "|    10|Nina HR Manager|         1|      3|110000|2020-06-01|\n",
      "+------+---------------+----------+-------+------+----------+\n",
      "\n",
      "\n",
      "=== Departments ===\n",
      "+-------+---------------+-----------------------+\n",
      "|dept_id|dept_name      |dept_description       |\n",
      "+-------+---------------+-----------------------+\n",
      "|1      |Engineering    |Building great products|\n",
      "|2      |Sales          |Revenue generation     |\n",
      "|3      |Human Resources|People management      |\n",
      "+-------+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create employee and department data\n",
    "employees_data = [\n",
    "    (1, \"John CEO\", None, 1, 250000, \"2020-01-01\"),\n",
    "    (2, \"Sarah VP-Eng\", 1, 1, 180000, \"2020-02-01\"),\n",
    "    (3, \"Mike VP-Sales\", 1, 2, 175000, \"2020-02-15\"),\n",
    "    (4, \"Emma Manager\", 2, 1, 120000, \"2020-03-01\"),\n",
    "    (5, \"David Engineer\", 4, 1, 95000, \"2021-01-15\"),\n",
    "    (6, \"Lisa Engineer\", 4, 1, 92000, \"2021-02-01\"),\n",
    "    (7, \"Tom Sales Rep\", 3, 2, 75000, \"2021-03-01\"),\n",
    "    (8, \"Anna Sales Rep\", 3, 2, 78000, \"2021-04-01\"),\n",
    "    (9, \"Chris Engineer\", 4, 1, 98000, \"2021-05-01\"),\n",
    "    (10, \"Nina HR Manager\", 1, 3, 110000, \"2020-06-01\")\n",
    "]\n",
    "\n",
    "departments_data = [\n",
    "    (1, \"Engineering\", \"Building great products\"),\n",
    "    (2, \"Sales\", \"Revenue generation\"),\n",
    "    (3, \"Human Resources\", \"People management\")\n",
    "]\n",
    "\n",
    "employees_df = spark.createDataFrame(\n",
    "    employees_data, \n",
    "    [\"emp_id\", \"emp_name\", \"manager_id\", \"dept_id\", \"salary\", \"hire_date\"]\n",
    ")\n",
    "\n",
    "departments_df = spark.createDataFrame(\n",
    "    departments_data, \n",
    "    [\"dept_id\", \"dept_name\", \"dept_description\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Employees ===\")\n",
    "employees_df.show()\n",
    "\n",
    "print(\"\\n=== Departments ===\")\n",
    "departments_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Employee-Manager Hierarchy ===\n",
      "+------+---------------+-------------+-------+------+\n",
      "|emp_id|emp_name       |manager_name |dept_id|salary|\n",
      "+------+---------------+-------------+-------+------+\n",
      "|3     |Mike VP-Sales  |John CEO     |2      |175000|\n",
      "|4     |Emma Manager   |Sarah VP-Eng |1      |120000|\n",
      "|10    |Nina HR Manager|John CEO     |3      |110000|\n",
      "|7     |Tom Sales Rep  |Mike VP-Sales|2      |75000 |\n",
      "|8     |Anna Sales Rep |Mike VP-Sales|2      |78000 |\n",
      "|9     |Chris Engineer |Emma Manager |1      |98000 |\n",
      "|1     |John CEO       |NULL         |1      |250000|\n",
      "|2     |Sarah VP-Eng   |John CEO     |1      |180000|\n",
      "|5     |David Engineer |Emma Manager |1      |95000 |\n",
      "|6     |Lisa Engineer  |Emma Manager |1      |92000 |\n",
      "+------+---------------+-------------+-------+------+\n",
      "\n",
      "\n",
      "=== Department Analytics ===\n",
      "+-------+---------------+--------------+-----------------+------------------+----------+----------+\n",
      "|dept_id|dept_name      |employee_count|total_salary_cost|avg_salary        |min_salary|max_salary|\n",
      "+-------+---------------+--------------+-----------------+------------------+----------+----------+\n",
      "|1      |Engineering    |6             |835000           |139166.66666666666|92000     |250000    |\n",
      "|2      |Sales          |3             |328000           |109333.33333333333|75000     |175000    |\n",
      "|3      |Human Resources|1             |110000           |110000.0          |110000    |110000    |\n",
      "+-------+---------------+--------------+-----------------+------------------+----------+----------+\n",
      "\n",
      "\n",
      "=== Salary Rankings Within Departments ===\n",
      "+---------------+---------------+------+------------+------------------+-----------------+\n",
      "|dept_name      |emp_name       |salary|rank_in_dept|dense_rank_in_dept|salary_percentile|\n",
      "+---------------+---------------+------+------------+------------------+-----------------+\n",
      "|Engineering    |John CEO       |250000|1           |1                 |0.0              |\n",
      "|Engineering    |Sarah VP-Eng   |180000|2           |2                 |0.2              |\n",
      "|Engineering    |Emma Manager   |120000|3           |3                 |0.4              |\n",
      "|Engineering    |Chris Engineer |98000 |4           |4                 |0.6              |\n",
      "|Engineering    |David Engineer |95000 |5           |5                 |0.8              |\n",
      "|Engineering    |Lisa Engineer  |92000 |6           |6                 |1.0              |\n",
      "|Human Resources|Nina HR Manager|110000|1           |1                 |0.0              |\n",
      "|Sales          |Mike VP-Sales  |175000|1           |1                 |0.0              |\n",
      "|Sales          |Anna Sales Rep |78000 |2           |2                 |0.5              |\n",
      "|Sales          |Tom Sales Rep  |75000 |3           |3                 |1.0              |\n",
      "+---------------+---------------+------+------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Self-join to get employee-manager relationship\n",
    "emp_with_manager = employees_df.alias(\"emp\") \\\n",
    "    .join(\n",
    "        employees_df.alias(\"mgr\"),\n",
    "        col(\"emp.manager_id\") == col(\"mgr.emp_id\"),\n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"emp.emp_id\"),\n",
    "        col(\"emp.emp_name\"),\n",
    "        col(\"mgr.emp_name\").alias(\"manager_name\"),\n",
    "        col(\"emp.dept_id\"),\n",
    "        col(\"emp.salary\")\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Employee-Manager Hierarchy ===\")\n",
    "emp_with_manager.show(truncate=False)\n",
    "\n",
    "# Join with departments and calculate department statistics\n",
    "dept_analytics = employees_df.join(departments_df, \"dept_id\") \\\n",
    "    .groupBy(\"dept_id\", \"dept_name\") \\\n",
    "    .agg(\n",
    "        count(\"emp_id\").alias(\"employee_count\"),\n",
    "        sum(\"salary\").alias(\"total_salary_cost\"),\n",
    "        avg(\"salary\").alias(\"avg_salary\"),\n",
    "        min(\"salary\").alias(\"min_salary\"),\n",
    "        max(\"salary\").alias(\"max_salary\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_salary_cost\"))\n",
    "\n",
    "print(\"\\n=== Department Analytics ===\")\n",
    "dept_analytics.show(truncate=False)\n",
    "\n",
    "# Use window functions with grouping for salary ranking within departments\n",
    "window_spec = Window.partitionBy(\"dept_id\").orderBy(desc(\"salary\"))\n",
    "\n",
    "salary_rankings = employees_df.join(departments_df, \"dept_id\") \\\n",
    "    .withColumn(\"rank_in_dept\", rank().over(window_spec)) \\\n",
    "    .withColumn(\"dense_rank_in_dept\", dense_rank().over(window_spec)) \\\n",
    "    .withColumn(\"salary_percentile\", percent_rank().over(window_spec)) \\\n",
    "    .select(\n",
    "        \"dept_name\", \"emp_name\", \"salary\", \n",
    "        \"rank_in_dept\", \"dense_rank_in_dept\", \n",
    "        round(\"salary_percentile\", 2).alias(\"salary_percentile\")\n",
    "    ) \\\n",
    "    .orderBy(\"dept_name\", \"rank_in_dept\")\n",
    "\n",
    "print(\"\\n=== Salary Rankings Within Departments ===\")\n",
    "salary_rankings.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1.3: Multi-Table Join with Complex Grouping - Student Course Enrollment\n",
    "\n",
    "Scenario: Analyze student enrollments, course performance, and instructor effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Students ===\n",
      "+----------+------------+----------------+---------+---+\n",
      "|student_id|student_name|           major|     year|gpa|\n",
      "+----------+------------+----------------+---------+---+\n",
      "|         1|       Alice|Computer Science|   Junior|3.8|\n",
      "|         2|         Bob|     Mathematics|   Senior|3.5|\n",
      "|         3|     Charlie|Computer Science|Sophomore|3.2|\n",
      "|         4|       Diana|         Physics|   Senior|3.9|\n",
      "|         5|         Eve|     Mathematics|   Junior|3.6|\n",
      "+----------+------------+----------------+---------+---+\n",
      "\n",
      "\n",
      "=== Courses ===\n",
      "+---------+-----------------+----------+-------+-------------+\n",
      "|course_id|course_name      |department|credits|instructor_id|\n",
      "+---------+-----------------+----------+-------+-------------+\n",
      "|101      |Data Structures  |CS        |4      |201          |\n",
      "|102      |Algorithms       |CS        |4      |201          |\n",
      "|103      |Calculus II      |MATH      |3      |202          |\n",
      "|104      |Linear Algebra   |MATH      |3      |202          |\n",
      "|105      |Quantum Mechanics|PHYS      |4      |203          |\n",
      "|106      |Database Systems |CS        |3      |204          |\n",
      "+---------+-----------------+----------+-------+-------------+\n",
      "\n",
      "\n",
      "=== Enrollments ===\n",
      "+----------+---------+---------+-----+-----+\n",
      "|student_id|course_id| semester|score|grade|\n",
      "+----------+---------+---------+-----+-----+\n",
      "|         1|      101|Fall 2024|   95|    A|\n",
      "|         1|      102|Fall 2024|   88|   B+|\n",
      "|         1|      106|Fall 2024|   92|   A-|\n",
      "|         2|      103|Fall 2024|   85|    B|\n",
      "|         2|      104|Fall 2024|   90|   A-|\n",
      "|         3|      101|Fall 2024|   78|   C+|\n",
      "|         3|      102|Fall 2024|   82|   B-|\n",
      "|         4|      105|Fall 2024|   96|    A|\n",
      "|         4|      103|Fall 2024|   93|    A|\n",
      "|         5|      104|Fall 2024|   87|   B+|\n",
      "|         5|      103|Fall 2024|   91|   A-|\n",
      "+----------+---------+---------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create student, course, and enrollment data\n",
    "students_data = [\n",
    "    (1, \"Alice\", \"Computer Science\", \"Junior\", 3.8),\n",
    "    (2, \"Bob\", \"Mathematics\", \"Senior\", 3.5),\n",
    "    (3, \"Charlie\", \"Computer Science\", \"Sophomore\", 3.2),\n",
    "    (4, \"Diana\", \"Physics\", \"Senior\", 3.9),\n",
    "    (5, \"Eve\", \"Mathematics\", \"Junior\", 3.6)\n",
    "]\n",
    "\n",
    "courses_data = [\n",
    "    (101, \"Data Structures\", \"CS\", 4, 201),\n",
    "    (102, \"Algorithms\", \"CS\", 4, 201),\n",
    "    (103, \"Calculus II\", \"MATH\", 3, 202),\n",
    "    (104, \"Linear Algebra\", \"MATH\", 3, 202),\n",
    "    (105, \"Quantum Mechanics\", \"PHYS\", 4, 203),\n",
    "    (106, \"Database Systems\", \"CS\", 3, 204)\n",
    "]\n",
    "\n",
    "instructors_data = [\n",
    "    (201, \"Dr. Smith\", \"Computer Science\", 15),\n",
    "    (202, \"Dr. Johnson\", \"Mathematics\", 20),\n",
    "    (203, \"Dr. Williams\", \"Physics\", 18),\n",
    "    (204, \"Dr. Brown\", \"Computer Science\", 10)\n",
    "]\n",
    "\n",
    "enrollments_data = [\n",
    "    (1, 101, \"Fall 2024\", 95, \"A\"),\n",
    "    (1, 102, \"Fall 2024\", 88, \"B+\"),\n",
    "    (1, 106, \"Fall 2024\", 92, \"A-\"),\n",
    "    (2, 103, \"Fall 2024\", 85, \"B\"),\n",
    "    (2, 104, \"Fall 2024\", 90, \"A-\"),\n",
    "    (3, 101, \"Fall 2024\", 78, \"C+\"),\n",
    "    (3, 102, \"Fall 2024\", 82, \"B-\"),\n",
    "    (4, 105, \"Fall 2024\", 96, \"A\"),\n",
    "    (4, 103, \"Fall 2024\", 93, \"A\"),\n",
    "    (5, 104, \"Fall 2024\", 87, \"B+\"),\n",
    "    (5, 103, \"Fall 2024\", 91, \"A-\")\n",
    "]\n",
    "\n",
    "students_df = spark.createDataFrame(\n",
    "    students_data, \n",
    "    [\"student_id\", \"student_name\", \"major\", \"year\", \"gpa\"]\n",
    ")\n",
    "\n",
    "courses_df = spark.createDataFrame(\n",
    "    courses_data, \n",
    "    [\"course_id\", \"course_name\", \"department\", \"credits\", \"instructor_id\"]\n",
    ")\n",
    "\n",
    "instructors_df = spark.createDataFrame(\n",
    "    instructors_data, \n",
    "    [\"instructor_id\", \"instructor_name\", \"department\", \"years_experience\"]\n",
    ")\n",
    "\n",
    "enrollments_df = spark.createDataFrame(\n",
    "    enrollments_data, \n",
    "    [\"student_id\", \"course_id\", \"semester\", \"score\", \"grade\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Students ===\")\n",
    "students_df.show()\n",
    "\n",
    "print(\"\\n=== Courses ===\")\n",
    "courses_df.show(truncate=False)\n",
    "\n",
    "print(\"\\n=== Enrollments ===\")\n",
    "enrollments_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Complete Enrollment Information ===\n",
      "+------------+----------------+-----------------+---------------+-----+-----+\n",
      "|student_name|major           |course_name      |instructor_name|score|grade|\n",
      "+------------+----------------+-----------------+---------------+-----+-----+\n",
      "|Charlie     |Computer Science|Data Structures  |Dr. Smith      |78   |C+   |\n",
      "|Alice       |Computer Science|Data Structures  |Dr. Smith      |95   |A    |\n",
      "|Charlie     |Computer Science|Algorithms       |Dr. Smith      |82   |B-   |\n",
      "|Alice       |Computer Science|Algorithms       |Dr. Smith      |88   |B+   |\n",
      "|Eve         |Mathematics     |Calculus II      |Dr. Johnson    |91   |A-   |\n",
      "|Diana       |Physics         |Calculus II      |Dr. Johnson    |93   |A    |\n",
      "|Bob         |Mathematics     |Calculus II      |Dr. Johnson    |85   |B    |\n",
      "|Eve         |Mathematics     |Linear Algebra   |Dr. Johnson    |87   |B+   |\n",
      "|Bob         |Mathematics     |Linear Algebra   |Dr. Johnson    |90   |A-   |\n",
      "|Diana       |Physics         |Quantum Mechanics|Dr. Williams   |96   |A    |\n",
      "|Alice       |Computer Science|Database Systems |Dr. Brown      |92   |A-   |\n",
      "+------------+----------------+-----------------+---------------+-----+-----+\n",
      "\n",
      "\n",
      "=== Student Performance Summary ===\n",
      "+----------+------------+----------------+---------+---+-------------+-------------+-----------------+--------------------+\n",
      "|student_id|student_name|major           |year     |gpa|courses_taken|total_credits|avg_score        |departments_explored|\n",
      "+----------+------------+----------------+---------+---+-------------+-------------+-----------------+--------------------+\n",
      "|4         |Diana       |Physics         |Senior   |3.9|2            |7            |94.5             |2                   |\n",
      "|1         |Alice       |Computer Science|Junior   |3.8|3            |11           |91.66666666666667|1                   |\n",
      "|5         |Eve         |Mathematics     |Junior   |3.6|2            |6            |89.0             |1                   |\n",
      "|2         |Bob         |Mathematics     |Senior   |3.5|2            |6            |87.5             |1                   |\n",
      "|3         |Charlie     |Computer Science|Sophomore|3.2|2            |8            |80.0             |1                   |\n",
      "+----------+------------+----------------+---------+---+-------------+-------------+-----------------+--------------------+\n",
      "\n",
      "\n",
      "=== Instructor Teaching Analytics ===\n",
      "+-------------+---------------+----------------+----------------+--------------+--------------+-----------------+--------------+------------------+\n",
      "|instructor_id|instructor_name|department      |years_experience|courses_taught|total_students|avg_student_score|a_grades_given|a_grade_percentage|\n",
      "+-------------+---------------+----------------+----------------+--------------+--------------+-----------------+--------------+------------------+\n",
      "|203          |Dr. Williams   |Physics         |18              |1             |1             |96.0             |1             |100.0             |\n",
      "|204          |Dr. Brown      |Computer Science|10              |1             |1             |92.0             |1             |100.0             |\n",
      "|202          |Dr. Johnson    |Mathematics     |20              |2             |5             |89.2             |3             |60.0              |\n",
      "|201          |Dr. Smith      |Computer Science|15              |2             |4             |85.75            |1             |25.0              |\n",
      "+-------------+---------------+----------------+----------------+--------------+--------------+-----------------+--------------+------------------+\n",
      "\n",
      "\n",
      "=== Cross-Department Analysis by Major ===\n",
      "+----------------+----------+----------------+---------+-------------+\n",
      "|major           |department|enrollment_count|avg_score|total_credits|\n",
      "+----------------+----------+----------------+---------+-------------+\n",
      "|Computer Science|CS        |5               |87.0     |19           |\n",
      "|Mathematics     |MATH      |4               |88.25    |12           |\n",
      "|Physics         |MATH      |1               |93.0     |3            |\n",
      "|Physics         |PHYS      |1               |96.0     |4            |\n",
      "+----------------+----------+----------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complex multi-table join with aliasing to avoid ambiguous columns\n",
    "complete_enrollments = enrollments_df \\\n",
    "    .join(students_df, \"student_id\") \\\n",
    "    .join(courses_df.alias(\"c\"), \"course_id\") \\\n",
    "    .join(instructors_df.alias(\"i\"), col(\"c.instructor_id\") == col(\"i.instructor_id\"))\n",
    "\n",
    "print(\"\\n=== Complete Enrollment Information ===\")\n",
    "complete_enrollments.select(\n",
    "    \"student_name\", \"major\", \"course_name\", \n",
    "    \"instructor_name\", \"score\", \"grade\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Group by student with performance metrics - using course department\n",
    "student_performance = complete_enrollments.groupBy(\n",
    "    \"student_id\", \"student_name\", \"major\", \"year\", \"gpa\"\n",
    ") \\\n",
    "    .agg(\n",
    "        count(\"course_id\").alias(\"courses_taken\"),\n",
    "        sum(\"credits\").alias(\"total_credits\"),\n",
    "        avg(\"score\").alias(\"avg_score\"),\n",
    "        countDistinct(col(\"c.department\")).alias(\"departments_explored\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"avg_score\"))\n",
    "\n",
    "print(\"\\n=== Student Performance Summary ===\")\n",
    "student_performance.show(truncate=False)\n",
    "\n",
    "# Group by instructor with teaching analytics - using instructor department\n",
    "instructor_analytics = complete_enrollments.groupBy(\n",
    "    col(\"i.instructor_id\"), \"instructor_name\", col(\"i.department\"), \"years_experience\"\n",
    ") \\\n",
    "    .agg(\n",
    "        countDistinct(\"course_id\").alias(\"courses_taught\"),\n",
    "        count(\"student_id\").alias(\"total_students\"),\n",
    "        avg(\"score\").alias(\"avg_student_score\"),\n",
    "        sum(when(col(\"grade\").isin([\"A\", \"A-\", \"A+\"]), 1).otherwise(0)).alias(\"a_grades_given\")\n",
    "    ) \\\n",
    "    .withColumn(\"a_grade_percentage\", round(col(\"a_grades_given\") / col(\"total_students\") * 100, 2)) \\\n",
    "    .orderBy(desc(\"avg_student_score\"))\n",
    "\n",
    "print(\"\\n=== Instructor Teaching Analytics ===\")\n",
    "instructor_analytics.show(truncate=False)\n",
    "\n",
    "# Group by department and major combination - using course department\n",
    "cross_dept_analysis = complete_enrollments.groupBy(\"major\", col(\"c.department\")) \\\n",
    "    .agg(\n",
    "        count(\"student_id\").alias(\"enrollment_count\"),\n",
    "        avg(\"score\").alias(\"avg_score\"),\n",
    "        sum(\"credits\").alias(\"total_credits\")\n",
    "    ) \\\n",
    "    .orderBy(\"major\", col(\"c.department\"))\n",
    "\n",
    "print(\"\\n=== Cross-Department Analysis by Major ===\")\n",
    "cross_dept_analysis.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Multidimensional DataFrames Examples\n",
    "\n",
    "This section demonstrates three comprehensive examples of working with multidimensional data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2.1: Sales Data Cube - Multi-Dimensional Analysis\n",
    "\n",
    "Scenario: Create a sales data cube with dimensions: product, region, time, and channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sales Data ===\n",
      "+-------+-------+-----------+------+-------+-------+----------+\n",
      "|month  |product|category   |region|channel|revenue|units_sold|\n",
      "+-------+-------+-----------+------+-------+-------+----------+\n",
      "|2024-01|Laptop |Electronics|North |Online |150000 |50        |\n",
      "|2024-01|Laptop |Electronics|South |Store  |120000 |40        |\n",
      "|2024-01|Phone  |Electronics|North |Online |200000 |100       |\n",
      "|2024-01|Shirt  |Clothing   |East  |Store  |30000  |150       |\n",
      "|2024-02|Laptop |Electronics|North |Online |180000 |60        |\n",
      "|2024-02|Phone  |Electronics|South |Online |220000 |110       |\n",
      "|2024-02|Shirt  |Clothing   |East  |Store  |35000  |175       |\n",
      "|2024-02|Pants  |Clothing   |West  |Store  |45000  |90        |\n",
      "|2024-03|Laptop |Electronics|North |Store  |160000 |55        |\n",
      "|2024-03|Phone  |Electronics|East  |Online |240000 |120       |\n",
      "|2024-03|Shirt  |Clothing   |South |Online |32000  |160       |\n",
      "|2024-03|Pants  |Clothing   |West  |Store  |48000  |96        |\n",
      "+-------+-------+-----------+------+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create multidimensional sales data\n",
    "sales_data = [\n",
    "    (\"2024-01\", \"Laptop\", \"Electronics\", \"North\", \"Online\", 150000, 50),\n",
    "    (\"2024-01\", \"Laptop\", \"Electronics\", \"South\", \"Store\", 120000, 40),\n",
    "    (\"2024-01\", \"Phone\", \"Electronics\", \"North\", \"Online\", 200000, 100),\n",
    "    (\"2024-01\", \"Shirt\", \"Clothing\", \"East\", \"Store\", 30000, 150),\n",
    "    (\"2024-02\", \"Laptop\", \"Electronics\", \"North\", \"Online\", 180000, 60),\n",
    "    (\"2024-02\", \"Phone\", \"Electronics\", \"South\", \"Online\", 220000, 110),\n",
    "    (\"2024-02\", \"Shirt\", \"Clothing\", \"East\", \"Store\", 35000, 175),\n",
    "    (\"2024-02\", \"Pants\", \"Clothing\", \"West\", \"Store\", 45000, 90),\n",
    "    (\"2024-03\", \"Laptop\", \"Electronics\", \"North\", \"Store\", 160000, 55),\n",
    "    (\"2024-03\", \"Phone\", \"Electronics\", \"East\", \"Online\", 240000, 120),\n",
    "    (\"2024-03\", \"Shirt\", \"Clothing\", \"South\", \"Online\", 32000, 160),\n",
    "    (\"2024-03\", \"Pants\", \"Clothing\", \"West\", \"Store\", 48000, 96)\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    [\"month\", \"product\", \"category\", \"region\", \"channel\", \"revenue\", \"units_sold\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Sales Data ===\")\n",
    "sales_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Time-based Rollup Analysis ===\n",
      "Shows aggregates at: (month, category), (month), and (grand total) levels\n",
      "+-------+-----------+-------------+-----------+-----------------+\n",
      "|month  |category   |total_revenue|total_units|transaction_count|\n",
      "+-------+-----------+-------------+-----------+-----------------+\n",
      "|NULL   |NULL       |1460000      |1206       |12               |\n",
      "|2024-01|NULL       |500000       |340        |4                |\n",
      "|2024-01|Clothing   |30000        |150        |1                |\n",
      "|2024-01|Electronics|470000       |190        |3                |\n",
      "|2024-02|NULL       |480000       |435        |4                |\n",
      "|2024-02|Clothing   |80000        |265        |2                |\n",
      "|2024-02|Electronics|400000       |170        |2                |\n",
      "|2024-03|NULL       |480000       |431        |4                |\n",
      "|2024-03|Clothing   |80000        |256        |2                |\n",
      "|2024-03|Electronics|400000       |175        |2                |\n",
      "+-------+-----------+-------------+-----------+-----------------+\n",
      "\n",
      "\n",
      "=== Geographic-Channel Cube Analysis ===\n",
      "Shows all possible combinations of region, channel, and category aggregations\n",
      "+------+-------+-----------+-------------+------------------+-----------+\n",
      "|region|channel|category   |total_revenue|avg_revenue       |total_units|\n",
      "+------+-------+-----------+-------------+------------------+-----------+\n",
      "|NULL  |NULL   |NULL       |1460000      |121666.66666666667|1206       |\n",
      "|NULL  |NULL   |Clothing   |190000       |38000.0           |671        |\n",
      "|NULL  |NULL   |Electronics|1270000      |181428.57142857142|535        |\n",
      "|NULL  |Online |NULL       |1022000      |170333.33333333334|600        |\n",
      "|NULL  |Online |Clothing   |32000        |32000.0           |160        |\n",
      "|NULL  |Online |Electronics|990000       |198000.0          |440        |\n",
      "|NULL  |Store  |NULL       |438000       |73000.0           |606        |\n",
      "|NULL  |Store  |Clothing   |158000       |39500.0           |511        |\n",
      "|NULL  |Store  |Electronics|280000       |140000.0          |95         |\n",
      "|East  |NULL   |NULL       |305000       |101666.66666666667|445        |\n",
      "|East  |NULL   |Clothing   |65000        |32500.0           |325        |\n",
      "|East  |NULL   |Electronics|240000       |240000.0          |120        |\n",
      "|East  |Online |NULL       |240000       |240000.0          |120        |\n",
      "|East  |Online |Electronics|240000       |240000.0          |120        |\n",
      "|East  |Store  |NULL       |65000        |32500.0           |325        |\n",
      "|East  |Store  |Clothing   |65000        |32500.0           |325        |\n",
      "|North |NULL   |NULL       |690000       |172500.0          |265        |\n",
      "|North |NULL   |Electronics|690000       |172500.0          |265        |\n",
      "|North |Online |NULL       |530000       |176666.66666666666|210        |\n",
      "|North |Online |Electronics|530000       |176666.66666666666|210        |\n",
      "|North |Store  |NULL       |160000       |160000.0          |55         |\n",
      "|North |Store  |Electronics|160000       |160000.0          |55         |\n",
      "|South |NULL   |NULL       |372000       |124000.0          |310        |\n",
      "|South |NULL   |Clothing   |32000        |32000.0           |160        |\n",
      "|South |NULL   |Electronics|340000       |170000.0          |150        |\n",
      "|South |Online |NULL       |252000       |126000.0          |270        |\n",
      "|South |Online |Clothing   |32000        |32000.0           |160        |\n",
      "|South |Online |Electronics|220000       |220000.0          |110        |\n",
      "|South |Store  |NULL       |120000       |120000.0          |40         |\n",
      "|South |Store  |Electronics|120000       |120000.0          |40         |\n",
      "|West  |NULL   |NULL       |93000        |46500.0           |186        |\n",
      "|West  |NULL   |Clothing   |93000        |46500.0           |186        |\n",
      "|West  |Store  |NULL       |93000        |46500.0           |186        |\n",
      "|West  |Store  |Clothing   |93000        |46500.0           |186        |\n",
      "+------+-------+-----------+-------------+------------------+-----------+\n",
      "\n",
      "\n",
      "=== Product-Region Pivot Table ===\n",
      "+-------+-----------+------+------+------+-----+\n",
      "|product|category   |North |South |East  |West |\n",
      "+-------+-----------+------+------+------+-----+\n",
      "|Laptop |Electronics|490000|120000|NULL  |NULL |\n",
      "|Pants  |Clothing   |NULL  |NULL  |NULL  |93000|\n",
      "|Shirt  |Clothing   |NULL  |32000 |65000 |NULL |\n",
      "|Phone  |Electronics|200000|220000|240000|NULL |\n",
      "+-------+-----------+------+------+------+-----+\n",
      "\n",
      "\n",
      "=== Time-Channel Multi-level Pivot ===\n",
      "+-------+--------------+------------+-------------+-----------+\n",
      "|month  |Online_revenue|Online_units|Store_revenue|Store_units|\n",
      "+-------+--------------+------------+-------------+-----------+\n",
      "|2024-02|400000        |170         |80000        |265        |\n",
      "|2024-03|272000        |280         |208000       |151        |\n",
      "|2024-01|350000        |150         |150000       |190        |\n",
      "+-------+--------------+------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dimension 1: Time-based analysis with rollup\n",
    "time_rollup = sales_df.rollup(\"month\", \"category\") \\\n",
    "    .agg(\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        sum(\"units_sold\").alias(\"total_units\"),\n",
    "        count(\"*\").alias(\"transaction_count\")\n",
    "    ) \\\n",
    "    .orderBy(\"month\", \"category\")\n",
    "\n",
    "print(\"\\n=== Time-based Rollup Analysis ===\")\n",
    "print(\"Shows aggregates at: (month, category), (month), and (grand total) levels\")\n",
    "time_rollup.show(truncate=False)\n",
    "\n",
    "# Dimension 2: Geographic and Channel cube analysis\n",
    "geo_channel_cube = sales_df.cube(\"region\", \"channel\", \"category\") \\\n",
    "    .agg(\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        avg(\"revenue\").alias(\"avg_revenue\"),\n",
    "        sum(\"units_sold\").alias(\"total_units\")\n",
    "    ) \\\n",
    "    .orderBy(\"region\", \"channel\", \"category\")\n",
    "\n",
    "print(\"\\n=== Geographic-Channel Cube Analysis ===\")\n",
    "print(\"Shows all possible combinations of region, channel, and category aggregations\")\n",
    "geo_channel_cube.show(50, truncate=False)\n",
    "\n",
    "# Dimension 3: Pivot table - Products across regions\n",
    "product_region_pivot = sales_df.groupBy(\"product\", \"category\") \\\n",
    "    .pivot(\"region\", [\"North\", \"South\", \"East\", \"West\"]) \\\n",
    "    .agg(sum(\"revenue\").alias(\"revenue\"))\n",
    "\n",
    "print(\"\\n=== Product-Region Pivot Table ===\")\n",
    "product_region_pivot.show(truncate=False)\n",
    "\n",
    "# Dimension 4: Multi-level pivot with time and channel\n",
    "time_channel_pivot = sales_df.groupBy(\"month\") \\\n",
    "    .pivot(\"channel\", [\"Online\", \"Store\"]) \\\n",
    "    .agg(\n",
    "        sum(\"revenue\").alias(\"revenue\"),\n",
    "        sum(\"units_sold\").alias(\"units\")\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Time-Channel Multi-level Pivot ===\")\n",
    "time_channel_pivot.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2.2: Multi-Dimensional Weather Data Analysis\n",
    "\n",
    "Scenario: Analyze weather patterns across multiple dimensions: location, time, and weather metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Weather Data ===\n",
      "+----------+--------+---------+-----------+--------+-------------+---------+------+\n",
      "|date      |city    |region   |temperature|humidity|precipitation|condition|season|\n",
      "+----------+--------+---------+-----------+--------+-------------+---------+------+\n",
      "|2024-01-01|New York|Northeast|32         |65      |0.1          |Cloudy   |Winter|\n",
      "|2024-01-01|Miami   |Southeast|75         |80      |0.0          |Sunny    |Winter|\n",
      "|2024-01-01|Seattle |Northwest|45         |85      |0.5          |Rainy    |Winter|\n",
      "|2024-01-01|Phoenix |Southwest|68         |30      |0.0          |Sunny    |Winter|\n",
      "|2024-04-01|New York|Northeast|58         |55      |0.2          |Rainy    |Spring|\n",
      "|2024-04-01|Miami   |Southeast|82         |75      |0.0          |Sunny    |Spring|\n",
      "|2024-04-01|Seattle |Northwest|52         |80      |0.4          |Cloudy   |Spring|\n",
      "|2024-04-01|Phoenix |Southwest|85         |25      |0.0          |Sunny    |Spring|\n",
      "|2024-07-01|New York|Northeast|85         |70      |0.0          |Sunny    |Summer|\n",
      "|2024-07-01|Miami   |Southeast|92         |85      |0.1          |Humid    |Summer|\n",
      "|2024-07-01|Seattle |Northwest|72         |60      |0.1          |Cloudy   |Summer|\n",
      "|2024-07-01|Phoenix |Southwest|108        |15      |0.0          |Sunny    |Summer|\n",
      "|2024-10-01|New York|Northeast|62         |60      |0.0          |Sunny    |Fall  |\n",
      "|2024-10-01|Miami   |Southeast|80         |78      |0.0          |Sunny    |Fall  |\n",
      "|2024-10-01|Seattle |Northwest|55         |75      |0.3          |Rainy    |Fall  |\n",
      "|2024-10-01|Phoenix |Southwest|88         |28      |0.0          |Sunny    |Fall  |\n",
      "+----------+--------+---------+-----------+--------+-------------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create multi-dimensional weather data\n",
    "weather_data = [\n",
    "    (\"2024-01-01\", \"New York\", \"Northeast\", 32, 65, 0.1, \"Cloudy\", \"Winter\"),\n",
    "    (\"2024-01-01\", \"Miami\", \"Southeast\", 75, 80, 0.0, \"Sunny\", \"Winter\"),\n",
    "    (\"2024-01-01\", \"Seattle\", \"Northwest\", 45, 85, 0.5, \"Rainy\", \"Winter\"),\n",
    "    (\"2024-01-01\", \"Phoenix\", \"Southwest\", 68, 30, 0.0, \"Sunny\", \"Winter\"),\n",
    "    (\"2024-04-01\", \"New York\", \"Northeast\", 58, 55, 0.2, \"Rainy\", \"Spring\"),\n",
    "    (\"2024-04-01\", \"Miami\", \"Southeast\", 82, 75, 0.0, \"Sunny\", \"Spring\"),\n",
    "    (\"2024-04-01\", \"Seattle\", \"Northwest\", 52, 80, 0.4, \"Cloudy\", \"Spring\"),\n",
    "    (\"2024-04-01\", \"Phoenix\", \"Southwest\", 85, 25, 0.0, \"Sunny\", \"Spring\"),\n",
    "    (\"2024-07-01\", \"New York\", \"Northeast\", 85, 70, 0.0, \"Sunny\", \"Summer\"),\n",
    "    (\"2024-07-01\", \"Miami\", \"Southeast\", 92, 85, 0.1, \"Humid\", \"Summer\"),\n",
    "    (\"2024-07-01\", \"Seattle\", \"Northwest\", 72, 60, 0.1, \"Cloudy\", \"Summer\"),\n",
    "    (\"2024-07-01\", \"Phoenix\", \"Southwest\", 108, 15, 0.0, \"Sunny\", \"Summer\"),\n",
    "    (\"2024-10-01\", \"New York\", \"Northeast\", 62, 60, 0.0, \"Sunny\", \"Fall\"),\n",
    "    (\"2024-10-01\", \"Miami\", \"Southeast\", 80, 78, 0.0, \"Sunny\", \"Fall\"),\n",
    "    (\"2024-10-01\", \"Seattle\", \"Northwest\", 55, 75, 0.3, \"Rainy\", \"Fall\"),\n",
    "    (\"2024-10-01\", \"Phoenix\", \"Southwest\", 88, 28, 0.0, \"Sunny\", \"Fall\")\n",
    "]\n",
    "\n",
    "weather_df = spark.createDataFrame(\n",
    "    weather_data,\n",
    "    [\"date\", \"city\", \"region\", \"temperature\", \"humidity\", \"precipitation\", \"condition\", \"season\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Weather Data ===\")\n",
    "weather_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Season-Region-Condition Cube Analysis ===\n",
      "+------+---------+---------+------------------+------------------+-------------------+-----------------+\n",
      "|season|region   |condition|avg_temp          |avg_humidity      |total_precipitation|observation_count|\n",
      "+------+---------+---------+------------------+------------------+-------------------+-----------------+\n",
      "|NULL  |NULL     |NULL     |71.1875           |60.375            |1.7000000000000002 |16               |\n",
      "|NULL  |NULL     |Cloudy   |52.0              |68.33333333333333 |0.6000000000000001 |3                |\n",
      "|NULL  |NULL     |Humid    |92.0              |85.0              |0.1                |1                |\n",
      "|NULL  |NULL     |Rainy    |52.666666666666664|71.66666666666667 |1.0                |3                |\n",
      "|NULL  |NULL     |Sunny    |81.44444444444444 |51.22222222222222 |0.0                |9                |\n",
      "|NULL  |Northeast|NULL     |59.25             |62.5              |0.30000000000000004|4                |\n",
      "|NULL  |Northeast|Cloudy   |32.0              |65.0              |0.1                |1                |\n",
      "|NULL  |Northeast|Rainy    |58.0              |55.0              |0.2                |1                |\n",
      "|NULL  |Northeast|Sunny    |73.5              |65.0              |0.0                |2                |\n",
      "|NULL  |Northwest|NULL     |56.0              |75.0              |1.3                |4                |\n",
      "|NULL  |Northwest|Cloudy   |62.0              |70.0              |0.5                |2                |\n",
      "|NULL  |Northwest|Rainy    |50.0              |80.0              |0.8                |2                |\n",
      "|NULL  |Southeast|NULL     |82.25             |79.5              |0.1                |4                |\n",
      "|NULL  |Southeast|Humid    |92.0              |85.0              |0.1                |1                |\n",
      "|NULL  |Southeast|Sunny    |79.0              |77.66666666666667 |0.0                |3                |\n",
      "|NULL  |Southwest|NULL     |87.25             |24.5              |0.0                |4                |\n",
      "|NULL  |Southwest|Sunny    |87.25             |24.5              |0.0                |4                |\n",
      "|Fall  |NULL     |NULL     |71.25             |60.25             |0.3                |4                |\n",
      "|Fall  |NULL     |Rainy    |55.0              |75.0              |0.3                |1                |\n",
      "|Fall  |NULL     |Sunny    |76.66666666666667 |55.333333333333336|0.0                |3                |\n",
      "|Fall  |Northeast|NULL     |62.0              |60.0              |0.0                |1                |\n",
      "|Fall  |Northeast|Sunny    |62.0              |60.0              |0.0                |1                |\n",
      "|Fall  |Northwest|NULL     |55.0              |75.0              |0.3                |1                |\n",
      "|Fall  |Northwest|Rainy    |55.0              |75.0              |0.3                |1                |\n",
      "|Fall  |Southeast|NULL     |80.0              |78.0              |0.0                |1                |\n",
      "|Fall  |Southeast|Sunny    |80.0              |78.0              |0.0                |1                |\n",
      "|Fall  |Southwest|NULL     |88.0              |28.0              |0.0                |1                |\n",
      "|Fall  |Southwest|Sunny    |88.0              |28.0              |0.0                |1                |\n",
      "|Spring|NULL     |NULL     |69.25             |58.75             |0.6000000000000001 |4                |\n",
      "|Spring|NULL     |Cloudy   |52.0              |80.0              |0.4                |1                |\n",
      "|Spring|NULL     |Rainy    |58.0              |55.0              |0.2                |1                |\n",
      "|Spring|NULL     |Sunny    |83.5              |50.0              |0.0                |2                |\n",
      "|Spring|Northeast|NULL     |58.0              |55.0              |0.2                |1                |\n",
      "|Spring|Northeast|Rainy    |58.0              |55.0              |0.2                |1                |\n",
      "|Spring|Northwest|NULL     |52.0              |80.0              |0.4                |1                |\n",
      "|Spring|Northwest|Cloudy   |52.0              |80.0              |0.4                |1                |\n",
      "|Spring|Southeast|NULL     |82.0              |75.0              |0.0                |1                |\n",
      "|Spring|Southeast|Sunny    |82.0              |75.0              |0.0                |1                |\n",
      "|Spring|Southwest|NULL     |85.0              |25.0              |0.0                |1                |\n",
      "|Spring|Southwest|Sunny    |85.0              |25.0              |0.0                |1                |\n",
      "|Summer|NULL     |NULL     |89.25             |57.5              |0.2                |4                |\n",
      "|Summer|NULL     |Cloudy   |72.0              |60.0              |0.1                |1                |\n",
      "|Summer|NULL     |Humid    |92.0              |85.0              |0.1                |1                |\n",
      "|Summer|NULL     |Sunny    |96.5              |42.5              |0.0                |2                |\n",
      "|Summer|Northeast|NULL     |85.0              |70.0              |0.0                |1                |\n",
      "|Summer|Northeast|Sunny    |85.0              |70.0              |0.0                |1                |\n",
      "|Summer|Northwest|NULL     |72.0              |60.0              |0.1                |1                |\n",
      "|Summer|Northwest|Cloudy   |72.0              |60.0              |0.1                |1                |\n",
      "|Summer|Southeast|NULL     |92.0              |85.0              |0.1                |1                |\n",
      "|Summer|Southeast|Humid    |92.0              |85.0              |0.1                |1                |\n",
      "+------+---------+---------+------------------+------------------+-------------------+-----------------+\n",
      "only showing top 50 rows\n",
      "\n",
      "\n",
      "=== Temperature Range Multi-dimensional Analysis ===\n",
      "+------+---------+----------+----------+------------+------------------+\n",
      "|season|region   |temp_range|days_count|avg_humidity|total_precip      |\n",
      "+------+---------+----------+----------+------------+------------------+\n",
      "|NULL  |NULL     |NULL      |16        |60.375      |1.7000000000000002|\n",
      "|Fall  |NULL     |NULL      |4         |60.25       |0.3               |\n",
      "|Fall  |Northeast|NULL      |1         |60.0        |0.0               |\n",
      "|Fall  |Northeast|Moderate  |1         |60.0        |0.0               |\n",
      "|Fall  |Northwest|NULL      |1         |75.0        |0.3               |\n",
      "|Fall  |Northwest|Moderate  |1         |75.0        |0.3               |\n",
      "|Fall  |Southeast|NULL      |1         |78.0        |0.0               |\n",
      "|Fall  |Southeast|Warm      |1         |78.0        |0.0               |\n",
      "|Fall  |Southwest|NULL      |1         |28.0        |0.0               |\n",
      "|Fall  |Southwest|Warm      |1         |28.0        |0.0               |\n",
      "|Spring|NULL     |NULL      |4         |58.75       |0.6000000000000001|\n",
      "|Spring|Northeast|NULL      |1         |55.0        |0.2               |\n",
      "|Spring|Northeast|Moderate  |1         |55.0        |0.2               |\n",
      "|Spring|Northwest|NULL      |1         |80.0        |0.4               |\n",
      "|Spring|Northwest|Moderate  |1         |80.0        |0.4               |\n",
      "|Spring|Southeast|NULL      |1         |75.0        |0.0               |\n",
      "|Spring|Southeast|Warm      |1         |75.0        |0.0               |\n",
      "|Spring|Southwest|NULL      |1         |25.0        |0.0               |\n",
      "|Spring|Southwest|Warm      |1         |25.0        |0.0               |\n",
      "|Summer|NULL     |NULL      |4         |57.5        |0.2               |\n",
      "|Summer|Northeast|NULL      |1         |70.0        |0.0               |\n",
      "|Summer|Northeast|Warm      |1         |70.0        |0.0               |\n",
      "|Summer|Northwest|NULL      |1         |60.0        |0.1               |\n",
      "|Summer|Northwest|Warm      |1         |60.0        |0.1               |\n",
      "|Summer|Southeast|NULL      |1         |85.0        |0.1               |\n",
      "|Summer|Southeast|Hot       |1         |85.0        |0.1               |\n",
      "|Summer|Southwest|NULL      |1         |15.0        |0.0               |\n",
      "|Summer|Southwest|Hot       |1         |15.0        |0.0               |\n",
      "|Winter|NULL     |NULL      |4         |65.0        |0.6               |\n",
      "|Winter|Northeast|NULL      |1         |65.0        |0.1               |\n",
      "|Winter|Northeast|Cold      |1         |65.0        |0.1               |\n",
      "|Winter|Northwest|NULL      |1         |85.0        |0.5               |\n",
      "|Winter|Northwest|Moderate  |1         |85.0        |0.5               |\n",
      "|Winter|Southeast|NULL      |1         |80.0        |0.0               |\n",
      "|Winter|Southeast|Warm      |1         |80.0        |0.0               |\n",
      "|Winter|Southwest|NULL      |1         |30.0        |0.0               |\n",
      "|Winter|Southwest|Moderate  |1         |30.0        |0.0               |\n",
      "+------+---------+----------+----------+------------+------------------+\n",
      "\n",
      "\n",
      "=== City-Season Pivot: Temperature and Humidity ===\n",
      "+--------+---------------+-------------------+---------------+-------------------+---------------+-------------------+-------------+-----------------+\n",
      "|city    |Winter_avg_temp|Winter_avg_humidity|Spring_avg_temp|Spring_avg_humidity|Summer_avg_temp|Summer_avg_humidity|Fall_avg_temp|Fall_avg_humidity|\n",
      "+--------+---------------+-------------------+---------------+-------------------+---------------+-------------------+-------------+-----------------+\n",
      "|Phoenix |68.0           |30.0               |85.0           |25.0               |108.0          |15.0               |88.0         |28.0             |\n",
      "|Seattle |45.0           |85.0               |52.0           |80.0               |72.0           |60.0               |55.0         |75.0             |\n",
      "|Miami   |75.0           |80.0               |82.0           |75.0               |92.0           |85.0               |80.0         |78.0             |\n",
      "|New York|32.0           |65.0               |58.0           |55.0               |85.0           |70.0               |62.0         |60.0             |\n",
      "+--------+---------------+-------------------+---------------+-------------------+---------------+-------------------+-------------+-----------------+\n",
      "\n",
      "\n",
      "=== Region-Condition Pivot Analysis ===\n",
      "+---------+------------+---------------+-----------+--------------+-----------+--------------+-----------+--------------+\n",
      "|region   |Cloudy_count|Cloudy_avg_temp|Humid_count|Humid_avg_temp|Rainy_count|Rainy_avg_temp|Sunny_count|Sunny_avg_temp|\n",
      "+---------+------------+---------------+-----------+--------------+-----------+--------------+-----------+--------------+\n",
      "|Northwest|2           |62.0           |NULL       |NULL          |2          |50.0          |NULL       |NULL          |\n",
      "|Southeast|NULL        |NULL           |1          |92.0          |NULL       |NULL          |3          |79.0          |\n",
      "|Southwest|NULL        |NULL           |NULL       |NULL          |NULL       |NULL          |4          |87.3          |\n",
      "|Northeast|1           |32.0           |NULL       |NULL          |1          |58.0          |2          |73.5          |\n",
      "+---------+------------+---------------+-----------+--------------+-----------+--------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-dimensional analysis 1: Season and Region Cube\n",
    "season_region_cube = weather_df.cube(\"season\", \"region\", \"condition\") \\\n",
    "    .agg(\n",
    "        avg(\"temperature\").alias(\"avg_temp\"),\n",
    "        avg(\"humidity\").alias(\"avg_humidity\"),\n",
    "        sum(\"precipitation\").alias(\"total_precipitation\"),\n",
    "        count(\"*\").alias(\"observation_count\")\n",
    "    ) \\\n",
    "    .orderBy(\"season\", \"region\", \"condition\")\n",
    "\n",
    "print(\"\\n=== Season-Region-Condition Cube Analysis ===\")\n",
    "season_region_cube.show(50, truncate=False)\n",
    "\n",
    "# Multi-dimensional analysis 2: Temperature ranges across dimensions\n",
    "weather_with_ranges = weather_df.withColumn(\n",
    "    \"temp_range\",\n",
    "    when(col(\"temperature\") < 40, \"Cold\")\n",
    "    .when(col(\"temperature\") < 70, \"Moderate\")\n",
    "    .when(col(\"temperature\") < 90, \"Warm\")\n",
    "    .otherwise(\"Hot\")\n",
    ")\n",
    "\n",
    "temp_range_analysis = weather_with_ranges.rollup(\"season\", \"region\", \"temp_range\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"days_count\"),\n",
    "        avg(\"humidity\").alias(\"avg_humidity\"),\n",
    "        sum(\"precipitation\").alias(\"total_precip\")\n",
    "    ) \\\n",
    "    .orderBy(\"season\", \"region\", \"temp_range\")\n",
    "\n",
    "print(\"\\n=== Temperature Range Multi-dimensional Analysis ===\")\n",
    "temp_range_analysis.show(50, truncate=False)\n",
    "\n",
    "# Multi-dimensional analysis 3: Pivot tables for different views\n",
    "city_season_pivot = weather_df.groupBy(\"city\") \\\n",
    "    .pivot(\"season\", [\"Winter\", \"Spring\", \"Summer\", \"Fall\"]) \\\n",
    "    .agg(\n",
    "        round(avg(\"temperature\"), 1).alias(\"avg_temp\"),\n",
    "        round(avg(\"humidity\"), 1).alias(\"avg_humidity\")\n",
    "    )\n",
    "\n",
    "print(\"\\n=== City-Season Pivot: Temperature and Humidity ===\")\n",
    "city_season_pivot.show(truncate=False)\n",
    "\n",
    "# Create a comprehensive pivot with regions and conditions\n",
    "region_condition_pivot = weather_df.groupBy(\"region\") \\\n",
    "    .pivot(\"condition\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        round(avg(\"temperature\"), 1).alias(\"avg_temp\")\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Region-Condition Pivot Analysis ===\")\n",
    "region_condition_pivot.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2.3: Financial Portfolio Multi-Dimensional Analysis\n",
    "\n",
    "Scenario: Analyze investment portfolio across asset types, sectors, time periods, and risk levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Portfolio Data ===\n",
      "+-------+--------+----------+--------------+------+----------+----------+-----------+\n",
      "|quarter|asset   |asset_type|sector        |value |return_pct|risk_level|category   |\n",
      "+-------+--------+----------+--------------+------+----------+----------+-----------+\n",
      "|2024-Q1|AAPL    |Stock     |Technology    |150000|12.5      |Medium    |Large Cap  |\n",
      "|2024-Q1|GOOGL   |Stock     |Technology    |120000|10.2      |Medium    |Large Cap  |\n",
      "|2024-Q1|JNJ     |Stock     |Healthcare    |80000 |5.8       |Low       |Large Cap  |\n",
      "|2024-Q1|BOND-1  |Bond      |Government    |50000 |3.2       |Low       |AAA        |\n",
      "|2024-Q1|REIT-1  |REIT      |Real Estate   |60000 |8.5       |Medium    |Diversified|\n",
      "|2024-Q2|AAPL    |Stock     |Technology    |165000|15.3      |Medium    |Large Cap  |\n",
      "|2024-Q2|GOOGL   |Stock     |Technology    |135000|13.7      |Medium    |Large Cap  |\n",
      "|2024-Q2|JNJ     |Stock     |Healthcare    |85000 |7.2       |Low       |Large Cap  |\n",
      "|2024-Q2|BOND-1  |Bond      |Government    |51500 |3.5       |Low       |AAA        |\n",
      "|2024-Q2|REIT-1  |REIT      |Real Estate   |63000 |9.1       |Medium    |Diversified|\n",
      "|2024-Q3|AAPL    |Stock     |Technology    |158000|-2.8      |Medium    |Large Cap  |\n",
      "|2024-Q3|GOOGL   |Stock     |Technology    |142000|8.5       |Medium    |Large Cap  |\n",
      "|2024-Q3|JNJ     |Stock     |Healthcare    |87000 |6.5       |Low       |Large Cap  |\n",
      "|2024-Q3|BOND-1  |Bond      |Government    |52000 |2.8       |Low       |AAA        |\n",
      "|2024-Q3|REIT-1  |REIT      |Real Estate   |65000 |7.8       |Medium    |Diversified|\n",
      "|2024-Q3|CRYPTO-1|Crypto    |Digital Assets|30000 |45.2      |High      |Volatile   |\n",
      "+-------+--------+----------+--------------+------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create multi-dimensional financial portfolio data\n",
    "portfolio_data = [\n",
    "    (\"2024-Q1\", \"AAPL\", \"Stock\", \"Technology\", 150000, 12.5, \"Medium\", \"Large Cap\"),\n",
    "    (\"2024-Q1\", \"GOOGL\", \"Stock\", \"Technology\", 120000, 10.2, \"Medium\", \"Large Cap\"),\n",
    "    (\"2024-Q1\", \"JNJ\", \"Stock\", \"Healthcare\", 80000, 5.8, \"Low\", \"Large Cap\"),\n",
    "    (\"2024-Q1\", \"BOND-1\", \"Bond\", \"Government\", 50000, 3.2, \"Low\", \"AAA\"),\n",
    "    (\"2024-Q1\", \"REIT-1\", \"REIT\", \"Real Estate\", 60000, 8.5, \"Medium\", \"Diversified\"),\n",
    "    (\"2024-Q2\", \"AAPL\", \"Stock\", \"Technology\", 165000, 15.3, \"Medium\", \"Large Cap\"),\n",
    "    (\"2024-Q2\", \"GOOGL\", \"Stock\", \"Technology\", 135000, 13.7, \"Medium\", \"Large Cap\"),\n",
    "    (\"2024-Q2\", \"JNJ\", \"Stock\", \"Healthcare\", 85000, 7.2, \"Low\", \"Large Cap\"),\n",
    "    (\"2024-Q2\", \"BOND-1\", \"Bond\", \"Government\", 51500, 3.5, \"Low\", \"AAA\"),\n",
    "    (\"2024-Q2\", \"REIT-1\", \"REIT\", \"Real Estate\", 63000, 9.1, \"Medium\", \"Diversified\"),\n",
    "    (\"2024-Q3\", \"AAPL\", \"Stock\", \"Technology\", 158000, -2.8, \"Medium\", \"Large Cap\"),\n",
    "    (\"2024-Q3\", \"GOOGL\", \"Stock\", \"Technology\", 142000, 8.5, \"Medium\", \"Large Cap\"),\n",
    "    (\"2024-Q3\", \"JNJ\", \"Stock\", \"Healthcare\", 87000, 6.5, \"Low\", \"Large Cap\"),\n",
    "    (\"2024-Q3\", \"BOND-1\", \"Bond\", \"Government\", 52000, 2.8, \"Low\", \"AAA\"),\n",
    "    (\"2024-Q3\", \"REIT-1\", \"REIT\", \"Real Estate\", 65000, 7.8, \"Medium\", \"Diversified\"),\n",
    "    (\"2024-Q3\", \"CRYPTO-1\", \"Crypto\", \"Digital Assets\", 30000, 45.2, \"High\", \"Volatile\")\n",
    "]\n",
    "\n",
    "portfolio_df = spark.createDataFrame(\n",
    "    portfolio_data,\n",
    "    [\"quarter\", \"asset\", \"asset_type\", \"sector\", \"value\", \"return_pct\", \"risk_level\", \"category\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Portfolio Data ===\")\n",
    "portfolio_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Complete Multi-Dimensional Cube ===\n",
      "Shows aggregations for all combinations of quarter, asset_type, sector, and risk_level\n",
      "+-------+----------+--------------+----------+-----------+------------------+-----------+------------------+\n",
      "|quarter|asset_type|sector        |risk_level|total_value|avg_return        |asset_count|return_volatility |\n",
      "+-------+----------+--------------+----------+-----------+------------------+-----------+------------------+\n",
      "|NULL   |NULL      |NULL          |NULL      |1493500    |9.8125            |16         |10.451786131247296|\n",
      "|NULL   |NULL      |NULL          |High      |30000      |45.2              |1          |NULL              |\n",
      "|NULL   |NULL      |NULL          |Low       |405500     |4.833333333333333 |6          |1.891736415747888 |\n",
      "|NULL   |NULL      |NULL          |Medium    |1058000    |9.2               |9          |5.199759609828131 |\n",
      "|NULL   |NULL      |Digital Assets|NULL      |30000      |45.2              |1          |NULL              |\n",
      "|NULL   |NULL      |Digital Assets|High      |30000      |45.2              |1          |NULL              |\n",
      "|NULL   |NULL      |Government    |NULL      |153500     |3.1666666666666665|3          |0.3511884584284248|\n",
      "|NULL   |NULL      |Government    |Low       |153500     |3.1666666666666665|3          |0.3511884584284248|\n",
      "|NULL   |NULL      |Healthcare    |NULL      |252000     |6.5               |3          |0.7000000000000002|\n",
      "|NULL   |NULL      |Healthcare    |Low       |252000     |6.5               |3          |0.7000000000000002|\n",
      "|NULL   |NULL      |Real Estate   |NULL      |188000     |8.466666666666667 |3          |0.6506407098647715|\n",
      "|NULL   |NULL      |Real Estate   |Medium    |188000     |8.466666666666667 |3          |0.6506407098647715|\n",
      "|NULL   |NULL      |Technology    |NULL      |870000     |9.566666666666666 |6          |6.527378238363904 |\n",
      "|NULL   |NULL      |Technology    |Medium    |870000     |9.566666666666666 |6          |6.527378238363904 |\n",
      "|NULL   |Bond      |NULL          |NULL      |153500     |3.1666666666666665|3          |0.3511884584284248|\n",
      "|NULL   |Bond      |NULL          |Low       |153500     |3.1666666666666665|3          |0.3511884584284248|\n",
      "|NULL   |Bond      |Government    |NULL      |153500     |3.1666666666666665|3          |0.3511884584284248|\n",
      "|NULL   |Bond      |Government    |Low       |153500     |3.1666666666666665|3          |0.3511884584284248|\n",
      "|NULL   |Crypto    |NULL          |NULL      |30000      |45.2              |1          |NULL              |\n",
      "|NULL   |Crypto    |NULL          |High      |30000      |45.2              |1          |NULL              |\n",
      "|NULL   |Crypto    |Digital Assets|NULL      |30000      |45.2              |1          |NULL              |\n",
      "|NULL   |Crypto    |Digital Assets|High      |30000      |45.2              |1          |NULL              |\n",
      "|NULL   |REIT      |NULL          |NULL      |188000     |8.466666666666667 |3          |0.6506407098647715|\n",
      "|NULL   |REIT      |NULL          |Medium    |188000     |8.466666666666667 |3          |0.6506407098647715|\n",
      "|NULL   |REIT      |Real Estate   |NULL      |188000     |8.466666666666667 |3          |0.6506407098647715|\n",
      "|NULL   |REIT      |Real Estate   |Medium    |188000     |8.466666666666667 |3          |0.6506407098647715|\n",
      "|NULL   |Stock     |NULL          |NULL      |1122000    |8.544444444444444 |9          |5.39469904422645  |\n",
      "|NULL   |Stock     |NULL          |Low       |252000     |6.5               |3          |0.7000000000000002|\n",
      "|NULL   |Stock     |NULL          |Medium    |870000     |9.566666666666666 |6          |6.527378238363904 |\n",
      "|NULL   |Stock     |Healthcare    |NULL      |252000     |6.5               |3          |0.7000000000000002|\n",
      "|NULL   |Stock     |Healthcare    |Low       |252000     |6.5               |3          |0.7000000000000002|\n",
      "|NULL   |Stock     |Technology    |NULL      |870000     |9.566666666666666 |6          |6.527378238363904 |\n",
      "|NULL   |Stock     |Technology    |Medium    |870000     |9.566666666666666 |6          |6.527378238363904 |\n",
      "|2024-Q1|NULL      |NULL          |NULL      |460000     |8.040000000000001 |5          |3.647327788943571 |\n",
      "|2024-Q1|NULL      |NULL          |Low       |130000     |4.5               |2          |1.8384776310850233|\n",
      "|2024-Q1|NULL      |NULL          |Medium    |330000     |10.4              |3          |2.0074859899884734|\n",
      "|2024-Q1|NULL      |Government    |NULL      |50000      |3.2               |1          |NULL              |\n",
      "|2024-Q1|NULL      |Government    |Low       |50000      |3.2               |1          |NULL              |\n",
      "|2024-Q1|NULL      |Healthcare    |NULL      |80000      |5.8               |1          |NULL              |\n",
      "|2024-Q1|NULL      |Healthcare    |Low       |80000      |5.8               |1          |NULL              |\n",
      "|2024-Q1|NULL      |Real Estate   |NULL      |60000      |8.5               |1          |NULL              |\n",
      "|2024-Q1|NULL      |Real Estate   |Medium    |60000      |8.5               |1          |NULL              |\n",
      "|2024-Q1|NULL      |Technology    |NULL      |270000     |11.35             |2          |1.6263455967290599|\n",
      "|2024-Q1|NULL      |Technology    |Medium    |270000     |11.35             |2          |1.6263455967290599|\n",
      "|2024-Q1|Bond      |NULL          |NULL      |50000      |3.2               |1          |NULL              |\n",
      "|2024-Q1|Bond      |NULL          |Low       |50000      |3.2               |1          |NULL              |\n",
      "|2024-Q1|Bond      |Government    |NULL      |50000      |3.2               |1          |NULL              |\n",
      "|2024-Q1|Bond      |Government    |Low       |50000      |3.2               |1          |NULL              |\n",
      "|2024-Q1|REIT      |NULL          |NULL      |60000      |8.5               |1          |NULL              |\n",
      "|2024-Q1|REIT      |NULL          |Medium    |60000      |8.5               |1          |NULL              |\n",
      "|2024-Q1|REIT      |Real Estate   |NULL      |60000      |8.5               |1          |NULL              |\n",
      "|2024-Q1|REIT      |Real Estate   |Medium    |60000      |8.5               |1          |NULL              |\n",
      "|2024-Q1|Stock     |NULL          |NULL      |350000     |9.5               |3          |3.4044089061098406|\n",
      "|2024-Q1|Stock     |NULL          |Low       |80000      |5.8               |1          |NULL              |\n",
      "|2024-Q1|Stock     |NULL          |Medium    |270000     |11.35             |2          |1.6263455967290599|\n",
      "|2024-Q1|Stock     |Healthcare    |NULL      |80000      |5.8               |1          |NULL              |\n",
      "|2024-Q1|Stock     |Healthcare    |Low       |80000      |5.8               |1          |NULL              |\n",
      "|2024-Q1|Stock     |Technology    |NULL      |270000     |11.35             |2          |1.6263455967290599|\n",
      "|2024-Q1|Stock     |Technology    |Medium    |270000     |11.35             |2          |1.6263455967290599|\n",
      "|2024-Q2|NULL      |NULL          |NULL      |499500     |9.760000000000002 |5          |4.806037869180808 |\n",
      "|2024-Q2|NULL      |NULL          |Low       |136500     |5.35              |2          |2.616295090390226 |\n",
      "|2024-Q2|NULL      |NULL          |Medium    |363000     |12.700000000000001|3          |3.2186953878862163|\n",
      "|2024-Q2|NULL      |Government    |NULL      |51500      |3.5               |1          |NULL              |\n",
      "|2024-Q2|NULL      |Government    |Low       |51500      |3.5               |1          |NULL              |\n",
      "|2024-Q2|NULL      |Healthcare    |NULL      |85000      |7.2               |1          |NULL              |\n",
      "|2024-Q2|NULL      |Healthcare    |Low       |85000      |7.2               |1          |NULL              |\n",
      "|2024-Q2|NULL      |Real Estate   |NULL      |63000      |9.1               |1          |NULL              |\n",
      "|2024-Q2|NULL      |Real Estate   |Medium    |63000      |9.1               |1          |NULL              |\n",
      "|2024-Q2|NULL      |Technology    |NULL      |300000     |14.5              |2          |1.1313708498984771|\n",
      "|2024-Q2|NULL      |Technology    |Medium    |300000     |14.5              |2          |1.1313708498984771|\n",
      "|2024-Q2|Bond      |NULL          |NULL      |51500      |3.5               |1          |NULL              |\n",
      "|2024-Q2|Bond      |NULL          |Low       |51500      |3.5               |1          |NULL              |\n",
      "|2024-Q2|Bond      |Government    |NULL      |51500      |3.5               |1          |NULL              |\n",
      "|2024-Q2|Bond      |Government    |Low       |51500      |3.5               |1          |NULL              |\n",
      "|2024-Q2|REIT      |NULL          |NULL      |63000      |9.1               |1          |NULL              |\n",
      "|2024-Q2|REIT      |NULL          |Medium    |63000      |9.1               |1          |NULL              |\n",
      "|2024-Q2|REIT      |Real Estate   |NULL      |63000      |9.1               |1          |NULL              |\n",
      "|2024-Q2|REIT      |Real Estate   |Medium    |63000      |9.1               |1          |NULL              |\n",
      "|2024-Q2|Stock     |NULL          |NULL      |385000     |12.066666666666668|3          |4.289910643980051 |\n",
      "|2024-Q2|Stock     |NULL          |Low       |85000      |7.2               |1          |NULL              |\n",
      "|2024-Q2|Stock     |NULL          |Medium    |300000     |14.5              |2          |1.1313708498984771|\n",
      "|2024-Q2|Stock     |Healthcare    |NULL      |85000      |7.2               |1          |NULL              |\n",
      "|2024-Q2|Stock     |Healthcare    |Low       |85000      |7.2               |1          |NULL              |\n",
      "|2024-Q2|Stock     |Technology    |NULL      |300000     |14.5              |2          |1.1313708498984771|\n",
      "|2024-Q2|Stock     |Technology    |Medium    |300000     |14.5              |2          |1.1313708498984771|\n",
      "|2024-Q3|NULL      |NULL          |NULL      |534000     |11.333333333333334|6          |17.10785394684753 |\n",
      "|2024-Q3|NULL      |NULL          |High      |30000      |45.2              |1          |NULL              |\n",
      "|2024-Q3|NULL      |NULL          |Low       |139000     |4.65              |2          |2.616295090390226 |\n",
      "|2024-Q3|NULL      |NULL          |Medium    |365000     |4.5               |3          |6.331666447310692 |\n",
      "|2024-Q3|NULL      |Digital Assets|NULL      |30000      |45.2              |1          |NULL              |\n",
      "|2024-Q3|NULL      |Digital Assets|High      |30000      |45.2              |1          |NULL              |\n",
      "|2024-Q3|NULL      |Government    |NULL      |52000      |2.8               |1          |NULL              |\n",
      "|2024-Q3|NULL      |Government    |Low       |52000      |2.8               |1          |NULL              |\n",
      "|2024-Q3|NULL      |Healthcare    |NULL      |87000      |6.5               |1          |NULL              |\n",
      "|2024-Q3|NULL      |Healthcare    |Low       |87000      |6.5               |1          |NULL              |\n",
      "|2024-Q3|NULL      |Real Estate   |NULL      |65000      |7.8               |1          |NULL              |\n",
      "|2024-Q3|NULL      |Real Estate   |Medium    |65000      |7.8               |1          |NULL              |\n",
      "|2024-Q3|NULL      |Technology    |NULL      |300000     |2.85              |2          |7.990306627407987 |\n",
      "|2024-Q3|NULL      |Technology    |Medium    |300000     |2.85              |2          |7.990306627407987 |\n",
      "|2024-Q3|Bond      |NULL          |NULL      |52000      |2.8               |1          |NULL              |\n",
      "+-------+----------+--------------+----------+-----------+------------------+-----------+------------------+\n",
      "only showing top 100 rows\n",
      "\n",
      "\n",
      "=== Hierarchical Rollup: Quarter -> Asset Type -> Sector ===\n",
      "+-------+----------+--------------+-----------+----------+-------+--------------+\n",
      "|quarter|asset_type|sector        |total_value|avg_return|std_dev|holdings_count|\n",
      "+-------+----------+--------------+-----------+----------+-------+--------------+\n",
      "|NULL   |NULL      |NULL          |1493500    |9.81      |10.45  |16            |\n",
      "|2024-Q1|NULL      |NULL          |460000     |8.04      |3.65   |5             |\n",
      "|2024-Q1|Bond      |NULL          |50000      |3.2       |NULL   |1             |\n",
      "|2024-Q1|Bond      |Government    |50000      |3.2       |NULL   |1             |\n",
      "|2024-Q1|REIT      |NULL          |60000      |8.5       |NULL   |1             |\n",
      "|2024-Q1|REIT      |Real Estate   |60000      |8.5       |NULL   |1             |\n",
      "|2024-Q1|Stock     |NULL          |350000     |9.5       |3.4    |3             |\n",
      "|2024-Q1|Stock     |Healthcare    |80000      |5.8       |NULL   |1             |\n",
      "|2024-Q1|Stock     |Technology    |270000     |11.35     |1.63   |2             |\n",
      "|2024-Q2|NULL      |NULL          |499500     |9.76      |4.81   |5             |\n",
      "|2024-Q2|Bond      |NULL          |51500      |3.5       |NULL   |1             |\n",
      "|2024-Q2|Bond      |Government    |51500      |3.5       |NULL   |1             |\n",
      "|2024-Q2|REIT      |NULL          |63000      |9.1       |NULL   |1             |\n",
      "|2024-Q2|REIT      |Real Estate   |63000      |9.1       |NULL   |1             |\n",
      "|2024-Q2|Stock     |NULL          |385000     |12.07     |4.29   |3             |\n",
      "|2024-Q2|Stock     |Healthcare    |85000      |7.2       |NULL   |1             |\n",
      "|2024-Q2|Stock     |Technology    |300000     |14.5      |1.13   |2             |\n",
      "|2024-Q3|NULL      |NULL          |534000     |11.33     |17.11  |6             |\n",
      "|2024-Q3|Bond      |NULL          |52000      |2.8       |NULL   |1             |\n",
      "|2024-Q3|Bond      |Government    |52000      |2.8       |NULL   |1             |\n",
      "|2024-Q3|Crypto    |NULL          |30000      |45.2      |NULL   |1             |\n",
      "|2024-Q3|Crypto    |Digital Assets|30000      |45.2      |NULL   |1             |\n",
      "|2024-Q3|REIT      |NULL          |65000      |7.8       |NULL   |1             |\n",
      "|2024-Q3|REIT      |Real Estate   |65000      |7.8       |NULL   |1             |\n",
      "|2024-Q3|Stock     |NULL          |387000     |4.07      |6.03   |3             |\n",
      "|2024-Q3|Stock     |Healthcare    |87000      |6.5       |NULL   |1             |\n",
      "|2024-Q3|Stock     |Technology    |300000     |2.85      |7.99   |2             |\n",
      "+-------+----------+--------------+-----------+----------+-------+--------------+\n",
      "\n",
      "\n",
      "=== Risk-Return Matrix Across Quarters ===\n",
      "+----------+----------+------------------+-------------------+------------------+-------------------+------------------+-------------------+\n",
      "|risk_level|asset_type|2024-Q1_avg_return|2024-Q1_total_value|2024-Q2_avg_return|2024-Q2_total_value|2024-Q3_avg_return|2024-Q3_total_value|\n",
      "+----------+----------+------------------+-------------------+------------------+-------------------+------------------+-------------------+\n",
      "|High      |Crypto    |NULL              |NULL               |NULL              |NULL               |45.2              |30000              |\n",
      "|Low       |Bond      |3.2               |50000              |3.5               |51500              |2.8               |52000              |\n",
      "|Low       |Stock     |5.8               |80000              |7.2               |85000              |6.5               |87000              |\n",
      "|Medium    |REIT      |8.5               |60000              |9.1               |63000              |7.8               |65000              |\n",
      "|Medium    |Stock     |11.35             |270000             |14.5              |300000             |2.85              |300000             |\n",
      "+----------+----------+------------------+-------------------+------------------+-------------------+------------------+-------------------+\n",
      "\n",
      "\n",
      "=== Performance Classification by Sector ===\n",
      "+--------------+----------+----------------+----------------+----------------------+-------------------+-------------------------+-----------------+-----------------------+\n",
      "|sector        |Loss_count|Loss_total_value|Low Growth_count|Low Growth_total_value|Medium Growth_count|Medium Growth_total_value|High Growth_count|High Growth_total_value|\n",
      "+--------------+----------+----------------+----------------+----------------------+-------------------+-------------------------+-----------------+-----------------------+\n",
      "|Healthcare    |NULL      |NULL            |NULL            |NULL                  |3                  |252000                   |NULL             |NULL                   |\n",
      "|Government    |NULL      |NULL            |3               |153500                |NULL               |NULL                     |NULL             |NULL                   |\n",
      "|Real Estate   |NULL      |NULL            |NULL            |NULL                  |3                  |188000                   |NULL             |NULL                   |\n",
      "|Digital Assets|NULL      |NULL            |NULL            |NULL                  |NULL               |NULL                     |1                |30000                  |\n",
      "|Technology    |1         |158000          |NULL            |NULL                  |4                  |547000                   |1                |165000                 |\n",
      "+--------------+----------+----------------+----------------+----------------------+-------------------+-------------------------+-----------------+-----------------------+\n",
      "\n",
      "\n",
      "=== Overall Portfolio Metrics by Quarter ===\n",
      "+-------+---------------------+-------------------+--------------------+----------------+--------------------+\n",
      "|quarter|total_portfolio_value|weighted_avg_return|asset_type_diversity|sector_diversity|portfolio_volatility|\n",
      "+-------+---------------------+-------------------+--------------------+----------------+--------------------+\n",
      "|2024-Q1|460000               |8.04               |3                   |4               |3.65                |\n",
      "|2024-Q2|499500               |9.76               |3                   |4               |4.81                |\n",
      "|2024-Q3|534000               |11.33              |4                   |5               |17.11               |\n",
      "+-------+---------------------+-------------------+--------------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-dimensional analysis 1: Complete Cube across all dimensions\n",
    "complete_cube = portfolio_df.cube(\"quarter\", \"asset_type\", \"sector\", \"risk_level\") \\\n",
    "    .agg(\n",
    "        sum(\"value\").alias(\"total_value\"),\n",
    "        avg(\"return_pct\").alias(\"avg_return\"),\n",
    "        count(\"asset\").alias(\"asset_count\"),\n",
    "        stddev(\"return_pct\").alias(\"return_volatility\")\n",
    "    ) \\\n",
    "    .orderBy(\"quarter\", \"asset_type\", \"sector\", \"risk_level\")\n",
    "\n",
    "print(\"\\n=== Complete Multi-Dimensional Cube ===\")\n",
    "print(\"Shows aggregations for all combinations of quarter, asset_type, sector, and risk_level\")\n",
    "complete_cube.show(100, truncate=False)\n",
    "\n",
    "# Multi-dimensional analysis 2: Hierarchical Rollup\n",
    "hierarchical_rollup = portfolio_df.rollup(\"quarter\", \"asset_type\", \"sector\") \\\n",
    "    .agg(\n",
    "        sum(\"value\").alias(\"total_value\"),\n",
    "        round(avg(\"return_pct\"), 2).alias(\"avg_return\"),\n",
    "        round(stddev(\"return_pct\"), 2).alias(\"std_dev\"),\n",
    "        count(\"*\").alias(\"holdings_count\")\n",
    "    ) \\\n",
    "    .orderBy(\"quarter\", \"asset_type\", \"sector\")\n",
    "\n",
    "print(\"\\n=== Hierarchical Rollup: Quarter -> Asset Type -> Sector ===\")\n",
    "hierarchical_rollup.show(50, truncate=False)\n",
    "\n",
    "# Multi-dimensional analysis 3: Risk-Return Matrix Pivot\n",
    "risk_return_pivot = portfolio_df.groupBy(\"risk_level\", \"asset_type\") \\\n",
    "    .pivot(\"quarter\") \\\n",
    "    .agg(\n",
    "        round(avg(\"return_pct\"), 2).alias(\"avg_return\"),\n",
    "        sum(\"value\").alias(\"total_value\")\n",
    "    ) \\\n",
    "    .orderBy(\"risk_level\", \"asset_type\")\n",
    "\n",
    "print(\"\\n=== Risk-Return Matrix Across Quarters ===\")\n",
    "risk_return_pivot.show(truncate=False)\n",
    "\n",
    "# Multi-dimensional analysis 4: Performance classification pivot\n",
    "performance_classification = portfolio_df.withColumn(\n",
    "    \"performance\",\n",
    "    when(col(\"return_pct\") < 0, \"Loss\")\n",
    "    .when(col(\"return_pct\") < 5, \"Low Growth\")\n",
    "    .when(col(\"return_pct\") < 15, \"Medium Growth\")\n",
    "    .otherwise(\"High Growth\")\n",
    ")\n",
    "\n",
    "performance_pivot = performance_classification.groupBy(\"sector\") \\\n",
    "    .pivot(\"performance\", [\"Loss\", \"Low Growth\", \"Medium Growth\", \"High Growth\"]) \\\n",
    "    .agg(\n",
    "        count(\"asset\").alias(\"count\"),\n",
    "        sum(\"value\").alias(\"total_value\")\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Performance Classification by Sector ===\")\n",
    "performance_pivot.show(truncate=False)\n",
    "\n",
    "# Calculate portfolio metrics across multiple dimensions\n",
    "portfolio_metrics = portfolio_df.groupBy(\"quarter\") \\\n",
    "    .agg(\n",
    "        sum(\"value\").alias(\"total_portfolio_value\"),\n",
    "        round(avg(\"return_pct\"), 2).alias(\"weighted_avg_return\"),\n",
    "        countDistinct(\"asset_type\").alias(\"asset_type_diversity\"),\n",
    "        countDistinct(\"sector\").alias(\"sector_diversity\"),\n",
    "        round(stddev(\"return_pct\"), 2).alias(\"portfolio_volatility\")\n",
    "    ) \\\n",
    "    .orderBy(\"quarter\")\n",
    "\n",
    "print(\"\\n=== Overall Portfolio Metrics by Quarter ===\")\n",
    "portfolio_metrics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Nested Columns Examples\n",
    "\n",
    "This section demonstrates three comprehensive examples of working with nested and complex data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3.1: E-Commerce Orders with Nested Structures\n",
    "\n",
    "Scenario: Work with deeply nested order data including customer info, shipping address, and order items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Nested Orders Schema ===\n",
      "root\n",
      " |-- order_id: integer (nullable = false)\n",
      " |-- order_date: string (nullable = false)\n",
      " |-- customer: struct (nullable = false)\n",
      " |    |-- customer_id: integer (nullable = false)\n",
      " |    |-- name: string (nullable = false)\n",
      " |    |-- email: string (nullable = false)\n",
      " |    |-- membership: struct (nullable = false)\n",
      " |    |    |-- tier: string (nullable = false)\n",
      " |    |    |-- since: string (nullable = false)\n",
      " |    |    |-- points: integer (nullable = false)\n",
      " |-- shipping_address: struct (nullable = false)\n",
      " |    |-- street: string (nullable = false)\n",
      " |    |-- city: string (nullable = false)\n",
      " |    |-- state: string (nullable = false)\n",
      " |    |-- zip: string (nullable = false)\n",
      " |    |-- coordinates: struct (nullable = false)\n",
      " |    |    |-- lat: double (nullable = false)\n",
      " |    |    |-- lon: double (nullable = false)\n",
      " |-- items: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- product_id: integer (nullable = false)\n",
      " |    |    |-- name: string (nullable = false)\n",
      " |    |    |-- quantity: integer (nullable = false)\n",
      " |    |    |-- price: double (nullable = false)\n",
      " |    |    |-- category: string (nullable = false)\n",
      " |-- payment: struct (nullable = false)\n",
      " |    |-- method: string (nullable = false)\n",
      " |    |-- last_four: string (nullable = true)\n",
      " |    |-- amount: double (nullable = false)\n",
      "\n",
      "\n",
      "=== Nested Orders Data ===\n",
      "+--------+----------+-------------------------------------------------------------+-----------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+---------------------------+\n",
      "|order_id|order_date|customer                                                     |shipping_address                                           |items                                                                                                               |payment                    |\n",
      "+--------+----------+-------------------------------------------------------------+-----------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+---------------------------+\n",
      "|1001    |2024-01-15|{1, John Doe, john@email.com, {Gold, 2020-01-01, 1500}}      |{123 Main St, New York, NY, 10001, {40.7128, -74.006}}     |[{101, Laptop, 1, 1200.0, Electronics}, {102, Mouse, 2, 25.0, Accessories}]                                         |{Credit Card, 4242, 1250.0}|\n",
      "|1002    |2024-01-16|{2, Jane Smith, jane@email.com, {Silver, 2021-06-15, 750}}   |{456 Oak Ave, Los Angeles, CA, 90001, {34.0522, -118.2437}}|[{201, Desk Chair, 1, 350.0, Furniture}, {202, Desk Lamp, 1, 45.0, Lighting}, {203, Notebook, 5, 3.0, Stationery}]  |{PayPal, NULL, 410.0}      |\n",
      "|1003    |2024-01-17|{3, Bob Johnson, bob@email.com, {Platinum, 2018-03-20, 3200}}|{789 Pine Rd, Chicago, IL, 60601, {41.8781, -87.6298}}     |[{301, Monitor, 2, 400.0, Electronics}, {302, Keyboard, 2, 120.0, Electronics}, {303, Webcam, 1, 85.0, Electronics}]|{Credit Card, 8888, 1125.0}|\n",
      "+--------+----------+-------------------------------------------------------------+-----------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define explicit schema for nested e-commerce data\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    "\n",
    "# Define the schema explicitly\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"order_date\", StringType(), False),\n",
    "    StructField(\"customer\", StructType([\n",
    "        StructField(\"customer_id\", IntegerType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"email\", StringType(), False),\n",
    "        StructField(\"membership\", StructType([\n",
    "            StructField(\"tier\", StringType(), False),\n",
    "            StructField(\"since\", StringType(), False),\n",
    "            StructField(\"points\", IntegerType(), False)\n",
    "        ]), False)\n",
    "    ]), False),\n",
    "    StructField(\"shipping_address\", StructType([\n",
    "        StructField(\"street\", StringType(), False),\n",
    "        StructField(\"city\", StringType(), False),\n",
    "        StructField(\"state\", StringType(), False),\n",
    "        StructField(\"zip\", StringType(), False),\n",
    "        StructField(\"coordinates\", StructType([\n",
    "            StructField(\"lat\", DoubleType(), False),\n",
    "            StructField(\"lon\", DoubleType(), False)\n",
    "        ]), False)\n",
    "    ]), False),\n",
    "    StructField(\"items\", ArrayType(StructType([\n",
    "        StructField(\"product_id\", IntegerType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"quantity\", IntegerType(), False),\n",
    "        StructField(\"price\", DoubleType(), False),\n",
    "        StructField(\"category\", StringType(), False)\n",
    "    ])), False),\n",
    "    StructField(\"payment\", StructType([\n",
    "        StructField(\"method\", StringType(), False),\n",
    "        StructField(\"last_four\", StringType(), True),\n",
    "        StructField(\"amount\", DoubleType(), False)\n",
    "    ]), False)\n",
    "])\n",
    "\n",
    "# Create nested order data\n",
    "orders_nested_data = [\n",
    "    {\n",
    "        \"order_id\": 1001,\n",
    "        \"order_date\": \"2024-01-15\",\n",
    "        \"customer\": {\n",
    "            \"customer_id\": 1,\n",
    "            \"name\": \"John Doe\",\n",
    "            \"email\": \"john@email.com\",\n",
    "            \"membership\": {\n",
    "                \"tier\": \"Gold\",\n",
    "                \"since\": \"2020-01-01\",\n",
    "                \"points\": 1500\n",
    "            }\n",
    "        },\n",
    "        \"shipping_address\": {\n",
    "            \"street\": \"123 Main St\",\n",
    "            \"city\": \"New York\",\n",
    "            \"state\": \"NY\",\n",
    "            \"zip\": \"10001\",\n",
    "            \"coordinates\": {\"lat\": 40.7128, \"lon\": -74.0060}\n",
    "        },\n",
    "        \"items\": [\n",
    "            {\"product_id\": 101, \"name\": \"Laptop\", \"quantity\": 1, \"price\": 1200.00, \"category\": \"Electronics\"},\n",
    "            {\"product_id\": 102, \"name\": \"Mouse\", \"quantity\": 2, \"price\": 25.00, \"category\": \"Accessories\"}\n",
    "        ],\n",
    "        \"payment\": {\n",
    "            \"method\": \"Credit Card\",\n",
    "            \"last_four\": \"4242\",\n",
    "            \"amount\": 1250.00\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"order_id\": 1002,\n",
    "        \"order_date\": \"2024-01-16\",\n",
    "        \"customer\": {\n",
    "            \"customer_id\": 2,\n",
    "            \"name\": \"Jane Smith\",\n",
    "            \"email\": \"jane@email.com\",\n",
    "            \"membership\": {\n",
    "                \"tier\": \"Silver\",\n",
    "                \"since\": \"2021-06-15\",\n",
    "                \"points\": 750\n",
    "            }\n",
    "        },\n",
    "        \"shipping_address\": {\n",
    "            \"street\": \"456 Oak Ave\",\n",
    "            \"city\": \"Los Angeles\",\n",
    "            \"state\": \"CA\",\n",
    "            \"zip\": \"90001\",\n",
    "            \"coordinates\": {\"lat\": 34.0522, \"lon\": -118.2437}\n",
    "        },\n",
    "        \"items\": [\n",
    "            {\"product_id\": 201, \"name\": \"Desk Chair\", \"quantity\": 1, \"price\": 350.00, \"category\": \"Furniture\"},\n",
    "            {\"product_id\": 202, \"name\": \"Desk Lamp\", \"quantity\": 1, \"price\": 45.00, \"category\": \"Lighting\"},\n",
    "            {\"product_id\": 203, \"name\": \"Notebook\", \"quantity\": 5, \"price\": 3.00, \"category\": \"Stationery\"}\n",
    "        ],\n",
    "        \"payment\": {\n",
    "            \"method\": \"PayPal\",\n",
    "            \"last_four\": None,\n",
    "            \"amount\": 410.00\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"order_id\": 1003,\n",
    "        \"order_date\": \"2024-01-17\",\n",
    "        \"customer\": {\n",
    "            \"customer_id\": 3,\n",
    "            \"name\": \"Bob Johnson\",\n",
    "            \"email\": \"bob@email.com\",\n",
    "            \"membership\": {\n",
    "                \"tier\": \"Platinum\",\n",
    "                \"since\": \"2018-03-20\",\n",
    "                \"points\": 3200\n",
    "            }\n",
    "        },\n",
    "        \"shipping_address\": {\n",
    "            \"street\": \"789 Pine Rd\",\n",
    "            \"city\": \"Chicago\",\n",
    "            \"state\": \"IL\",\n",
    "            \"zip\": \"60601\",\n",
    "            \"coordinates\": {\"lat\": 41.8781, \"lon\": -87.6298}\n",
    "        },\n",
    "        \"items\": [\n",
    "            {\"product_id\": 301, \"name\": \"Monitor\", \"quantity\": 2, \"price\": 400.00, \"category\": \"Electronics\"},\n",
    "            {\"product_id\": 302, \"name\": \"Keyboard\", \"quantity\": 2, \"price\": 120.00, \"category\": \"Electronics\"},\n",
    "            {\"product_id\": 303, \"name\": \"Webcam\", \"quantity\": 1, \"price\": 85.00, \"category\": \"Electronics\"}\n",
    "        ],\n",
    "        \"payment\": {\n",
    "            \"method\": \"Credit Card\",\n",
    "            \"last_four\": \"8888\",\n",
    "            \"amount\": 1125.00\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame with explicit schema\n",
    "orders_nested_df = spark.createDataFrame(orders_nested_data, schema=order_schema)\n",
    "\n",
    "print(\"\\n=== Nested Orders Schema ===\")\n",
    "orders_nested_df.printSchema()\n",
    "\n",
    "print(\"\\n=== Nested Orders Data ===\")\n",
    "orders_nested_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Flattened Order Data ===\n",
      "+--------+----------+-----------+-------------+---------------+--------------+-----------+-----+--------------+------------+\n",
      "|order_id|order_date|customer_id|customer_name|membership_tier|loyalty_points|city       |state|payment_method|total_amount|\n",
      "+--------+----------+-----------+-------------+---------------+--------------+-----------+-----+--------------+------------+\n",
      "|1001    |2024-01-15|1          |John Doe     |Gold           |1500          |New York   |NY   |Credit Card   |1250.0      |\n",
      "|1002    |2024-01-16|2          |Jane Smith   |Silver         |750           |Los Angeles|CA   |PayPal        |410.0       |\n",
      "|1003    |2024-01-17|3          |Bob Johnson  |Platinum       |3200          |Chicago    |IL   |Credit Card   |1125.0      |\n",
      "+--------+----------+-----------+-------------+---------------+--------------+-----------+-----+--------------+------------+\n",
      "\n",
      "\n",
      "=== Exploded Item Details ===\n",
      "+--------+-------------+----------+------------+-----------+--------+----------+----------+\n",
      "|order_id|customer_name|product_id|product_name|category   |quantity|unit_price|line_total|\n",
      "+--------+-------------+----------+------------+-----------+--------+----------+----------+\n",
      "|1001    |John Doe     |101       |Laptop      |Electronics|1       |1200.0    |1200.0    |\n",
      "|1001    |John Doe     |102       |Mouse       |Accessories|2       |25.0      |50.0      |\n",
      "|1002    |Jane Smith   |201       |Desk Chair  |Furniture  |1       |350.0     |350.0     |\n",
      "|1002    |Jane Smith   |202       |Desk Lamp   |Lighting   |1       |45.0      |45.0      |\n",
      "|1002    |Jane Smith   |203       |Notebook    |Stationery |5       |3.0       |15.0      |\n",
      "|1003    |Bob Johnson  |301       |Monitor     |Electronics|2       |400.0     |800.0     |\n",
      "|1003    |Bob Johnson  |302       |Keyboard    |Electronics|2       |120.0     |240.0     |\n",
      "|1003    |Bob Johnson  |303       |Webcam      |Electronics|1       |85.0      |85.0      |\n",
      "+--------+-------------+----------+------------+-----------+--------+----------+----------+\n",
      "\n",
      "\n",
      "=== Customer Summary with Aggregated Nested Data ===\n",
      "+-------------+------------+-----------+-----------+--------------------+---------------------------------+\n",
      "|customer_name|total_orders|total_items|total_spent|categories_purchased|all_products                     |\n",
      "+-------------+------------+-----------+-----------+--------------------+---------------------------------+\n",
      "|John Doe     |1           |3          |1250.0     |2                   |[Laptop, Mouse]                  |\n",
      "|Bob Johnson  |1           |5          |1125.0     |1                   |[Monitor, Keyboard, Webcam]      |\n",
      "|Jane Smith   |1           |7          |410.0      |3                   |[Desk Chair, Desk Lamp, Notebook]|\n",
      "+-------------+------------+-----------+-----------+--------------------+---------------------------------+\n",
      "\n",
      "\n",
      "=== Re-nested Order Summary ===\n",
      "root\n",
      " |-- order_id: integer (nullable = false)\n",
      " |-- customer_name: string (nullable = false)\n",
      " |-- order_summary: struct (nullable = false)\n",
      " |    |-- total_items: long (nullable = true)\n",
      " |    |-- total_amount: double (nullable = true)\n",
      " |    |-- category_count: long (nullable = false)\n",
      " |-- items: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- product_name: string (nullable = true)\n",
      " |    |    |-- category: string (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- line_total: double (nullable = true)\n",
      "\n",
      "+--------+-------------+--------------+-----------------------------------------------------------------------------------------------------+\n",
      "|order_id|customer_name|order_summary |items                                                                                                |\n",
      "+--------+-------------+--------------+-----------------------------------------------------------------------------------------------------+\n",
      "|1003    |Bob Johnson  |{5, 1125.0, 1}|[{Monitor, Electronics, 2, 800.0}, {Keyboard, Electronics, 2, 240.0}, {Webcam, Electronics, 1, 85.0}]|\n",
      "|1002    |Jane Smith   |{7, 410.0, 3} |[{Notebook, Stationery, 5, 15.0}, {Desk Chair, Furniture, 1, 350.0}, {Desk Lamp, Lighting, 1, 45.0}] |\n",
      "|1001    |John Doe     |{3, 1250.0, 2}|[{Mouse, Accessories, 2, 50.0}, {Laptop, Electronics, 1, 1200.0}]                                    |\n",
      "+--------+-------------+--------------+-----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Working with nested columns - Example 1: Extract nested fields\n",
    "orders_flattened = orders_nested_df.select(\n",
    "    \"order_id\",\n",
    "    \"order_date\",\n",
    "    col(\"customer.customer_id\").alias(\"customer_id\"),\n",
    "    col(\"customer.name\").alias(\"customer_name\"),\n",
    "    col(\"customer.membership.tier\").alias(\"membership_tier\"),\n",
    "    col(\"customer.membership.points\").alias(\"loyalty_points\"),\n",
    "    col(\"shipping_address.city\").alias(\"city\"),\n",
    "    col(\"shipping_address.state\").alias(\"state\"),\n",
    "    col(\"payment.method\").alias(\"payment_method\"),\n",
    "    col(\"payment.amount\").alias(\"total_amount\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== Flattened Order Data ===\")\n",
    "orders_flattened.show(truncate=False)\n",
    "\n",
    "# Working with nested arrays - Example 2: Explode items array\n",
    "orders_with_items = orders_nested_df.select(\n",
    "    \"order_id\",\n",
    "    \"order_date\",\n",
    "    col(\"customer.name\").alias(\"customer_name\"),\n",
    "    explode(\"items\").alias(\"item\")\n",
    ")\n",
    "\n",
    "items_detailed = orders_with_items.select(\n",
    "    \"order_id\",\n",
    "    \"customer_name\",\n",
    "    col(\"item.product_id\").alias(\"product_id\"),\n",
    "    col(\"item.name\").alias(\"product_name\"),\n",
    "    col(\"item.category\").alias(\"category\"),\n",
    "    col(\"item.quantity\").alias(\"quantity\"),\n",
    "    col(\"item.price\").alias(\"unit_price\"),\n",
    "    (col(\"item.quantity\") * col(\"item.price\")).alias(\"line_total\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== Exploded Item Details ===\")\n",
    "items_detailed.show(truncate=False)\n",
    "\n",
    "# Aggregate operations on nested data - Example 3\n",
    "customer_summary = items_detailed.groupBy(\"customer_name\") \\\n",
    "    .agg(\n",
    "        countDistinct(\"order_id\").alias(\"total_orders\"),\n",
    "        sum(\"quantity\").alias(\"total_items\"),\n",
    "        sum(\"line_total\").alias(\"total_spent\"),\n",
    "        countDistinct(\"category\").alias(\"categories_purchased\"),\n",
    "        collect_list(\"product_name\").alias(\"all_products\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_spent\"))\n",
    "\n",
    "print(\"\\n=== Customer Summary with Aggregated Nested Data ===\")\n",
    "customer_summary.show(truncate=False)\n",
    "\n",
    "# Create new nested structures - Example 4\n",
    "order_summary_nested = items_detailed.groupBy(\"order_id\", \"customer_name\") \\\n",
    "    .agg(\n",
    "        struct(\n",
    "            sum(\"quantity\").alias(\"total_items\"),\n",
    "            sum(\"line_total\").alias(\"total_amount\"),\n",
    "            countDistinct(\"category\").alias(\"category_count\")\n",
    "        ).alias(\"order_summary\"),\n",
    "        collect_list(\n",
    "            struct(\n",
    "                col(\"product_name\"),\n",
    "                col(\"category\"),\n",
    "                col(\"quantity\"),\n",
    "                col(\"line_total\")\n",
    "            )\n",
    "        ).alias(\"items\")\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Re-nested Order Summary ===\")\n",
    "order_summary_nested.printSchema()\n",
    "order_summary_nested.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3.2: Social Media Posts with Complex Nested Structures\n",
    "\n",
    "Scenario: Analyze social media posts with nested user info, comments, reactions, and hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Social Media Posts Schema ===\n",
      "root\n",
      " |-- post_id: string (nullable = false)\n",
      " |-- timestamp: string (nullable = false)\n",
      " |-- author: struct (nullable = false)\n",
      " |    |-- user_id: string (nullable = false)\n",
      " |    |-- username: string (nullable = false)\n",
      " |    |-- display_name: string (nullable = false)\n",
      " |    |-- followers: integer (nullable = false)\n",
      " |    |-- verified: boolean (nullable = false)\n",
      " |    |-- profile: struct (nullable = false)\n",
      " |    |    |-- bio: string (nullable = false)\n",
      " |    |    |-- location: string (nullable = false)\n",
      " |    |    |-- joined_date: string (nullable = false)\n",
      " |-- content: struct (nullable = false)\n",
      " |    |-- text: string (nullable = false)\n",
      " |    |-- media: array (nullable = false)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- type: string (nullable = false)\n",
      " |    |    |    |-- url: string (nullable = false)\n",
      " |    |    |    |-- width: integer (nullable = true)\n",
      " |    |    |    |-- height: integer (nullable = true)\n",
      " |    |    |    |-- duration: integer (nullable = true)\n",
      " |    |-- hashtags: array (nullable = false)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- engagement: struct (nullable = false)\n",
      " |    |-- likes: integer (nullable = false)\n",
      " |    |-- shares: integer (nullable = false)\n",
      " |    |-- views: integer (nullable = false)\n",
      " |    |-- reactions: array (nullable = false)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- type: string (nullable = false)\n",
      " |    |    |    |-- count: integer (nullable = false)\n",
      " |-- comments: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- comment_id: string (nullable = false)\n",
      " |    |    |-- user: struct (nullable = false)\n",
      " |    |    |    |-- user_id: string (nullable = false)\n",
      " |    |    |    |-- username: string (nullable = false)\n",
      " |    |    |-- text: string (nullable = false)\n",
      " |    |    |-- likes: integer (nullable = false)\n",
      " |    |    |-- replies: array (nullable = false)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- user_id: string (nullable = false)\n",
      " |    |    |    |    |-- text: string (nullable = false)\n",
      "\n",
      "\n",
      "=== Social Media Posts ===\n",
      "+-------+-------------------+---------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|post_id|timestamp          |author                                                                                                               |content                                                                                                                                                                                                                       |engagement                                                |comments                                                                                                                                                                                                                                |\n",
      "+-------+-------------------+---------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|P001   |2024-01-15 10:30:00|{U001, tech_guru, Tech Guru, 15000, true, {Technology enthusiast, San Francisco, 2020-01-01}}                        |{Just launched my new app!, [{image, img1.jpg, 1920, 1080, NULL}, {image, img2.jpg, 1920, 1080, NULL}], [#tech, #app, #launch]}                                                                                               |{250, 45, 5000, [{like, 200}, {love, 30}, {wow, 20}]}     |[{C001, {U002, fan_user}, Looks amazing!, 10, [{U001, Thank you!}]}, {C002, {U003, developer}, What tech stack did you use?, 5, []}]                                                                                                    |\n",
      "|P002   |2024-01-16 14:20:00|{U004, food_blogger, Foodie Adventures, 28000, true, {Exploring culinary delights, New York, 2019-05-15}}            |{Best pasta in the city!, [{image, food1.jpg, 1080, 1080, NULL}], [#food, #pasta, #foodie, #restaurant]}                                                                                                                      |{580, 92, 12000, [{like, 400}, {love, 150}, {yum, 30}]}   |[{C003, {U005, food_lover}, Where is this place?, 25, [{U004, Little Italy, NYC!}]}]                                                                                                                                                    |\n",
      "|P003   |2024-01-17 09:00:00|{U006, travel_pro, Travel Professional, 42000, true, {Exploring the world one country at a time, Global, 2018-03-20}}|{Sunrise at Santorini is breathtaking!, [{image, santorini1.jpg, 2560, 1440, NULL}, {image, santorini2.jpg, 2560, 1440, NULL}, {video, santorini.mp4, NULL, NULL, 45}], [#travel, #santorini, #greece, #sunrise, #wanderlust]}|{1250, 210, 35000, [{like, 800}, {love, 350}, {wow, 100}]}|[{C004, {U007, traveler123}, Adding this to my bucket list!, 45, [{U006, You won't regret it!}]}, {C005, {U008, photographer}, Great shots! What camera?, 30, []}, {C006, {U009, travel_agent}, I can help you plan trips here!, 8, []}]|\n",
      "+-------+-------------------+---------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create nested social media data with explicit schema\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# Define explicit schema for social media posts\n",
    "social_media_schema = StructType([\n",
    "    StructField(\"post_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"author\", StructType([\n",
    "        StructField(\"user_id\", StringType(), False),\n",
    "        StructField(\"username\", StringType(), False),\n",
    "        StructField(\"display_name\", StringType(), False),\n",
    "        StructField(\"followers\", IntegerType(), False),\n",
    "        StructField(\"verified\", BooleanType(), False),\n",
    "        StructField(\"profile\", StructType([\n",
    "            StructField(\"bio\", StringType(), False),\n",
    "            StructField(\"location\", StringType(), False),\n",
    "            StructField(\"joined_date\", StringType(), False)\n",
    "        ]), False)\n",
    "    ]), False),\n",
    "    StructField(\"content\", StructType([\n",
    "        StructField(\"text\", StringType(), False),\n",
    "        StructField(\"media\", ArrayType(StructType([\n",
    "            StructField(\"type\", StringType(), False),\n",
    "            StructField(\"url\", StringType(), False),\n",
    "            StructField(\"width\", IntegerType(), True),\n",
    "            StructField(\"height\", IntegerType(), True),\n",
    "            StructField(\"duration\", IntegerType(), True)\n",
    "        ])), False),\n",
    "        StructField(\"hashtags\", ArrayType(StringType()), False)\n",
    "    ]), False),\n",
    "    StructField(\"engagement\", StructType([\n",
    "        StructField(\"likes\", IntegerType(), False),\n",
    "        StructField(\"shares\", IntegerType(), False),\n",
    "        StructField(\"views\", IntegerType(), False),\n",
    "        StructField(\"reactions\", ArrayType(StructType([\n",
    "            StructField(\"type\", StringType(), False),\n",
    "            StructField(\"count\", IntegerType(), False)\n",
    "        ])), False)\n",
    "    ]), False),\n",
    "    StructField(\"comments\", ArrayType(StructType([\n",
    "        StructField(\"comment_id\", StringType(), False),\n",
    "        StructField(\"user\", StructType([\n",
    "            StructField(\"user_id\", StringType(), False),\n",
    "            StructField(\"username\", StringType(), False)\n",
    "        ]), False),\n",
    "        StructField(\"text\", StringType(), False),\n",
    "        StructField(\"likes\", IntegerType(), False),\n",
    "        StructField(\"replies\", ArrayType(StructType([\n",
    "            StructField(\"user_id\", StringType(), False),\n",
    "            StructField(\"text\", StringType(), False)\n",
    "        ])), False)\n",
    "    ])), False)\n",
    "])\n",
    "\n",
    "social_posts_data = [\n",
    "    {\n",
    "        \"post_id\": \"P001\",\n",
    "        \"timestamp\": \"2024-01-15 10:30:00\",\n",
    "        \"author\": {\n",
    "            \"user_id\": \"U001\",\n",
    "            \"username\": \"tech_guru\",\n",
    "            \"display_name\": \"Tech Guru\",\n",
    "            \"followers\": 15000,\n",
    "            \"verified\": True,\n",
    "            \"profile\": {\n",
    "                \"bio\": \"Technology enthusiast\",\n",
    "                \"location\": \"San Francisco\",\n",
    "                \"joined_date\": \"2020-01-01\"\n",
    "            }\n",
    "        },\n",
    "        \"content\": {\n",
    "            \"text\": \"Just launched my new app!\",\n",
    "            \"media\": [\n",
    "                {\"type\": \"image\", \"url\": \"img1.jpg\", \"width\": 1920, \"height\": 1080, \"duration\": None},\n",
    "                {\"type\": \"image\", \"url\": \"img2.jpg\", \"width\": 1920, \"height\": 1080, \"duration\": None}\n",
    "            ],\n",
    "            \"hashtags\": [\"#tech\", \"#app\", \"#launch\"]\n",
    "        },\n",
    "        \"engagement\": {\n",
    "            \"likes\": 250,\n",
    "            \"shares\": 45,\n",
    "            \"views\": 5000,\n",
    "            \"reactions\": [\n",
    "                {\"type\": \"like\", \"count\": 200},\n",
    "                {\"type\": \"love\", \"count\": 30},\n",
    "                {\"type\": \"wow\", \"count\": 20}\n",
    "            ]\n",
    "        },\n",
    "        \"comments\": [\n",
    "            {\n",
    "                \"comment_id\": \"C001\",\n",
    "                \"user\": {\"user_id\": \"U002\", \"username\": \"fan_user\"},\n",
    "                \"text\": \"Looks amazing!\",\n",
    "                \"likes\": 10,\n",
    "                \"replies\": [\n",
    "                    {\"user_id\": \"U001\", \"text\": \"Thank you!\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"comment_id\": \"C002\",\n",
    "                \"user\": {\"user_id\": \"U003\", \"username\": \"developer\"},\n",
    "                \"text\": \"What tech stack did you use?\",\n",
    "                \"likes\": 5,\n",
    "                \"replies\": []\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"post_id\": \"P002\",\n",
    "        \"timestamp\": \"2024-01-16 14:20:00\",\n",
    "        \"author\": {\n",
    "            \"user_id\": \"U004\",\n",
    "            \"username\": \"food_blogger\",\n",
    "            \"display_name\": \"Foodie Adventures\",\n",
    "            \"followers\": 28000,\n",
    "            \"verified\": True,\n",
    "            \"profile\": {\n",
    "                \"bio\": \"Exploring culinary delights\",\n",
    "                \"location\": \"New York\",\n",
    "                \"joined_date\": \"2019-05-15\"\n",
    "            }\n",
    "        },\n",
    "        \"content\": {\n",
    "            \"text\": \"Best pasta in the city!\",\n",
    "            \"media\": [\n",
    "                {\"type\": \"image\", \"url\": \"food1.jpg\", \"width\": 1080, \"height\": 1080, \"duration\": None}\n",
    "            ],\n",
    "            \"hashtags\": [\"#food\", \"#pasta\", \"#foodie\", \"#restaurant\"]\n",
    "        },\n",
    "        \"engagement\": {\n",
    "            \"likes\": 580,\n",
    "            \"shares\": 92,\n",
    "            \"views\": 12000,\n",
    "            \"reactions\": [\n",
    "                {\"type\": \"like\", \"count\": 400},\n",
    "                {\"type\": \"love\", \"count\": 150},\n",
    "                {\"type\": \"yum\", \"count\": 30}\n",
    "            ]\n",
    "        },\n",
    "        \"comments\": [\n",
    "            {\n",
    "                \"comment_id\": \"C003\",\n",
    "                \"user\": {\"user_id\": \"U005\", \"username\": \"food_lover\"},\n",
    "                \"text\": \"Where is this place?\",\n",
    "                \"likes\": 25,\n",
    "                \"replies\": [\n",
    "                    {\"user_id\": \"U004\", \"text\": \"Little Italy, NYC!\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"post_id\": \"P003\",\n",
    "        \"timestamp\": \"2024-01-17 09:00:00\",\n",
    "        \"author\": {\n",
    "            \"user_id\": \"U006\",\n",
    "            \"username\": \"travel_pro\",\n",
    "            \"display_name\": \"Travel Professional\",\n",
    "            \"followers\": 42000,\n",
    "            \"verified\": True,\n",
    "            \"profile\": {\n",
    "                \"bio\": \"Exploring the world one country at a time\",\n",
    "                \"location\": \"Global\",\n",
    "                \"joined_date\": \"2018-03-20\"\n",
    "            }\n",
    "        },\n",
    "        \"content\": {\n",
    "            \"text\": \"Sunrise at Santorini is breathtaking!\",\n",
    "            \"media\": [\n",
    "                {\"type\": \"image\", \"url\": \"santorini1.jpg\", \"width\": 2560, \"height\": 1440, \"duration\": None},\n",
    "                {\"type\": \"image\", \"url\": \"santorini2.jpg\", \"width\": 2560, \"height\": 1440, \"duration\": None},\n",
    "                {\"type\": \"video\", \"url\": \"santorini.mp4\", \"width\": None, \"height\": None, \"duration\": 45}\n",
    "            ],\n",
    "            \"hashtags\": [\"#travel\", \"#santorini\", \"#greece\", \"#sunrise\", \"#wanderlust\"]\n",
    "        },\n",
    "        \"engagement\": {\n",
    "            \"likes\": 1250,\n",
    "            \"shares\": 210,\n",
    "            \"views\": 35000,\n",
    "            \"reactions\": [\n",
    "                {\"type\": \"like\", \"count\": 800},\n",
    "                {\"type\": \"love\", \"count\": 350},\n",
    "                {\"type\": \"wow\", \"count\": 100}\n",
    "            ]\n",
    "        },\n",
    "        \"comments\": [\n",
    "            {\n",
    "                \"comment_id\": \"C004\",\n",
    "                \"user\": {\"user_id\": \"U007\", \"username\": \"traveler123\"},\n",
    "                \"text\": \"Adding this to my bucket list!\",\n",
    "                \"likes\": 45,\n",
    "                \"replies\": [\n",
    "                    {\"user_id\": \"U006\", \"text\": \"You won't regret it!\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"comment_id\": \"C005\",\n",
    "                \"user\": {\"user_id\": \"U008\", \"username\": \"photographer\"},\n",
    "                \"text\": \"Great shots! What camera?\",\n",
    "                \"likes\": 30,\n",
    "                \"replies\": []\n",
    "            },\n",
    "            {\n",
    "                \"comment_id\": \"C006\",\n",
    "                \"user\": {\"user_id\": \"U009\", \"username\": \"travel_agent\"},\n",
    "                \"text\": \"I can help you plan trips here!\",\n",
    "                \"likes\": 8,\n",
    "                \"replies\": []\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame with explicit schema\n",
    "social_posts_df = spark.createDataFrame(social_posts_data, schema=social_media_schema)\n",
    "\n",
    "print(\"\\n=== Social Media Posts Schema ===\")\n",
    "social_posts_df.printSchema()\n",
    "\n",
    "print(\"\\n=== Social Media Posts ===\")\n",
    "social_posts_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Author Statistics ===\n",
      "+-------+------------+-------------------+---------+--------+-------------+-----+------+-----+---------------+\n",
      "|post_id|username    |display_name       |followers|verified|location     |likes|shares|views|engagement_rate|\n",
      "+-------+------------+-------------------+---------+--------+-------------+-----+------+-----+---------------+\n",
      "|P001   |tech_guru   |Tech Guru          |15000    |true    |San Francisco|250  |45    |5000 |5.9            |\n",
      "|P002   |food_blogger|Foodie Adventures  |28000    |true    |New York     |580  |92    |12000|5.6            |\n",
      "|P003   |travel_pro  |Travel Professional|42000    |true    |Global       |1250 |210   |35000|4.17           |\n",
      "+-------+------------+-------------------+---------+--------+-------------+-----+------+-----+---------------+\n",
      "\n",
      "\n",
      "=== Hashtag Performance Analysis ===\n",
      "+-----------+-----------+-----------+------------------+--------------+\n",
      "|hashtag    |usage_count|total_views|avg_views_per_post|users         |\n",
      "+-----------+-----------+-----------+------------------+--------------+\n",
      "|#santorini |1          |35000      |35000.0           |[travel_pro]  |\n",
      "|#sunrise   |1          |35000      |35000.0           |[travel_pro]  |\n",
      "|#wanderlust|1          |35000      |35000.0           |[travel_pro]  |\n",
      "|#travel    |1          |35000      |35000.0           |[travel_pro]  |\n",
      "|#greece    |1          |35000      |35000.0           |[travel_pro]  |\n",
      "|#pasta     |1          |12000      |12000.0           |[food_blogger]|\n",
      "|#restaurant|1          |12000      |12000.0           |[food_blogger]|\n",
      "|#foodie    |1          |12000      |12000.0           |[food_blogger]|\n",
      "|#food      |1          |12000      |12000.0           |[food_blogger]|\n",
      "|#tech      |1          |5000       |5000.0            |[tech_guru]   |\n",
      "|#launch    |1          |5000       |5000.0            |[tech_guru]   |\n",
      "|#app       |1          |5000       |5000.0            |[tech_guru]   |\n",
      "+-----------+-----------+-----------+------------------+--------------+\n",
      "\n",
      "\n",
      "=== Exploded Reactions ===\n",
      "+-------+------------+-------------+--------------+\n",
      "|post_id|username    |reaction_type|reaction_count|\n",
      "+-------+------------+-------------+--------------+\n",
      "|P001   |tech_guru   |like         |200           |\n",
      "|P001   |tech_guru   |love         |30            |\n",
      "|P001   |tech_guru   |wow          |20            |\n",
      "|P002   |food_blogger|like         |400           |\n",
      "|P002   |food_blogger|love         |150           |\n",
      "|P002   |food_blogger|yum          |30            |\n",
      "|P003   |travel_pro  |like         |800           |\n",
      "|P003   |travel_pro  |love         |350           |\n",
      "|P003   |travel_pro  |wow          |100           |\n",
      "+-------+------------+-------------+--------------+\n",
      "\n",
      "\n",
      "=== Reaction Summary by User (Pivot) ===\n",
      "+------------+----+----+---+---+\n",
      "|username    |like|love|wow|yum|\n",
      "+------------+----+----+---+---+\n",
      "|tech_guru   |200 |30  |20 |0  |\n",
      "|food_blogger|400 |150 |0  |30 |\n",
      "|travel_pro  |800 |350 |100|0  |\n",
      "+------------+----+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract and analyze nested social media data - Part 1: User analytics\n",
    "author_stats = social_posts_df.select(\n",
    "    \"post_id\",\n",
    "    col(\"author.username\").alias(\"username\"),\n",
    "    col(\"author.display_name\").alias(\"display_name\"),\n",
    "    col(\"author.followers\").alias(\"followers\"),\n",
    "    col(\"author.verified\").alias(\"verified\"),\n",
    "    col(\"author.profile.location\").alias(\"location\"),\n",
    "    col(\"engagement.likes\").alias(\"likes\"),\n",
    "    col(\"engagement.shares\").alias(\"shares\"),\n",
    "    col(\"engagement.views\").alias(\"views\")\n",
    ").withColumn(\n",
    "    \"engagement_rate\",\n",
    "    round((col(\"likes\") + col(\"shares\")) / col(\"views\") * 100, 2)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Author Statistics ===\")\n",
    "author_stats.show(truncate=False)\n",
    "\n",
    "# Explode and analyze hashtags - Part 2\n",
    "hashtag_analysis = social_posts_df.select(\n",
    "    \"post_id\",\n",
    "    col(\"author.username\").alias(\"username\"),\n",
    "    col(\"engagement.views\").alias(\"views\"),\n",
    "    explode(\"content.hashtags\").alias(\"hashtag\")\n",
    ")\n",
    "\n",
    "hashtag_performance = hashtag_analysis.groupBy(\"hashtag\") \\\n",
    "    .agg(\n",
    "        count(\"post_id\").alias(\"usage_count\"),\n",
    "        sum(\"views\").alias(\"total_views\"),\n",
    "        avg(\"views\").alias(\"avg_views_per_post\"),\n",
    "        collect_set(\"username\").alias(\"users\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_views\"))\n",
    "\n",
    "print(\"\\n=== Hashtag Performance Analysis ===\")\n",
    "hashtag_performance.show(truncate=False)\n",
    "\n",
    "# Explode and analyze reactions - Part 3\n",
    "reactions_exploded = social_posts_df.select(\n",
    "    \"post_id\",\n",
    "    col(\"author.username\").alias(\"username\"),\n",
    "    explode(\"engagement.reactions\").alias(\"reaction\")\n",
    ").select(\n",
    "    \"post_id\",\n",
    "    \"username\",\n",
    "    col(\"reaction.type\").alias(\"reaction_type\"),\n",
    "    col(\"reaction.count\").alias(\"reaction_count\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== Exploded Reactions ===\")\n",
    "reactions_exploded.show(truncate=False)\n",
    "\n",
    "reaction_summary = reactions_exploded.groupBy(\"username\") \\\n",
    "    .pivot(\"reaction_type\") \\\n",
    "    .sum(\"reaction_count\") \\\n",
    "    .na.fill(0)\n",
    "\n",
    "print(\"\\n=== Reaction Summary by User (Pivot) ===\")\n",
    "reaction_summary.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Exploded Comments with Details ===\n",
      "+-------+------------+----------+----------+------------+-------------------------------+-------------+-----------+\n",
      "|post_id|post_author |post_likes|comment_id|commenter   |comment_text                   |comment_likes|reply_count|\n",
      "+-------+------------+----------+----------+------------+-------------------------------+-------------+-----------+\n",
      "|P001   |tech_guru   |250       |C001      |fan_user    |Looks amazing!                 |10           |1          |\n",
      "|P001   |tech_guru   |250       |C002      |developer   |What tech stack did you use?   |5            |0          |\n",
      "|P002   |food_blogger|580       |C003      |food_lover  |Where is this place?           |25           |1          |\n",
      "|P003   |travel_pro  |1250      |C004      |traveler123 |Adding this to my bucket list! |45           |1          |\n",
      "|P003   |travel_pro  |1250      |C005      |photographer|Great shots! What camera?      |30           |0          |\n",
      "|P003   |travel_pro  |1250      |C006      |travel_agent|I can help you plan trips here!|8            |0          |\n",
      "+-------+------------+----------+----------+------------+-------------------------------+-------------+-----------+\n",
      "\n",
      "\n",
      "=== Comment Statistics per Post ===\n",
      "+-------+------------+--------------+-------------------+-------------+---------------------+-----------------------------------------+\n",
      "|post_id|post_author |total_comments|total_comment_likes|total_replies|avg_likes_per_comment|commenters                               |\n",
      "+-------+------------+--------------+-------------------+-------------+---------------------+-----------------------------------------+\n",
      "|P001   |tech_guru   |2             |15                 |1            |7.5                  |[fan_user, developer]                    |\n",
      "|P003   |travel_pro  |3             |83                 |1            |27.666666666666668   |[traveler123, photographer, travel_agent]|\n",
      "|P002   |food_blogger|1             |25                 |1            |25.0                 |[food_lover]                             |\n",
      "+-------+------------+--------------+-------------------+-------------+---------------------+-----------------------------------------+\n",
      "\n",
      "\n",
      "=== Media Type Analysis ===\n",
      "+------------+----------+-----------+-----------+---------+\n",
      "|username    |media_type|media_count|total_views|avg_views|\n",
      "+------------+----------+-----------+-----------+---------+\n",
      "|food_blogger|image     |1          |12000      |12000.0  |\n",
      "|tech_guru   |image     |2          |10000      |5000.0   |\n",
      "|travel_pro  |image     |2          |70000      |35000.0  |\n",
      "|travel_pro  |video     |1          |35000      |35000.0  |\n",
      "+------------+----------+-----------+-----------+---------+\n",
      "\n",
      "\n",
      "=== Post Summary with Nested Engagement ===\n",
      "root\n",
      " |-- post_id: string (nullable = false)\n",
      " |-- username: string (nullable = false)\n",
      " |-- engagement_summary: struct (nullable = false)\n",
      " |    |-- total_comments: long (nullable = true)\n",
      " |    |-- total_comment_likes: long (nullable = true)\n",
      " |    |-- total_replies: long (nullable = true)\n",
      " |    |-- commenters: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = false)\n",
      "\n",
      "+-------+------------+-----------------------------------------------------+\n",
      "|post_id|username    |engagement_summary                                   |\n",
      "+-------+------------+-----------------------------------------------------+\n",
      "|P002   |food_blogger|{1, 25, 1, [food_lover]}                             |\n",
      "|P001   |tech_guru   |{2, 15, 1, [fan_user, developer]}                    |\n",
      "|P003   |travel_pro  |{3, 83, 1, [traveler123, photographer, travel_agent]}|\n",
      "+-------+------------+-----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complex nested analysis - Part 4: Comments and engagement\n",
    "comments_exploded = social_posts_df.select(\n",
    "    \"post_id\",\n",
    "    col(\"author.username\").alias(\"post_author\"),\n",
    "    col(\"engagement.likes\").alias(\"post_likes\"),\n",
    "    explode(\"comments\").alias(\"comment\")\n",
    ").select(\n",
    "    \"post_id\",\n",
    "    \"post_author\",\n",
    "    \"post_likes\",\n",
    "    col(\"comment.comment_id\").alias(\"comment_id\"),\n",
    "    col(\"comment.user.username\").alias(\"commenter\"),\n",
    "    col(\"comment.text\").alias(\"comment_text\"),\n",
    "    col(\"comment.likes\").alias(\"comment_likes\"),\n",
    "    size(col(\"comment.replies\")).alias(\"reply_count\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== Exploded Comments with Details ===\")\n",
    "comments_exploded.show(truncate=False)\n",
    "\n",
    "# Aggregate comment statistics per post\n",
    "comment_stats = comments_exploded.groupBy(\"post_id\", \"post_author\") \\\n",
    "    .agg(\n",
    "        count(\"comment_id\").alias(\"total_comments\"),\n",
    "        sum(\"comment_likes\").alias(\"total_comment_likes\"),\n",
    "        sum(\"reply_count\").alias(\"total_replies\"),\n",
    "        avg(\"comment_likes\").alias(\"avg_likes_per_comment\"),\n",
    "        collect_list(\"commenter\").alias(\"commenters\")\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Comment Statistics per Post ===\")\n",
    "comment_stats.show(truncate=False)\n",
    "\n",
    "# Media analysis - Part 5\n",
    "media_exploded = social_posts_df.select(\n",
    "    \"post_id\",\n",
    "    col(\"author.username\").alias(\"username\"),\n",
    "    col(\"engagement.views\").alias(\"views\"),\n",
    "    explode(\"content.media\").alias(\"media\")\n",
    ").select(\n",
    "    \"post_id\",\n",
    "    \"username\",\n",
    "    \"views\",\n",
    "    col(\"media.type\").alias(\"media_type\"),\n",
    "    col(\"media.url\").alias(\"url\")\n",
    ")\n",
    "\n",
    "media_analysis = media_exploded.groupBy(\"username\", \"media_type\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"media_count\"),\n",
    "        sum(\"views\").alias(\"total_views\"),\n",
    "        avg(\"views\").alias(\"avg_views\")\n",
    "    ) \\\n",
    "    .orderBy(\"username\", \"media_type\")\n",
    "\n",
    "print(\"\\n=== Media Type Analysis ===\")\n",
    "media_analysis.show(truncate=False)\n",
    "\n",
    "# Create comprehensive nested summary\n",
    "post_summary = social_posts_df.select(\n",
    "    \"post_id\",\n",
    "    col(\"author.username\").alias(\"username\")\n",
    ").join(\n",
    "    comment_stats,\n",
    "    [\"post_id\"],\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"engagement_summary\",\n",
    "    struct(\n",
    "        col(\"total_comments\"),\n",
    "        col(\"total_comment_likes\"),\n",
    "        col(\"total_replies\"),\n",
    "        col(\"commenters\")\n",
    "    )\n",
    ").select(\"post_id\", \"username\", \"engagement_summary\")\n",
    "\n",
    "print(\"\\n=== Post Summary with Nested Engagement ===\")\n",
    "post_summary.printSchema()\n",
    "post_summary.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3.3: IoT Sensor Data with Complex Nested Structures\n",
    "\n",
    "Scenario: Process IoT sensor data with nested device info, multiple sensor readings, and event logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IoT Sensor Data Schema ===\n",
      "root\n",
      " |-- device_id: string (nullable = false)\n",
      " |-- timestamp: string (nullable = false)\n",
      " |-- device_info: struct (nullable = false)\n",
      " |    |-- name: string (nullable = false)\n",
      " |    |-- type: string (nullable = false)\n",
      " |    |-- manufacturer: string (nullable = false)\n",
      " |    |-- model: string (nullable = false)\n",
      " |    |-- firmware_version: string (nullable = false)\n",
      " |    |-- location: struct (nullable = false)\n",
      " |    |    |-- building: string (nullable = false)\n",
      " |    |    |-- floor: integer (nullable = false)\n",
      " |    |    |-- room: string (nullable = false)\n",
      " |    |    |-- coordinates: struct (nullable = false)\n",
      " |    |    |    |-- x: double (nullable = false)\n",
      " |    |    |    |-- y: double (nullable = false)\n",
      " |-- sensors: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- sensor_id: string (nullable = false)\n",
      " |    |    |-- type: string (nullable = false)\n",
      " |    |    |-- value: double (nullable = false)\n",
      " |    |    |-- unit: string (nullable = false)\n",
      " |    |    |-- status: string (nullable = false)\n",
      " |-- metrics: struct (nullable = false)\n",
      " |    |-- power_consumption: double (nullable = false)\n",
      " |    |-- uptime_hours: integer (nullable = false)\n",
      " |    |-- signal_strength: integer (nullable = false)\n",
      " |    |-- battery_level: integer (nullable = true)\n",
      " |-- events: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- time: string (nullable = false)\n",
      " |    |    |-- type: string (nullable = false)\n",
      " |    |    |-- description: string (nullable = false)\n",
      "\n",
      "\n",
      "=== IoT Sensor Data ===\n",
      "+---------+-------------------+------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|device_id|timestamp          |device_info                                                                                                             |sensors                                                                                                                                                                     |metrics                |events                                                                                                                                 |\n",
      "+---------+-------------------+------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|D001     |2024-01-15 10:00:00|{Smart Thermostat Living Room, thermostat, SmartHome Inc, TH-2000, 2.1.5, {Main Building, 1, Living Room, {10.5, 20.3}}}|[{S001, temperature, 72.5, F, normal}, {S002, humidity, 45.2, %, normal}, {S003, air_quality, 85.0, AQI, good}]                                                             |{0.15, 720, -45, NULL} |[{10:00:00, reading, Normal operation}, {10:15:00, adjustment, Temperature set to 73F}]                                                |\n",
      "|D002     |2024-01-15 10:00:00|{Motion Sensor Entry, motion_sensor, SecureHome, MS-100, 1.8.2, {Main Building, 1, Entry Hall, {5.2, 15.8}}}            |[{S004, motion, 1.0, binary, detected}, {S005, light, 450.0, lux, normal}]                                                                                                  |{0.05, 1440, -52, 85}  |[{09:45:00, motion_detected, Movement detected}, {10:00:00, reading, Normal operation}, {10:05:00, motion_detected, Movement detected}]|\n",
      "|D003     |2024-01-15 10:00:00|{Smart Meter Kitchen, energy_monitor, EnergyTech, EM-500, 3.2.1, {Main Building, 1, Kitchen, {15.7, 22.1}}}             |[{S006, power, 2.45, kW, normal}, {S007, voltage, 120.2, V, normal}, {S008, current, 20.4, A, normal}, {S009, frequency, 60.0, Hz, normal}]                                 |{0.02, 2160, -38, NULL}|[{08:00:00, high_usage, Power spike detected}, {10:00:00, reading, Normal operation}]                                                  |\n",
      "|D004     |2024-01-15 10:00:00|{Air Quality Monitor Bedroom, air_quality, AirSense, AQ-300, 2.0.8, {Main Building, 2, Master Bedroom, {12.3, 25.6}}}   |[{S010, co2, 450.0, ppm, normal}, {S011, pm25, 12.0, g/m, good}, {S012, voc, 0.3, mg/m, normal}, {S013, temperature, 68.5, F, normal}, {S014, humidity, 50.1, %, normal}]|{0.08, 1800, -48, NULL}|[{10:00:00, reading, All sensors normal}]                                                                                              |\n",
      "+---------+-------------------+------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create nested IoT sensor data with explicit schema\n",
    "\n",
    "# Define explicit schema for IoT data\n",
    "iot_schema = StructType([\n",
    "    StructField(\"device_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"device_info\", StructType([\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"type\", StringType(), False),\n",
    "        StructField(\"manufacturer\", StringType(), False),\n",
    "        StructField(\"model\", StringType(), False),\n",
    "        StructField(\"firmware_version\", StringType(), False),\n",
    "        StructField(\"location\", StructType([\n",
    "            StructField(\"building\", StringType(), False),\n",
    "            StructField(\"floor\", IntegerType(), False),\n",
    "            StructField(\"room\", StringType(), False),\n",
    "            StructField(\"coordinates\", StructType([\n",
    "                StructField(\"x\", DoubleType(), False),\n",
    "                StructField(\"y\", DoubleType(), False)\n",
    "            ]), False)\n",
    "        ]), False)\n",
    "    ]), False),\n",
    "    StructField(\"sensors\", ArrayType(StructType([\n",
    "        StructField(\"sensor_id\", StringType(), False),\n",
    "        StructField(\"type\", StringType(), False),\n",
    "        StructField(\"value\", DoubleType(), False),\n",
    "        StructField(\"unit\", StringType(), False),\n",
    "        StructField(\"status\", StringType(), False)\n",
    "    ])), False),\n",
    "    StructField(\"metrics\", StructType([\n",
    "        StructField(\"power_consumption\", DoubleType(), False),\n",
    "        StructField(\"uptime_hours\", IntegerType(), False),\n",
    "        StructField(\"signal_strength\", IntegerType(), False),\n",
    "        StructField(\"battery_level\", IntegerType(), True)\n",
    "    ]), False),\n",
    "    StructField(\"events\", ArrayType(StructType([\n",
    "        StructField(\"time\", StringType(), False),\n",
    "        StructField(\"type\", StringType(), False),\n",
    "        StructField(\"description\", StringType(), False)\n",
    "    ])), False)\n",
    "])\n",
    "\n",
    "iot_data = [\n",
    "    {\n",
    "        \"device_id\": \"D001\",\n",
    "        \"timestamp\": \"2024-01-15 10:00:00\",\n",
    "        \"device_info\": {\n",
    "            \"name\": \"Smart Thermostat Living Room\",\n",
    "            \"type\": \"thermostat\",\n",
    "            \"manufacturer\": \"SmartHome Inc\",\n",
    "            \"model\": \"TH-2000\",\n",
    "            \"firmware_version\": \"2.1.5\",\n",
    "            \"location\": {\n",
    "                \"building\": \"Main Building\",\n",
    "                \"floor\": 1,\n",
    "                \"room\": \"Living Room\",\n",
    "                \"coordinates\": {\"x\": 10.5, \"y\": 20.3}\n",
    "            }\n",
    "        },\n",
    "        \"sensors\": [\n",
    "            {\"sensor_id\": \"S001\", \"type\": \"temperature\", \"value\": 72.5, \"unit\": \"F\", \"status\": \"normal\"},\n",
    "            {\"sensor_id\": \"S002\", \"type\": \"humidity\", \"value\": 45.2, \"unit\": \"%\", \"status\": \"normal\"},\n",
    "            {\"sensor_id\": \"S003\", \"type\": \"air_quality\", \"value\": 85.0, \"unit\": \"AQI\", \"status\": \"good\"}\n",
    "        ],\n",
    "        \"metrics\": {\n",
    "            \"power_consumption\": 0.15,\n",
    "            \"uptime_hours\": 720,\n",
    "            \"signal_strength\": -45,\n",
    "            \"battery_level\": None\n",
    "        },\n",
    "        \"events\": [\n",
    "            {\"time\": \"10:00:00\", \"type\": \"reading\", \"description\": \"Normal operation\"},\n",
    "            {\"time\": \"10:15:00\", \"type\": \"adjustment\", \"description\": \"Temperature set to 73F\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"device_id\": \"D002\",\n",
    "        \"timestamp\": \"2024-01-15 10:00:00\",\n",
    "        \"device_info\": {\n",
    "            \"name\": \"Motion Sensor Entry\",\n",
    "            \"type\": \"motion_sensor\",\n",
    "            \"manufacturer\": \"SecureHome\",\n",
    "            \"model\": \"MS-100\",\n",
    "            \"firmware_version\": \"1.8.2\",\n",
    "            \"location\": {\n",
    "                \"building\": \"Main Building\",\n",
    "                \"floor\": 1,\n",
    "                \"room\": \"Entry Hall\",\n",
    "                \"coordinates\": {\"x\": 5.2, \"y\": 15.8}\n",
    "            }\n",
    "        },\n",
    "        \"sensors\": [\n",
    "            {\"sensor_id\": \"S004\", \"type\": \"motion\", \"value\": 1.0, \"unit\": \"binary\", \"status\": \"detected\"},\n",
    "            {\"sensor_id\": \"S005\", \"type\": \"light\", \"value\": 450.0, \"unit\": \"lux\", \"status\": \"normal\"}\n",
    "        ],\n",
    "        \"metrics\": {\n",
    "            \"power_consumption\": 0.05,\n",
    "            \"uptime_hours\": 1440,\n",
    "            \"signal_strength\": -52,\n",
    "            \"battery_level\": 85\n",
    "        },\n",
    "        \"events\": [\n",
    "            {\"time\": \"09:45:00\", \"type\": \"motion_detected\", \"description\": \"Movement detected\"},\n",
    "            {\"time\": \"10:00:00\", \"type\": \"reading\", \"description\": \"Normal operation\"},\n",
    "            {\"time\": \"10:05:00\", \"type\": \"motion_detected\", \"description\": \"Movement detected\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"device_id\": \"D003\",\n",
    "        \"timestamp\": \"2024-01-15 10:00:00\",\n",
    "        \"device_info\": {\n",
    "            \"name\": \"Smart Meter Kitchen\",\n",
    "            \"type\": \"energy_monitor\",\n",
    "            \"manufacturer\": \"EnergyTech\",\n",
    "            \"model\": \"EM-500\",\n",
    "            \"firmware_version\": \"3.2.1\",\n",
    "            \"location\": {\n",
    "                \"building\": \"Main Building\",\n",
    "                \"floor\": 1,\n",
    "                \"room\": \"Kitchen\",\n",
    "                \"coordinates\": {\"x\": 15.7, \"y\": 22.1}\n",
    "            }\n",
    "        },\n",
    "        \"sensors\": [\n",
    "            {\"sensor_id\": \"S006\", \"type\": \"power\", \"value\": 2.45, \"unit\": \"kW\", \"status\": \"normal\"},\n",
    "            {\"sensor_id\": \"S007\", \"type\": \"voltage\", \"value\": 120.2, \"unit\": \"V\", \"status\": \"normal\"},\n",
    "            {\"sensor_id\": \"S008\", \"type\": \"current\", \"value\": 20.4, \"unit\": \"A\", \"status\": \"normal\"},\n",
    "            {\"sensor_id\": \"S009\", \"type\": \"frequency\", \"value\": 60.0, \"unit\": \"Hz\", \"status\": \"normal\"}\n",
    "        ],\n",
    "        \"metrics\": {\n",
    "            \"power_consumption\": 0.02,\n",
    "            \"uptime_hours\": 2160,\n",
    "            \"signal_strength\": -38,\n",
    "            \"battery_level\": None\n",
    "        },\n",
    "        \"events\": [\n",
    "            {\"time\": \"08:00:00\", \"type\": \"high_usage\", \"description\": \"Power spike detected\"},\n",
    "            {\"time\": \"10:00:00\", \"type\": \"reading\", \"description\": \"Normal operation\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"device_id\": \"D004\",\n",
    "        \"timestamp\": \"2024-01-15 10:00:00\",\n",
    "        \"device_info\": {\n",
    "            \"name\": \"Air Quality Monitor Bedroom\",\n",
    "            \"type\": \"air_quality\",\n",
    "            \"manufacturer\": \"AirSense\",\n",
    "            \"model\": \"AQ-300\",\n",
    "            \"firmware_version\": \"2.0.8\",\n",
    "            \"location\": {\n",
    "                \"building\": \"Main Building\",\n",
    "                \"floor\": 2,\n",
    "                \"room\": \"Master Bedroom\",\n",
    "                \"coordinates\": {\"x\": 12.3, \"y\": 25.6}\n",
    "            }\n",
    "        },\n",
    "        \"sensors\": [\n",
    "            {\"sensor_id\": \"S010\", \"type\": \"co2\", \"value\": 450.0, \"unit\": \"ppm\", \"status\": \"normal\"},\n",
    "            {\"sensor_id\": \"S011\", \"type\": \"pm25\", \"value\": 12.0, \"unit\": \"g/m\", \"status\": \"good\"},\n",
    "            {\"sensor_id\": \"S012\", \"type\": \"voc\", \"value\": 0.3, \"unit\": \"mg/m\", \"status\": \"normal\"},\n",
    "            {\"sensor_id\": \"S013\", \"type\": \"temperature\", \"value\": 68.5, \"unit\": \"F\", \"status\": \"normal\"},\n",
    "            {\"sensor_id\": \"S014\", \"type\": \"humidity\", \"value\": 50.1, \"unit\": \"%\", \"status\": \"normal\"}\n",
    "        ],\n",
    "        \"metrics\": {\n",
    "            \"power_consumption\": 0.08,\n",
    "            \"uptime_hours\": 1800,\n",
    "            \"signal_strength\": -48,\n",
    "            \"battery_level\": None\n",
    "        },\n",
    "        \"events\": [\n",
    "            {\"time\": \"10:00:00\", \"type\": \"reading\", \"description\": \"All sensors normal\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame with explicit schema\n",
    "iot_df = spark.createDataFrame(iot_data, schema=iot_schema)\n",
    "\n",
    "print(\"\\n=== IoT Sensor Data Schema ===\")\n",
    "iot_df.printSchema()\n",
    "\n",
    "print(\"\\n=== IoT Sensor Data ===\")\n",
    "iot_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Device Overview ===\n",
      "+---------+-------------------+----------------------------+--------------+-------------+-----+--------------+--------+------------+----------+------------+-----------+\n",
      "|device_id|timestamp          |device_name                 |device_type   |manufacturer |floor|room          |power_kw|uptime_hours|signal_dbm|sensor_count|event_count|\n",
      "+---------+-------------------+----------------------------+--------------+-------------+-----+--------------+--------+------------+----------+------------+-----------+\n",
      "|D001     |2024-01-15 10:00:00|Smart Thermostat Living Room|thermostat    |SmartHome Inc|1    |Living Room   |0.15    |720         |-45       |3           |2          |\n",
      "|D002     |2024-01-15 10:00:00|Motion Sensor Entry         |motion_sensor |SecureHome   |1    |Entry Hall    |0.05    |1440        |-52       |2           |3          |\n",
      "|D003     |2024-01-15 10:00:00|Smart Meter Kitchen         |energy_monitor|EnergyTech   |1    |Kitchen       |0.02    |2160        |-38       |4           |2          |\n",
      "|D004     |2024-01-15 10:00:00|Air Quality Monitor Bedroom |air_quality   |AirSense     |2    |Master Bedroom|0.08    |1800        |-48       |5           |1          |\n",
      "+---------+-------------------+----------------------------+--------------+-------------+-----+--------------+--------+------------+----------+------------+-----------+\n",
      "\n",
      "\n",
      "=== Exploded Sensor Readings ===\n",
      "+---------+--------------+--------------+---------+-----------+-----+------+--------+\n",
      "|device_id|device_type   |room          |sensor_id|sensor_type|value|unit  |status  |\n",
      "+---------+--------------+--------------+---------+-----------+-----+------+--------+\n",
      "|D001     |thermostat    |Living Room   |S001     |temperature|72.5 |F     |normal  |\n",
      "|D001     |thermostat    |Living Room   |S002     |humidity   |45.2 |%     |normal  |\n",
      "|D001     |thermostat    |Living Room   |S003     |air_quality|85.0 |AQI   |good    |\n",
      "|D002     |motion_sensor |Entry Hall    |S004     |motion     |1.0  |binary|detected|\n",
      "|D002     |motion_sensor |Entry Hall    |S005     |light      |450.0|lux   |normal  |\n",
      "|D003     |energy_monitor|Kitchen       |S006     |power      |2.45 |kW    |normal  |\n",
      "|D003     |energy_monitor|Kitchen       |S007     |voltage    |120.2|V     |normal  |\n",
      "|D003     |energy_monitor|Kitchen       |S008     |current    |20.4 |A     |normal  |\n",
      "|D003     |energy_monitor|Kitchen       |S009     |frequency  |60.0 |Hz    |normal  |\n",
      "|D004     |air_quality   |Master Bedroom|S010     |co2        |450.0|ppm   |normal  |\n",
      "|D004     |air_quality   |Master Bedroom|S011     |pm25       |12.0 |g/m |good    |\n",
      "|D004     |air_quality   |Master Bedroom|S012     |voc        |0.3  |mg/m |normal  |\n",
      "|D004     |air_quality   |Master Bedroom|S013     |temperature|68.5 |F     |normal  |\n",
      "|D004     |air_quality   |Master Bedroom|S014     |humidity   |50.1 |%     |normal  |\n",
      "+---------+--------------+--------------+---------+-----------+-----+------+--------+\n",
      "\n",
      "\n",
      "=== Sensor Statistics by Type ===\n",
      "+-----------+------+-------------+---------+---------+---------+------------+----------+\n",
      "|sensor_type|unit  |reading_count|avg_value|min_value|max_value|devices     |statuses  |\n",
      "+-----------+------+-------------+---------+---------+---------+------------+----------+\n",
      "|air_quality|AQI   |1            |85.0     |85.0     |85.0     |[D001]      |[good]    |\n",
      "|co2        |ppm   |1            |450.0    |450.0    |450.0    |[D004]      |[normal]  |\n",
      "|current    |A     |1            |20.4     |20.4     |20.4     |[D003]      |[normal]  |\n",
      "|frequency  |Hz    |1            |60.0     |60.0     |60.0     |[D003]      |[normal]  |\n",
      "|humidity   |%     |2            |47.65    |45.2     |50.1     |[D004, D001]|[normal]  |\n",
      "|light      |lux   |1            |450.0    |450.0    |450.0    |[D002]      |[normal]  |\n",
      "|motion     |binary|1            |1.0      |1.0      |1.0      |[D002]      |[detected]|\n",
      "|pm25       |g/m |1            |12.0     |12.0     |12.0     |[D004]      |[good]    |\n",
      "|power      |kW    |1            |2.45     |2.45     |2.45     |[D003]      |[normal]  |\n",
      "|temperature|F     |2            |70.5     |68.5     |72.5     |[D004, D001]|[normal]  |\n",
      "|voc        |mg/m |1            |0.3      |0.3      |0.3      |[D004]      |[normal]  |\n",
      "|voltage    |V     |1            |120.2    |120.2    |120.2    |[D003]      |[normal]  |\n",
      "+-----------+------+-------------+---------+---------+---------+------------+----------+\n",
      "\n",
      "\n",
      "=== Room-Level Analysis with Nested Readings ===\n",
      "root\n",
      " |-- room: string (nullable = false)\n",
      " |-- device_count: long (nullable = false)\n",
      " |-- sensor_type_count: long (nullable = false)\n",
      " |-- total_readings: long (nullable = false)\n",
      " |-- all_readings: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- sensor_type: string (nullable = true)\n",
      " |    |    |-- value: double (nullable = true)\n",
      " |    |    |-- unit: string (nullable = true)\n",
      "\n",
      "+--------------+------------+-----------------+--------------+--------------------------------------------------------------------------------------------------------+\n",
      "|room          |device_count|sensor_type_count|total_readings|all_readings                                                                                            |\n",
      "+--------------+------------+-----------------+--------------+--------------------------------------------------------------------------------------------------------+\n",
      "|Entry Hall    |1           |2                |2             |[{motion, 1.0, binary}, {light, 450.0, lux}]                                                            |\n",
      "|Kitchen       |1           |4                |4             |[{power, 2.45, kW}, {voltage, 120.2, V}, {current, 20.4, A}, {frequency, 60.0, Hz}]                     |\n",
      "|Living Room   |1           |3                |3             |[{temperature, 72.5, F}, {humidity, 45.2, %}, {air_quality, 85.0, AQI}]                                 |\n",
      "|Master Bedroom|1           |5                |5             |[{co2, 450.0, ppm}, {pm25, 12.0, g/m}, {voc, 0.3, mg/m}, {temperature, 68.5, F}, {humidity, 50.1, %}]|\n",
      "+--------------+------------+-----------------+--------------+--------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract and analyze nested IoT data - Part 1: Device overview\n",
    "device_overview = iot_df.select(\n",
    "    \"device_id\",\n",
    "    \"timestamp\",\n",
    "    col(\"device_info.name\").alias(\"device_name\"),\n",
    "    col(\"device_info.type\").alias(\"device_type\"),\n",
    "    col(\"device_info.manufacturer\").alias(\"manufacturer\"),\n",
    "    col(\"device_info.location.floor\").alias(\"floor\"),\n",
    "    col(\"device_info.location.room\").alias(\"room\"),\n",
    "    col(\"metrics.power_consumption\").alias(\"power_kw\"),\n",
    "    col(\"metrics.uptime_hours\").alias(\"uptime_hours\"),\n",
    "    col(\"metrics.signal_strength\").alias(\"signal_dbm\"),\n",
    "    size(\"sensors\").alias(\"sensor_count\"),\n",
    "    size(\"events\").alias(\"event_count\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== Device Overview ===\")\n",
    "device_overview.show(truncate=False)\n",
    "\n",
    "# Explode sensors and analyze readings - Part 2\n",
    "sensor_readings = iot_df.select(\n",
    "    \"device_id\",\n",
    "    col(\"device_info.type\").alias(\"device_type\"),\n",
    "    col(\"device_info.location.room\").alias(\"room\"),\n",
    "    explode(\"sensors\").alias(\"sensor\")\n",
    ").select(\n",
    "    \"device_id\",\n",
    "    \"device_type\",\n",
    "    \"room\",\n",
    "    col(\"sensor.sensor_id\").alias(\"sensor_id\"),\n",
    "    col(\"sensor.type\").alias(\"sensor_type\"),\n",
    "    col(\"sensor.value\").alias(\"value\"),\n",
    "    col(\"sensor.unit\").alias(\"unit\"),\n",
    "    col(\"sensor.status\").alias(\"status\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== Exploded Sensor Readings ===\")\n",
    "sensor_readings.show(truncate=False)\n",
    "\n",
    "# Aggregate sensor statistics - Part 3\n",
    "sensor_stats = sensor_readings.groupBy(\"sensor_type\", \"unit\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"reading_count\"),\n",
    "        round(avg(\"value\"), 2).alias(\"avg_value\"),\n",
    "        round(min(\"value\"), 2).alias(\"min_value\"),\n",
    "        round(max(\"value\"), 2).alias(\"max_value\"),\n",
    "        collect_set(\"device_id\").alias(\"devices\"),\n",
    "        collect_set(\"status\").alias(\"statuses\")\n",
    "    ) \\\n",
    "    .orderBy(\"sensor_type\")\n",
    "\n",
    "print(\"\\n=== Sensor Statistics by Type ===\")\n",
    "sensor_stats.show(truncate=False)\n",
    "\n",
    "# Room-level aggregation with nested sensors - Part 4\n",
    "room_analysis = sensor_readings.groupBy(\"room\") \\\n",
    "    .agg(\n",
    "        countDistinct(\"device_id\").alias(\"device_count\"),\n",
    "        countDistinct(\"sensor_type\").alias(\"sensor_type_count\"),\n",
    "        count(\"*\").alias(\"total_readings\"),\n",
    "        collect_list(\n",
    "            struct(\n",
    "                col(\"sensor_type\"),\n",
    "                col(\"value\"),\n",
    "                col(\"unit\")\n",
    "            )\n",
    "        ).alias(\"all_readings\")\n",
    "    ) \\\n",
    "    .orderBy(\"room\")\n",
    "\n",
    "print(\"\\n=== Room-Level Analysis with Nested Readings ===\")\n",
    "room_analysis.printSchema()\n",
    "room_analysis.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Exploded Events ===\n",
      "+---------+----------------------------+--------------+----------+---------------+----------------------+\n",
      "|device_id|device_name                 |device_type   |event_time|event_type     |description           |\n",
      "+---------+----------------------------+--------------+----------+---------------+----------------------+\n",
      "|D001     |Smart Thermostat Living Room|thermostat    |10:00:00  |reading        |Normal operation      |\n",
      "|D001     |Smart Thermostat Living Room|thermostat    |10:15:00  |adjustment     |Temperature set to 73F|\n",
      "|D002     |Motion Sensor Entry         |motion_sensor |09:45:00  |motion_detected|Movement detected     |\n",
      "|D002     |Motion Sensor Entry         |motion_sensor |10:00:00  |reading        |Normal operation      |\n",
      "|D002     |Motion Sensor Entry         |motion_sensor |10:05:00  |motion_detected|Movement detected     |\n",
      "|D003     |Smart Meter Kitchen         |energy_monitor|08:00:00  |high_usage     |Power spike detected  |\n",
      "|D003     |Smart Meter Kitchen         |energy_monitor|10:00:00  |reading        |Normal operation      |\n",
      "|D004     |Air Quality Monitor Bedroom |air_quality   |10:00:00  |reading        |All sensors normal    |\n",
      "+---------+----------------------------+--------------+----------+---------------+----------------------+\n",
      "\n",
      "\n",
      "=== Event Summary by Device and Event Type ===\n",
      "+--------------+---------------+-----------+----------------+--------------------------------------+\n",
      "|device_type   |event_type     |event_count|affected_devices|descriptions                          |\n",
      "+--------------+---------------+-----------+----------------+--------------------------------------+\n",
      "|air_quality   |reading        |1          |1               |[All sensors normal]                  |\n",
      "|energy_monitor|high_usage     |1          |1               |[Power spike detected]                |\n",
      "|energy_monitor|reading        |1          |1               |[Normal operation]                    |\n",
      "|motion_sensor |motion_detected|2          |1               |[Movement detected, Movement detected]|\n",
      "|motion_sensor |reading        |1          |1               |[Normal operation]                    |\n",
      "|thermostat    |reading        |1          |1               |[Normal operation]                    |\n",
      "|thermostat    |adjustment     |1          |1               |[Temperature set to 73F]              |\n",
      "+--------------+---------------+-----------+----------------+--------------------------------------+\n",
      "\n",
      "\n",
      "=== Comprehensive Device Health Report ===\n",
      "root\n",
      " |-- device_id: string (nullable = false)\n",
      " |-- name: string (nullable = false)\n",
      " |-- type: string (nullable = false)\n",
      " |-- health_report: struct (nullable = false)\n",
      " |    |-- sensor_count: long (nullable = true)\n",
      " |    |-- sensor_data: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = false)\n",
      " |    |    |    |-- sensor_type: string (nullable = true)\n",
      " |    |    |    |-- value: double (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |-- event_count: long (nullable = true)\n",
      " |    |-- recent_events: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = false)\n",
      " |    |    |    |-- event_type: string (nullable = true)\n",
      " |    |    |    |-- description: string (nullable = true)\n",
      "\n",
      "+---------+----------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|device_id|name                        |type          |health_report                                                                                                                                                                   |\n",
      "+---------+----------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|D002     |Motion Sensor Entry         |motion_sensor |{2, [{motion, 1.0, detected}, {light, 450.0, normal}], 3, [{motion_detected, Movement detected}, {reading, Normal operation}, {motion_detected, Movement detected}]}            |\n",
      "|D004     |Air Quality Monitor Bedroom |air_quality   |{5, [{co2, 450.0, normal}, {pm25, 12.0, good}, {voc, 0.3, normal}, {temperature, 68.5, normal}, {humidity, 50.1, normal}], 1, [{reading, All sensors normal}]}                  |\n",
      "|D001     |Smart Thermostat Living Room|thermostat    |{3, [{temperature, 72.5, normal}, {humidity, 45.2, normal}, {air_quality, 85.0, good}], 2, [{reading, Normal operation}, {adjustment, Temperature set to 73F}]}                 |\n",
      "|D003     |Smart Meter Kitchen         |energy_monitor|{4, [{power, 2.45, normal}, {voltage, 120.2, normal}, {current, 20.4, normal}, {frequency, 60.0, normal}], 2, [{high_usage, Power spike detected}, {reading, Normal operation}]}|\n",
      "+---------+----------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "=== Temperature Analysis by Device Type and Room ===\n",
      "+-----------+-----------+--------------+\n",
      "|device_type|Living Room|Master Bedroom|\n",
      "+-----------+-----------+--------------+\n",
      "|air_quality|NULL       |68.5          |\n",
      "|thermostat |72.5       |NULL          |\n",
      "+-----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Event analysis - Part 5\n",
    "events_exploded = iot_df.select(\n",
    "    \"device_id\",\n",
    "    col(\"device_info.name\").alias(\"device_name\"),\n",
    "    col(\"device_info.type\").alias(\"device_type\"),\n",
    "    \"timestamp\",\n",
    "    explode(\"events\").alias(\"event\")\n",
    ").select(\n",
    "    \"device_id\",\n",
    "    \"device_name\",\n",
    "    \"device_type\",\n",
    "    col(\"event.time\").alias(\"event_time\"),\n",
    "    col(\"event.type\").alias(\"event_type\"),\n",
    "    col(\"event.description\").alias(\"description\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== Exploded Events ===\")\n",
    "events_exploded.show(truncate=False)\n",
    "\n",
    "# Event type analysis\n",
    "event_summary = events_exploded.groupBy(\"device_type\", \"event_type\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"event_count\"),\n",
    "        countDistinct(\"device_id\").alias(\"affected_devices\"),\n",
    "        collect_list(\"description\").alias(\"descriptions\")\n",
    "    ) \\\n",
    "    .orderBy(\"device_type\", desc(\"event_count\"))\n",
    "\n",
    "print(\"\\n=== Event Summary by Device and Event Type ===\")\n",
    "event_summary.show(truncate=False)\n",
    "\n",
    "# Create comprehensive device health report with nested structures - Part 6\n",
    "device_health = iot_df.select(\n",
    "    \"device_id\",\n",
    "    col(\"device_info.name\").alias(\"name\"),\n",
    "    col(\"device_info.type\").alias(\"type\")\n",
    ")\n",
    "\n",
    "# Join with sensor stats per device\n",
    "device_sensor_stats = sensor_readings.groupBy(\"device_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"sensor_count\"),\n",
    "        collect_list(\n",
    "            struct(\n",
    "                col(\"sensor_type\"),\n",
    "                col(\"value\"),\n",
    "                col(\"status\")\n",
    "            )\n",
    "        ).alias(\"sensor_data\")\n",
    "    )\n",
    "\n",
    "# Join with event stats per device\n",
    "device_event_stats = events_exploded.groupBy(\"device_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"event_count\"),\n",
    "        collect_list(\n",
    "            struct(\n",
    "                col(\"event_type\"),\n",
    "                col(\"description\")\n",
    "            )\n",
    "        ).alias(\"recent_events\")\n",
    "    )\n",
    "\n",
    "# Create final nested health report\n",
    "health_report = device_health \\\n",
    "    .join(device_sensor_stats, \"device_id\", \"left\") \\\n",
    "    .join(device_event_stats, \"device_id\", \"left\") \\\n",
    "    .withColumn(\n",
    "        \"health_report\",\n",
    "        struct(\n",
    "            col(\"sensor_count\"),\n",
    "            col(\"sensor_data\"),\n",
    "            col(\"event_count\"),\n",
    "            col(\"recent_events\")\n",
    "        )\n",
    "    ) \\\n",
    "    .select(\"device_id\", \"name\", \"type\", \"health_report\")\n",
    "\n",
    "print(\"\\n=== Comprehensive Device Health Report ===\")\n",
    "health_report.printSchema()\n",
    "health_report.show(truncate=False)\n",
    "\n",
    "# Temperature analysis with pivot\n",
    "temp_sensors = sensor_readings.filter(col(\"sensor_type\") == \"temperature\")\n",
    "temp_pivot = temp_sensors.groupBy(\"device_type\") \\\n",
    "    .pivot(\"room\") \\\n",
    "    .agg(round(avg(\"value\"), 1))\n",
    "\n",
    "print(\"\\n=== Temperature Analysis by Device Type and Room ===\")\n",
    "temp_pivot.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cleanup and Summary\n",
    "\n",
    "Stop the Spark session when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY OF EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "Section 1: Joining & Grouping\n",
      "  - Example 1.1: E-Commerce multi-table joins with aggregations\n",
      "  - Example 1.2: Employee hierarchy with self-joins and window functions\n",
      "  - Example 1.3: Student enrollment with complex grouping\n",
      "\n",
      "Section 2: Multidimensional DataFrames\n",
      "  - Example 2.1: Sales data cube with rollup and pivot\n",
      "  - Example 2.2: Weather data multi-dimensional analysis\n",
      "  - Example 2.3: Financial portfolio with complete cube operations\n",
      "\n",
      "Section 3: Nested Columns\n",
      "  - Example 3.1: E-Commerce orders with deep nesting and array operations\n",
      "  - Example 3.2: Social media posts with complex nested structures\n",
      "  - Example 3.3: IoT sensor data with nested device info and events\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Spark Application UI: http://localhost:4040\n",
      "Spark Master UI: http://localhost:8080\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nSection 1: Joining & Grouping\")\n",
    "print(\"  - Example 1.1: E-Commerce multi-table joins with aggregations\")\n",
    "print(\"  - Example 1.2: Employee hierarchy with self-joins and window functions\")\n",
    "print(\"  - Example 1.3: Student enrollment with complex grouping\")\n",
    "print(\"\\nSection 2: Multidimensional DataFrames\")\n",
    "print(\"  - Example 2.1: Sales data cube with rollup and pivot\")\n",
    "print(\"  - Example 2.2: Weather data multi-dimensional analysis\")\n",
    "print(\"  - Example 2.3: Financial portfolio with complete cube operations\")\n",
    "print(\"\\nSection 3: Nested Columns\")\n",
    "print(\"  - Example 3.1: E-Commerce orders with deep nesting and array operations\")\n",
    "print(\"  - Example 3.2: Social media posts with complex nested structures\")\n",
    "print(\"  - Example 3.3: IoT sensor data with nested device info and events\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\\nSpark Application UI: http://localhost:4040\")\n",
    "print(f\"Spark Master UI: http://localhost:8080\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Uncomment to stop the Spark session\n",
    "# spark.stop()\n",
    "# print(\"\\nSpark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
